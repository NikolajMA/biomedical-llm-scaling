{
  "results": {
    "pubmedqa": {
      "acc,none": 0.736,
      "acc_stderr,none": 0.019732885585922177,
      "alias": "pubmedqa"
    },
    "mmlu_professional_medicine": {
      "alias": "professional_medicine",
      "acc,none": 0.19852941176470587,
      "acc_stderr,none": 0.02423101337054108
    },
    "mmlu_medical_genetics": {
      "alias": "medical_genetics",
      "acc,none": 0.28,
      "acc_stderr,none": 0.04512608598542129
    },
    "mmlu_college_medicine": {
      "alias": "college_medicine",
      "acc,none": 0.2254335260115607,
      "acc_stderr,none": 0.03186209851641144
    },
    "mmlu_college_biology": {
      "alias": "college_biology",
      "acc,none": 0.24305555555555555,
      "acc_stderr,none": 0.03586879280080341
    },
    "mmlu_clinical_knowledge": {
      "alias": "clinical_knowledge",
      "acc,none": 0.30943396226415093,
      "acc_stderr,none": 0.028450154794118627
    },
    "mmlu_anatomy": {
      "alias": "anatomy",
      "acc,none": 0.3037037037037037,
      "acc_stderr,none": 0.039725528847851375
    },
    "medqa_4options": {
      "acc,none": 0.26865671641791045,
      "acc_stderr,none": 0.01242842037319496,
      "acc_norm,none": 0.26865671641791045,
      "acc_norm_stderr,none": 0.01242842037319496,
      "alias": "medqa_4options"
    },
    "medmcqa": {
      "acc,none": 0.2598613435333493,
      "acc_stderr,none": 0.0067816499544602355,
      "acc_norm,none": 0.2598613435333493,
      "acc_norm_stderr,none": 0.0067816499544602355,
      "alias": "medmcqa"
    }
  },
  "group_subtasks": {
    "medmcqa": [],
    "medqa_4options": [],
    "mmlu_anatomy": [],
    "mmlu_clinical_knowledge": [],
    "mmlu_college_biology": [],
    "mmlu_college_medicine": [],
    "mmlu_medical_genetics": [],
    "mmlu_professional_medicine": [],
    "pubmedqa": []
  },
  "configs": {
    "medmcqa": {
      "task": "medmcqa",
      "dataset_path": "medmcqa",
      "training_split": "train",
      "validation_split": "validation",
      "test_split": "validation",
      "doc_to_text": "def doc_to_text(doc) -> str:\n    \"\"\"\n    Question: <question>\n    Choices:\n    A. <choice1>\n    B. <choice2>\n    C. <choice3>\n    D. <choice4>\n    Answer:\n    \"\"\"\n    choices = [doc[\"opa\"], doc[\"opb\"], doc[\"opc\"], doc[\"opd\"]]\n    option_choices = {\n        \"A\": choices[0],\n        \"B\": choices[1],\n        \"C\": choices[2],\n        \"D\": choices[3],\n    }\n\n    prompt = \"Question: \" + doc[\"question\"] + \"\\nChoices:\\n\"\n    for choice, option in option_choices.items():\n        prompt += f\"{choice.upper()}. {option}\\n\"\n    prompt += \"Answer:\"\n    return prompt\n",
      "doc_to_target": "cop",
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": true,
      "doc_to_decontamination_query": "{{question}}"
    },
    "medqa_4options": {
      "task": "medqa_4options",
      "dataset_path": "GBaker/MedQA-USMLE-4-options-hf",
      "training_split": "train",
      "validation_split": "validation",
      "test_split": "test",
      "doc_to_text": "def doc_to_text(doc) -> str:\n    option_choices = {\n        \"A\": doc[\"ending0\"],\n        \"B\": doc[\"ending1\"],\n        \"C\": doc[\"ending2\"],\n        \"D\": doc[\"ending3\"],\n    }\n    answers = \"\".join((f\"{k}. {v}\\n\") for k, v in option_choices.items())\n    return f\"Question: {doc['sent1']}\\n{answers}Answer:\"\n",
      "doc_to_target": "def doc_to_target(doc) -> int:\n    return doc[\"label\"]\n",
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_anatomy": {
      "task": "mmlu_anatomy",
      "task_alias": "anatomy",
      "group": "mmlu_stem",
      "group_alias": "stem",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "anatomy",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "The following are multiple choice questions (with answers) about anatomy.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 0.0
      }
    },
    "mmlu_clinical_knowledge": {
      "task": "mmlu_clinical_knowledge",
      "task_alias": "clinical_knowledge",
      "group": "mmlu_other",
      "group_alias": "other",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "clinical_knowledge",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "The following are multiple choice questions (with answers) about clinical knowledge.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 0.0
      }
    },
    "mmlu_college_biology": {
      "task": "mmlu_college_biology",
      "task_alias": "college_biology",
      "group": "mmlu_stem",
      "group_alias": "stem",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "college_biology",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "The following are multiple choice questions (with answers) about college biology.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 0.0
      }
    },
    "mmlu_college_medicine": {
      "task": "mmlu_college_medicine",
      "task_alias": "college_medicine",
      "group": "mmlu_other",
      "group_alias": "other",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "college_medicine",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "The following are multiple choice questions (with answers) about college medicine.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 0.0
      }
    },
    "mmlu_medical_genetics": {
      "task": "mmlu_medical_genetics",
      "task_alias": "medical_genetics",
      "group": "mmlu_other",
      "group_alias": "other",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "medical_genetics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "The following are multiple choice questions (with answers) about medical genetics.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 0.0
      }
    },
    "mmlu_professional_medicine": {
      "task": "mmlu_professional_medicine",
      "task_alias": "professional_medicine",
      "group": "mmlu_other",
      "group_alias": "other",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "professional_medicine",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "The following are multiple choice questions (with answers) about professional medicine.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 0.0
      }
    },
    "pubmedqa": {
      "task": "pubmedqa",
      "dataset_path": "bigbio/pubmed_qa",
      "dataset_name": "pubmed_qa_labeled_fold0_source",
      "training_split": "train",
      "validation_split": "validation",
      "test_split": "test",
      "doc_to_text": "def doc_to_text(doc) -> str:\n    ctxs = \"\\n\".join(doc[\"CONTEXTS\"])\n    return \"Abstract: {}\\nQuestion: {}\\nAnswer:\".format(\n        ctxs,\n        doc[\"QUESTION\"],\n    )\n",
      "doc_to_target": "final_decision",
      "doc_to_choice": [
        "yes",
        "no",
        "maybe"
      ],
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0
      }
    }
  },
  "versions": {
    "medmcqa": "Yaml",
    "medqa_4options": "Yaml",
    "mmlu_anatomy": 0.0,
    "mmlu_clinical_knowledge": 0.0,
    "mmlu_college_biology": 0.0,
    "mmlu_college_medicine": 0.0,
    "mmlu_medical_genetics": 0.0,
    "mmlu_professional_medicine": 0.0,
    "pubmedqa": 1.0
  },
  "n-shot": {
    "medmcqa": 0,
    "medqa_4options": 0,
    "mmlu_anatomy": 0,
    "mmlu_clinical_knowledge": 0,
    "mmlu_college_biology": 0,
    "mmlu_college_medicine": 0,
    "mmlu_medical_genetics": 0,
    "mmlu_professional_medicine": 0,
    "pubmedqa": 0
  },
  "config": {
    "model": "hf",
    "model_args": "pretrained=openlm-research/open_llama_7b,trust_remote_code=True",
    "batch_size": "auto",
    "batch_sizes": [
      8
    ],
    "device": "cuda:0",
    "use_cache": null,
    "limit": null,
    "bootstrap_iters": 100000,
    "gen_kwargs": null
  },
  "git_hash": "3196e907",
  "date": 1713519897.811775,
  "pretty_env_info": "PyTorch version: 2.2.1+cu121\nIs debug build: False\nCUDA used to build PyTorch: 12.1\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.3 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: 14.0.0-1ubuntu1.1\nCMake version: version 3.27.9\nLibc version: glibc-2.35\n\nPython version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] (64-bit runtime)\nPython platform: Linux-6.1.58+-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: 12.2.140\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: GPU 0: NVIDIA L4\nNvidia driver version: 535.104.05\ncuDNN version: Probably one of the following:\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nAddress sizes:                      46 bits physical, 48 bits virtual\nByte Order:                         Little Endian\nCPU(s):                             16\nOn-line CPU(s) list:                0-15\nVendor ID:                          GenuineIntel\nModel name:                         Intel(R) Xeon(R) CPU @ 2.20GHz\nCPU family:                         6\nModel:                              85\nThread(s) per core:                 2\nCore(s) per socket:                 8\nSocket(s):                          1\nStepping:                           7\nBogoMIPS:                           4400.45\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat avx512_vnni md_clear arch_capabilities\nHypervisor vendor:                  KVM\nVirtualization type:                full\nL1d cache:                          256 KiB (8 instances)\nL1i cache:                          256 KiB (8 instances)\nL2 cache:                           8 MiB (8 instances)\nL3 cache:                           38.5 MiB (1 instance)\nNUMA node(s):                       1\nNUMA node0 CPU(s):                  0-15\nVulnerability Gather data sampling: Not affected\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Vulnerable; SMT Host state unknown\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Vulnerable\nVulnerability Retbleed:             Vulnerable\nVulnerability Spec rstack overflow: Not affected\nVulnerability Spec store bypass:    Vulnerable\nVulnerability Spectre v1:           Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\nVulnerability Spectre v2:           Vulnerable, IBPB: disabled, STIBP: disabled, PBRSB-eIBRS: Vulnerable\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Vulnerable\n\nVersions of relevant libraries:\n[pip3] numpy==1.25.2\n[pip3] torch==2.2.1+cu121\n[pip3] torchaudio==2.2.1+cu121\n[pip3] torchdata==0.7.1\n[pip3] torchsummary==1.5.1\n[pip3] torchtext==0.17.1\n[pip3] torchvision==0.17.1+cu121\n[pip3] triton==2.2.0\n[conda] Could not collect",
  "transformers_version": "4.38.2",
  "upper_git_hash": null
}