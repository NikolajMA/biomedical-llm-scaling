{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tables from paper\n",
    "This notebook contains the code to recreate all the analyses and results presented in tables in the paper. Its recommended to run all cells from the top."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 1\n",
    "Table 1 is an overview of the format and size of MultiMedQA datasets. This data is manually assessed from the paper of each respective dataset, and does not require any additional processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['step0-tokens0B', 'step1000-tokens4B', 'step2000-tokens9B', 'step3000-tokens13B', 'step4000-tokens18B', 'step5000-tokens22B', 'step6000-tokens27B', 'step7000-tokens31B', 'step8000-tokens35B', 'step9000-tokens40B', 'step10000-tokens44B', 'step11000-tokens49B', 'step12000-tokens53B', 'step13000-tokens58B', 'step14000-tokens62B', 'step15000-tokens66B', 'step16000-tokens71B', 'step17000-tokens75B', 'step18000-tokens80B', 'step19000-tokens84B', 'step20000-tokens88B', 'step21000-tokens93B', 'step22000-tokens97B', 'step23000-tokens102B', 'step24000-tokens106B', 'step25000-tokens111B', 'step26000-tokens115B', 'step27000-tokens119B', 'step28000-tokens124B', 'step29000-tokens128B', 'step30000-tokens133B', 'step31000-tokens137B', 'step32000-tokens142B', 'step33000-tokens146B', 'step34000-tokens150B', 'step35000-tokens155B', 'step36000-tokens159B', 'step37000-tokens164B', 'step38000-tokens168B', 'step39000-tokens173B', 'step40000-tokens177B', 'step41000-tokens181B', 'step42000-tokens186B', 'step43000-tokens190B', 'step44000-tokens195B', 'step45000-tokens199B', 'step46000-tokens203B', 'step47000-tokens208B', 'step48000-tokens212B', 'step49000-tokens217B', 'step50000-tokens221B', 'step51000-tokens226B', 'step52000-tokens230B', 'step53000-tokens234B', 'step54000-tokens239B', 'step55000-tokens243B', 'step56000-tokens248B', 'step57000-tokens252B', 'step58000-tokens257B', 'step59000-tokens261B', 'step60000-tokens265B', 'step61000-tokens270B', 'step62000-tokens274B', 'step63000-tokens279B', 'step64000-tokens283B', 'step65000-tokens288B', 'step66000-tokens292B', 'step67000-tokens296B', 'step68000-tokens301B', 'step69000-tokens305B', 'step70000-tokens310B', 'step71000-tokens314B', 'step72000-tokens319B', 'step73000-tokens323B', 'step74000-tokens327B', 'step75000-tokens332B', 'step76000-tokens336B', 'step77000-tokens341B', 'step78000-tokens345B', 'step79000-tokens349B', 'step80000-tokens354B', 'step81000-tokens358B', 'step82000-tokens363B', 'step83000-tokens367B', 'step84000-tokens372B', 'step85000-tokens376B', 'step86000-tokens380B', 'step87000-tokens385B', 'step88000-tokens389B', 'step89000-tokens394B', 'step90000-tokens398B', 'step91000-tokens403B', 'step92000-tokens407B', 'step93000-tokens411B', 'step94000-tokens416B', 'step95000-tokens420B', 'step96000-tokens425B', 'step97000-tokens429B', 'step98000-tokens434B', 'step99000-tokens438B', 'step100000-tokens442B', 'step101000-tokens447B', 'step102000-tokens451B', 'step103000-tokens456B', 'step104000-tokens460B', 'step105000-tokens464B', 'step106000-tokens469B', 'step107000-tokens473B', 'step108000-tokens478B', 'step109000-tokens482B', 'step110000-tokens487B', 'step111000-tokens491B', 'step112000-tokens495B', 'step113000-tokens500B', 'step114000-tokens504B', 'step115000-tokens509B', 'step116000-tokens513B', 'step117000-tokens518B', 'step118000-tokens522B', 'step119000-tokens526B', 'step120000-tokens531B', 'step121000-tokens535B', 'step122000-tokens540B', 'step123000-tokens544B', 'step124000-tokens549B', 'step125000-tokens553B', 'step126000-tokens557B', 'step127000-tokens562B', 'step128000-tokens566B', 'step129000-tokens571B', 'step130000-tokens575B', 'step131000-tokens580B', 'step132000-tokens584B', 'step133000-tokens588B', 'step134000-tokens593B', 'step135000-tokens597B', 'step136000-tokens602B', 'step137000-tokens606B', 'step138000-tokens610B', 'step139000-tokens615B', 'step140000-tokens619B', 'step141000-tokens624B', 'step142000-tokens628B', 'step143000-tokens633B', 'step144000-tokens637B', 'step145000-tokens641B', 'step146000-tokens646B', 'step147000-tokens650B', 'step148000-tokens655B', 'step149000-tokens659B', 'step150000-tokens664B', 'step151000-tokens668B', 'step152000-tokens672B', 'step153000-tokens677B', 'step154000-tokens681B', 'step155000-tokens686B', 'step156000-tokens690B', 'step157000-tokens695B', 'step158000-tokens699B', 'step159000-tokens703B', 'step160000-tokens708B', 'step161000-tokens712B', 'step162000-tokens717B', 'step163000-tokens721B', 'step164000-tokens725B', 'step165000-tokens730B', 'step166000-tokens734B', 'step167000-tokens739B', 'step168000-tokens743B', 'step169000-tokens748B', 'step170000-tokens752B', 'step171000-tokens756B', 'step172000-tokens761B', 'step173000-tokens765B', 'step174000-tokens770B', 'step175000-tokens774B', 'step176000-tokens779B', 'step177000-tokens783B', 'step178000-tokens787B', 'step179000-tokens792B', 'step180000-tokens796B', 'step181000-tokens801B', 'step182000-tokens805B', 'step183000-tokens810B', 'step184000-tokens814B', 'step185000-tokens818B', 'step186000-tokens823B', 'step187000-tokens827B', 'step188000-tokens832B', 'step189000-tokens836B', 'step190000-tokens840B', 'step191000-tokens845B', 'step192000-tokens849B', 'step193000-tokens854B', 'step194000-tokens858B', 'step195000-tokens863B', 'step196000-tokens867B', 'step197000-tokens871B', 'step198000-tokens876B', 'step199000-tokens880B', 'step200000-tokens885B', 'step201000-tokens889B', 'step202000-tokens894B', 'step203000-tokens898B', 'step204000-tokens902B', 'step205000-tokens907B', 'step206000-tokens911B', 'step207000-tokens916B', 'step208000-tokens920B', 'step209000-tokens925B', 'step210000-tokens929B', 'step211000-tokens933B', 'step212000-tokens938B', 'step213000-tokens942B', 'step214000-tokens947B', 'step215000-tokens951B', 'step216000-tokens956B', 'step217000-tokens960B', 'step218000-tokens964B', 'step219000-tokens969B', 'step220000-tokens973B', 'step221000-tokens978B', 'step222000-tokens982B', 'step223000-tokens986B', 'step224000-tokens991B', 'step225000-tokens995B', 'step226000-tokens1000B', 'step227000-tokens1004B', 'step228000-tokens1009B', 'step229000-tokens1013B', 'step230000-tokens1017B', 'step231000-tokens1022B', 'step232000-tokens1026B', 'step233000-tokens1031B', 'step234000-tokens1035B', 'step235000-tokens1040B', 'step236000-tokens1044B', 'step237000-tokens1048B', 'step238000-tokens1053B', 'step239000-tokens1057B', 'step240000-tokens1062B', 'step241000-tokens1066B', 'step242000-tokens1071B', 'step243000-tokens1075B', 'step244000-tokens1079B', 'step245000-tokens1084B', 'step246000-tokens1088B', 'step247000-tokens1093B', 'step248000-tokens1097B', 'step249000-tokens1101B', 'step250000-tokens1106B', 'step251000-tokens1110B', 'step252000-tokens1115B', 'step253000-tokens1119B', 'step254000-tokens1124B', 'step255000-tokens1128B', 'step256000-tokens1132B', 'step257000-tokens1137B', 'step258000-tokens1141B', 'step259000-tokens1146B', 'step260000-tokens1150B', 'step261000-tokens1155B', 'step262000-tokens1159B', 'step263000-tokens1163B', 'step264000-tokens1168B', 'step265000-tokens1172B', 'step266000-tokens1177B', 'step267000-tokens1181B', 'step268000-tokens1186B', 'step269000-tokens1190B', 'step270000-tokens1194B', 'step271000-tokens1199B', 'step272000-tokens1203B', 'step273000-tokens1208B', 'step274000-tokens1212B', 'step275000-tokens1217B', 'step276000-tokens1221B', 'step277000-tokens1225B', 'step278000-tokens1230B', 'step279000-tokens1234B', 'step280000-tokens1239B', 'step281000-tokens1243B', 'step282000-tokens1247B', 'step283000-tokens1252B', 'step284000-tokens1256B', 'step285000-tokens1261B', 'step286000-tokens1265B', 'step287000-tokens1270B', 'step288000-tokens1274B', 'step289000-tokens1278B', 'step290000-tokens1283B', 'step291000-tokens1287B', 'step292000-tokens1292B', 'step293000-tokens1296B', 'step294000-tokens1301B', 'step295000-tokens1305B', 'step296000-tokens1309B', 'step297000-tokens1314B', 'step298000-tokens1318B', 'step299000-tokens1323B', 'step300000-tokens1327B', 'step301000-tokens1332B', 'step302000-tokens1336B', 'step303000-tokens1340B', 'step304000-tokens1345B', 'step305000-tokens1349B', 'step306000-tokens1354B', 'step307000-tokens1358B', 'step308000-tokens1362B', 'step309000-tokens1367B', 'step310000-tokens1371B', 'step311000-tokens1376B', 'step312000-tokens1380B', 'step313000-tokens1385B', 'step314000-tokens1389B', 'step315000-tokens1393B', 'step316000-tokens1398B', 'step317000-tokens1402B', 'step318000-tokens1407B', 'step319000-tokens1411B', 'step320000-tokens1416B', 'step321000-tokens1420B', 'step322000-tokens1424B', 'step323000-tokens1429B', 'step324000-tokens1433B', 'step325000-tokens1438B', 'step326000-tokens1442B', 'step327000-tokens1447B', 'step328000-tokens1451B', 'step329000-tokens1455B', 'step330000-tokens1460B', 'step331000-tokens1464B', 'step332000-tokens1469B', 'step333000-tokens1473B', 'step334000-tokens1478B', 'step335000-tokens1482B', 'step336000-tokens1486B', 'step337000-tokens1491B', 'step338000-tokens1495B', 'step339000-tokens1500B', 'step340000-tokens1504B', 'step341000-tokens1508B', 'step342000-tokens1513B', 'step343000-tokens1517B', 'step344000-tokens1522B', 'step345000-tokens1526B', 'step346000-tokens1531B', 'step347000-tokens1535B', 'step348000-tokens1539B', 'step349000-tokens1544B', 'step350000-tokens1548B', 'step351000-tokens1553B', 'step352000-tokens1557B', 'step353000-tokens1562B', 'step354000-tokens1566B', 'step355000-tokens1570B', 'step356000-tokens1575B', 'step357000-tokens1579B', 'step358000-tokens1584B', 'step359000-tokens1588B', 'step360000-tokens1593B', 'step361000-tokens1597B', 'step362000-tokens1601B', 'step363000-tokens1606B', 'step364000-tokens1610B', 'step365000-tokens1615B', 'step366000-tokens1619B', 'step367000-tokens1623B', 'step368000-tokens1628B', 'step369000-tokens1632B', 'step370000-tokens1637B', 'step371000-tokens1641B', 'step372000-tokens1646B', 'step373000-tokens1650B', 'step374000-tokens1654B', 'step375000-tokens1659B', 'step376000-tokens1663B', 'step377000-tokens1668B', 'step378000-tokens1672B', 'step379000-tokens1677B', 'step380000-tokens1681B', 'step381000-tokens1685B', 'step382000-tokens1690B', 'step383000-tokens1694B', 'step384000-tokens1699B', 'step385000-tokens1703B', 'step386000-tokens1708B', 'step387000-tokens1712B', 'step388000-tokens1716B', 'step389000-tokens1721B', 'step390000-tokens1725B', 'step391000-tokens1730B', 'step392000-tokens1734B', 'step393000-tokens1739B', 'step394000-tokens1743B', 'step395000-tokens1747B', 'step396000-tokens1752B', 'step397000-tokens1756B', 'step398000-tokens1761B', 'step399000-tokens1765B', 'step400000-tokens1769B', 'step401000-tokens1774B', 'step402000-tokens1778B', 'step403000-tokens1783B', 'step404000-tokens1787B', 'step405000-tokens1792B', 'step406000-tokens1796B', 'step407000-tokens1800B', 'step408000-tokens1805B', 'step409000-tokens1809B', 'step410000-tokens1814B', 'step411000-tokens1818B', 'step412000-tokens1823B', 'step413000-tokens1827B', 'step414000-tokens1831B', 'step415000-tokens1836B', 'step416000-tokens1840B', 'step417000-tokens1845B', 'step418000-tokens1849B', 'step419000-tokens1854B', 'step420000-tokens1858B', 'step421000-tokens1862B', 'step422000-tokens1867B', 'step423000-tokens1871B', 'step424000-tokens1876B', 'step425000-tokens1880B', 'step426000-tokens1884B', 'step427000-tokens1889B', 'step428000-tokens1893B', 'step429000-tokens1898B', 'step430000-tokens1902B', 'step431000-tokens1907B', 'step432000-tokens1911B', 'step433000-tokens1915B', 'step434000-tokens1920B', 'step435000-tokens1924B', 'step436000-tokens1929B', 'step437000-tokens1933B', 'step438000-tokens1938B', 'step439000-tokens1942B', 'step440000-tokens1946B', 'step441000-tokens1951B', 'step442000-tokens1955B', 'step443000-tokens1960B', 'step444000-tokens1964B', 'step445000-tokens1969B', 'step446000-tokens1973B', 'step447000-tokens1977B', 'step448000-tokens1982B', 'step449000-tokens1986B', 'step450000-tokens1991B', 'step451000-tokens1995B', 'step452000-tokens2000B', 'step453000-tokens2004B', 'step454000-tokens2008B', 'step455000-tokens2013B', 'step456000-tokens2017B', 'step457000-tokens2022B', 'step458000-tokens2026B', 'step459000-tokens2030B', 'step460000-tokens2035B', 'step461000-tokens2039B', 'step462000-tokens2044B', 'step463000-tokens2048B', 'step464000-tokens2053B', 'step465000-tokens2057B', 'step466000-tokens2061B', 'step467000-tokens2066B', 'step468000-tokens2070B', 'step469000-tokens2075B', 'step470000-tokens2079B', 'step471000-tokens2084B', 'step472000-tokens2088B', 'step473000-tokens2092B', 'step474000-tokens2097B', 'step475000-tokens2101B', 'step476000-tokens2106B', 'step477000-tokens2110B', 'step478000-tokens2115B', 'step479000-tokens2119B', 'step480000-tokens2123B', 'step481000-tokens2128B', 'step482000-tokens2132B', 'step483000-tokens2137B', 'step484000-tokens2141B', 'step485000-tokens2145B', 'step486000-tokens2150B', 'step487000-tokens2154B', 'step488000-tokens2159B', 'step489000-tokens2163B', 'step490000-tokens2168B', 'step491000-tokens2172B', 'step492000-tokens2176B', 'step493000-tokens2181B', 'step494000-tokens2185B', 'step495000-tokens2190B', 'step496000-tokens2194B', 'step497000-tokens2199B', 'step498000-tokens2203B', 'step499000-tokens2207B', 'step500000-tokens2212B', 'step501000-tokens2216B', 'step502000-tokens2221B', 'step503000-tokens2225B', 'step504000-tokens2230B', 'step505000-tokens2234B', 'step506000-tokens2238B', 'step507000-tokens2243B', 'step508000-tokens2247B', 'step509000-tokens2252B', 'step510000-tokens2256B', 'step511000-tokens2261B', 'step512000-tokens2265B', 'step513000-tokens2269B', 'step514000-tokens2274B', 'step515000-tokens2278B', 'step516000-tokens2283B', 'step517000-tokens2287B', 'step518000-tokens2291B', 'step519000-tokens2296B', 'step520000-tokens2300B', 'step521000-tokens2305B', 'step522000-tokens2309B', 'step523000-tokens2314B', 'step524000-tokens2318B', 'step525000-tokens2322B', 'step526000-tokens2327B', 'step527000-tokens2331B', 'step528000-tokens2336B', 'step529000-tokens2340B', 'step530000-tokens2345B', 'step531000-tokens2349B', 'step532000-tokens2353B', 'step533000-tokens2358B', 'step534000-tokens2362B', 'step535000-tokens2367B', 'step536000-tokens2371B', 'step537000-tokens2376B', 'step538000-tokens2380B', 'step539000-tokens2384B', 'step540000-tokens2389B', 'step541000-tokens2393B', 'step542000-tokens2398B', 'step543000-tokens2402B', 'step544000-tokens2406B', 'step545000-tokens2411B', 'step546000-tokens2415B', 'step547000-tokens2420B', 'step548000-tokens2424B', 'step549000-tokens2429B', 'step550000-tokens2433B', 'step551000-tokens2437B', 'step552000-tokens2442B', 'step553000-tokens2446B', 'step554000-tokens2451B', 'step555000-tokens2455B', 'step556000-tokens2460B', 'step557000-tokens2464B', 'main']\n"
     ]
    }
   ],
   "source": [
    "#Overview of intermediate pre-training steps used to evaluate OLMo 7B. \n",
    "# The specific revisions are noted in the paper\n",
    "\n",
    "from huggingface_hub import list_repo_refs\n",
    "\n",
    "out = list_repo_refs(\"allenai/OLMo-7B\")\n",
    "branches = [b.name for b in out.branches]\n",
    "\n",
    "# Extract the step number from the branch name\n",
    "def get_step_number(branch_name):\n",
    "    return int(branch_name.split('-')[0].replace('step', ''))\n",
    "\n",
    "sorted_branches = sorted(branches, key=lambda x: get_step_number(x) if x != \"main\" else float('inf'))\n",
    "\n",
    "print(sorted_branches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>model_name</th>\n",
       "      <th>medmcqa/acc</th>\n",
       "      <th>medmcqa/acc_stderr</th>\n",
       "      <th>medqa_4options/acc</th>\n",
       "      <th>medqa_4options/acc_stderr</th>\n",
       "      <th>mmlu_anatomy/acc</th>\n",
       "      <th>mmlu_anatomy/acc_stderr</th>\n",
       "      <th>mmlu_clinical_knowledge/acc</th>\n",
       "      <th>mmlu_clinical_knowledge/acc_stderr</th>\n",
       "      <th>...</th>\n",
       "      <th>mmlu_college_medicine/acc</th>\n",
       "      <th>mmlu_college_medicine/acc_stderr</th>\n",
       "      <th>mmlu_medical_genetics/acc</th>\n",
       "      <th>mmlu_medical_genetics/acc_stderr</th>\n",
       "      <th>mmlu_professional_medicine/acc</th>\n",
       "      <th>mmlu_professional_medicine/acc_stderr</th>\n",
       "      <th>model_family</th>\n",
       "      <th>param_count</th>\n",
       "      <th>pubmedqa/acc</th>\n",
       "      <th>pubmedqa/acc_stderr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Mamba-1.4b</td>\n",
       "      <td>0.235477</td>\n",
       "      <td>0.006561</td>\n",
       "      <td>0.228594</td>\n",
       "      <td>0.011774</td>\n",
       "      <td>0.237037</td>\n",
       "      <td>0.036737</td>\n",
       "      <td>0.196226</td>\n",
       "      <td>0.024442</td>\n",
       "      <td>...</td>\n",
       "      <td>0.219653</td>\n",
       "      <td>0.031568</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.046057</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.026304</td>\n",
       "      <td>Mamba</td>\n",
       "      <td>1400000000</td>\n",
       "      <td>0.652</td>\n",
       "      <td>0.021324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Mamba-130m</td>\n",
       "      <td>0.320344</td>\n",
       "      <td>0.007215</td>\n",
       "      <td>0.276512</td>\n",
       "      <td>0.012541</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0.033557</td>\n",
       "      <td>0.211321</td>\n",
       "      <td>0.025126</td>\n",
       "      <td>...</td>\n",
       "      <td>0.208092</td>\n",
       "      <td>0.030953</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.046057</td>\n",
       "      <td>0.191176</td>\n",
       "      <td>0.023887</td>\n",
       "      <td>Mamba</td>\n",
       "      <td>130000000</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.022343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Mamba-2.8b</td>\n",
       "      <td>0.257471</td>\n",
       "      <td>0.006761</td>\n",
       "      <td>0.251375</td>\n",
       "      <td>0.012163</td>\n",
       "      <td>0.251852</td>\n",
       "      <td>0.037499</td>\n",
       "      <td>0.324528</td>\n",
       "      <td>0.028816</td>\n",
       "      <td>...</td>\n",
       "      <td>0.375723</td>\n",
       "      <td>0.036928</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.046057</td>\n",
       "      <td>0.393382</td>\n",
       "      <td>0.029674</td>\n",
       "      <td>Mamba</td>\n",
       "      <td>2800000000</td>\n",
       "      <td>0.734</td>\n",
       "      <td>0.019781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Mamba-370m</td>\n",
       "      <td>0.323213</td>\n",
       "      <td>0.007232</td>\n",
       "      <td>0.270228</td>\n",
       "      <td>0.012451</td>\n",
       "      <td>0.274074</td>\n",
       "      <td>0.038533</td>\n",
       "      <td>0.218868</td>\n",
       "      <td>0.025448</td>\n",
       "      <td>...</td>\n",
       "      <td>0.202312</td>\n",
       "      <td>0.030631</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.046057</td>\n",
       "      <td>0.150735</td>\n",
       "      <td>0.021734</td>\n",
       "      <td>Mamba</td>\n",
       "      <td>370000000</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.022343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Mamba-790m</td>\n",
       "      <td>0.314129</td>\n",
       "      <td>0.007178</td>\n",
       "      <td>0.266300</td>\n",
       "      <td>0.012394</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.034555</td>\n",
       "      <td>0.222642</td>\n",
       "      <td>0.025604</td>\n",
       "      <td>...</td>\n",
       "      <td>0.265896</td>\n",
       "      <td>0.033688</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.047258</td>\n",
       "      <td>0.238971</td>\n",
       "      <td>0.025905</td>\n",
       "      <td>Mamba</td>\n",
       "      <td>790000000</td>\n",
       "      <td>0.660</td>\n",
       "      <td>0.021206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>OLMo-1B</td>\n",
       "      <td>0.262013</td>\n",
       "      <td>0.006800</td>\n",
       "      <td>0.274156</td>\n",
       "      <td>0.012508</td>\n",
       "      <td>0.318519</td>\n",
       "      <td>0.040248</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.024618</td>\n",
       "      <td>...</td>\n",
       "      <td>0.225434</td>\n",
       "      <td>0.031862</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.042923</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.023157</td>\n",
       "      <td>OLMo</td>\n",
       "      <td>1000000000</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.022001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>OLMo-7B</td>\n",
       "      <td>0.240258</td>\n",
       "      <td>0.006607</td>\n",
       "      <td>0.239592</td>\n",
       "      <td>0.011968</td>\n",
       "      <td>0.288889</td>\n",
       "      <td>0.039155</td>\n",
       "      <td>0.267925</td>\n",
       "      <td>0.027257</td>\n",
       "      <td>...</td>\n",
       "      <td>0.329480</td>\n",
       "      <td>0.035839</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.046883</td>\n",
       "      <td>0.216912</td>\n",
       "      <td>0.025036</td>\n",
       "      <td>OLMo</td>\n",
       "      <td>7000000000</td>\n",
       "      <td>0.690</td>\n",
       "      <td>0.020704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Qwen1.5-0.5B</td>\n",
       "      <td>0.329429</td>\n",
       "      <td>0.007268</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.013033</td>\n",
       "      <td>0.348148</td>\n",
       "      <td>0.041153</td>\n",
       "      <td>0.403774</td>\n",
       "      <td>0.030198</td>\n",
       "      <td>...</td>\n",
       "      <td>0.346821</td>\n",
       "      <td>0.036291</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.050091</td>\n",
       "      <td>0.275735</td>\n",
       "      <td>0.027146</td>\n",
       "      <td>Qwen</td>\n",
       "      <td>500000000</td>\n",
       "      <td>0.620</td>\n",
       "      <td>0.021729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>Qwen1.5-1.8B</td>\n",
       "      <td>0.368157</td>\n",
       "      <td>0.007458</td>\n",
       "      <td>0.348782</td>\n",
       "      <td>0.013363</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.042321</td>\n",
       "      <td>0.479245</td>\n",
       "      <td>0.030746</td>\n",
       "      <td>...</td>\n",
       "      <td>0.456647</td>\n",
       "      <td>0.037981</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.049604</td>\n",
       "      <td>0.485294</td>\n",
       "      <td>0.030360</td>\n",
       "      <td>Qwen</td>\n",
       "      <td>1800000000</td>\n",
       "      <td>0.520</td>\n",
       "      <td>0.022365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>Qwen1.5-14B</td>\n",
       "      <td>0.531676</td>\n",
       "      <td>0.007716</td>\n",
       "      <td>0.542812</td>\n",
       "      <td>0.013968</td>\n",
       "      <td>0.651852</td>\n",
       "      <td>0.041153</td>\n",
       "      <td>0.735849</td>\n",
       "      <td>0.027134</td>\n",
       "      <td>...</td>\n",
       "      <td>0.676301</td>\n",
       "      <td>0.035676</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.042295</td>\n",
       "      <td>0.713235</td>\n",
       "      <td>0.027472</td>\n",
       "      <td>Qwen</td>\n",
       "      <td>14000000000</td>\n",
       "      <td>0.764</td>\n",
       "      <td>0.019009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>Qwen1.5-14B</td>\n",
       "      <td>0.531676</td>\n",
       "      <td>0.007716</td>\n",
       "      <td>0.542812</td>\n",
       "      <td>0.013968</td>\n",
       "      <td>0.651852</td>\n",
       "      <td>0.041153</td>\n",
       "      <td>0.735849</td>\n",
       "      <td>0.027134</td>\n",
       "      <td>...</td>\n",
       "      <td>0.676301</td>\n",
       "      <td>0.035676</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.042295</td>\n",
       "      <td>0.713235</td>\n",
       "      <td>0.027472</td>\n",
       "      <td>Qwen</td>\n",
       "      <td>14000000000</td>\n",
       "      <td>0.764</td>\n",
       "      <td>0.019009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>Qwen1.5-4B</td>\n",
       "      <td>0.436768</td>\n",
       "      <td>0.007670</td>\n",
       "      <td>0.414768</td>\n",
       "      <td>0.013814</td>\n",
       "      <td>0.488889</td>\n",
       "      <td>0.043183</td>\n",
       "      <td>0.592453</td>\n",
       "      <td>0.030242</td>\n",
       "      <td>...</td>\n",
       "      <td>0.537572</td>\n",
       "      <td>0.038017</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.049431</td>\n",
       "      <td>0.477941</td>\n",
       "      <td>0.030343</td>\n",
       "      <td>Qwen</td>\n",
       "      <td>4000000000</td>\n",
       "      <td>0.664</td>\n",
       "      <td>0.021145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>Qwen1.5-7B</td>\n",
       "      <td>0.502271</td>\n",
       "      <td>0.007732</td>\n",
       "      <td>0.487824</td>\n",
       "      <td>0.014015</td>\n",
       "      <td>0.518519</td>\n",
       "      <td>0.043164</td>\n",
       "      <td>0.649057</td>\n",
       "      <td>0.029374</td>\n",
       "      <td>...</td>\n",
       "      <td>0.606936</td>\n",
       "      <td>0.037242</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.046482</td>\n",
       "      <td>0.580882</td>\n",
       "      <td>0.029973</td>\n",
       "      <td>Qwen</td>\n",
       "      <td>7000000000</td>\n",
       "      <td>0.678</td>\n",
       "      <td>0.020917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>pythia-1.4b-deduped</td>\n",
       "      <td>0.311499</td>\n",
       "      <td>0.007161</td>\n",
       "      <td>0.257659</td>\n",
       "      <td>0.012263</td>\n",
       "      <td>0.207407</td>\n",
       "      <td>0.035026</td>\n",
       "      <td>0.222642</td>\n",
       "      <td>0.025604</td>\n",
       "      <td>...</td>\n",
       "      <td>0.225434</td>\n",
       "      <td>0.031862</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.042295</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.023710</td>\n",
       "      <td>Pythia</td>\n",
       "      <td>1400000000</td>\n",
       "      <td>0.584</td>\n",
       "      <td>0.022065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>pythia-12b-deduped</td>\n",
       "      <td>0.251255</td>\n",
       "      <td>0.006707</td>\n",
       "      <td>0.245090</td>\n",
       "      <td>0.012061</td>\n",
       "      <td>0.251852</td>\n",
       "      <td>0.037499</td>\n",
       "      <td>0.256604</td>\n",
       "      <td>0.026881</td>\n",
       "      <td>...</td>\n",
       "      <td>0.265896</td>\n",
       "      <td>0.033688</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.048242</td>\n",
       "      <td>0.150735</td>\n",
       "      <td>0.021734</td>\n",
       "      <td>Pythia</td>\n",
       "      <td>12000000000</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.020514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>pythia-12b-deduped</td>\n",
       "      <td>0.251255</td>\n",
       "      <td>0.006707</td>\n",
       "      <td>0.245090</td>\n",
       "      <td>0.012061</td>\n",
       "      <td>0.251852</td>\n",
       "      <td>0.037499</td>\n",
       "      <td>0.256604</td>\n",
       "      <td>0.026881</td>\n",
       "      <td>...</td>\n",
       "      <td>0.265896</td>\n",
       "      <td>0.033688</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.048242</td>\n",
       "      <td>0.150735</td>\n",
       "      <td>0.021734</td>\n",
       "      <td>Pythia</td>\n",
       "      <td>12000000000</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.020514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>pythia-160m-deduped</td>\n",
       "      <td>0.316758</td>\n",
       "      <td>0.007194</td>\n",
       "      <td>0.281225</td>\n",
       "      <td>0.012606</td>\n",
       "      <td>0.192593</td>\n",
       "      <td>0.034065</td>\n",
       "      <td>0.211321</td>\n",
       "      <td>0.025126</td>\n",
       "      <td>...</td>\n",
       "      <td>0.208092</td>\n",
       "      <td>0.030953</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.046057</td>\n",
       "      <td>0.191176</td>\n",
       "      <td>0.023887</td>\n",
       "      <td>Pythia</td>\n",
       "      <td>160000000</td>\n",
       "      <td>0.446</td>\n",
       "      <td>0.022252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>pythia-1b-deduped</td>\n",
       "      <td>0.304566</td>\n",
       "      <td>0.007117</td>\n",
       "      <td>0.237235</td>\n",
       "      <td>0.011927</td>\n",
       "      <td>0.207407</td>\n",
       "      <td>0.035026</td>\n",
       "      <td>0.271698</td>\n",
       "      <td>0.027378</td>\n",
       "      <td>...</td>\n",
       "      <td>0.248555</td>\n",
       "      <td>0.032953</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.046057</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.023710</td>\n",
       "      <td>Pythia</td>\n",
       "      <td>1000000000</td>\n",
       "      <td>0.506</td>\n",
       "      <td>0.022381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>pythia-2.8b-deduped</td>\n",
       "      <td>0.261057</td>\n",
       "      <td>0.006792</td>\n",
       "      <td>0.257659</td>\n",
       "      <td>0.012263</td>\n",
       "      <td>0.281481</td>\n",
       "      <td>0.038850</td>\n",
       "      <td>0.256604</td>\n",
       "      <td>0.026881</td>\n",
       "      <td>...</td>\n",
       "      <td>0.236994</td>\n",
       "      <td>0.032424</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.044620</td>\n",
       "      <td>0.382353</td>\n",
       "      <td>0.029520</td>\n",
       "      <td>Pythia</td>\n",
       "      <td>2800000000</td>\n",
       "      <td>0.640</td>\n",
       "      <td>0.021488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>pythia-410m-deduped</td>\n",
       "      <td>0.320822</td>\n",
       "      <td>0.007218</td>\n",
       "      <td>0.271799</td>\n",
       "      <td>0.012474</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0.033557</td>\n",
       "      <td>0.222642</td>\n",
       "      <td>0.025604</td>\n",
       "      <td>...</td>\n",
       "      <td>0.236994</td>\n",
       "      <td>0.032424</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.046057</td>\n",
       "      <td>0.183824</td>\n",
       "      <td>0.023529</td>\n",
       "      <td>Pythia</td>\n",
       "      <td>410000000</td>\n",
       "      <td>0.542</td>\n",
       "      <td>0.022304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>pythia-6.9b-deduped</td>\n",
       "      <td>0.215396</td>\n",
       "      <td>0.006357</td>\n",
       "      <td>0.215240</td>\n",
       "      <td>0.011524</td>\n",
       "      <td>0.288889</td>\n",
       "      <td>0.039155</td>\n",
       "      <td>0.245283</td>\n",
       "      <td>0.026480</td>\n",
       "      <td>...</td>\n",
       "      <td>0.242775</td>\n",
       "      <td>0.032693</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.044620</td>\n",
       "      <td>0.334559</td>\n",
       "      <td>0.028662</td>\n",
       "      <td>Pythia</td>\n",
       "      <td>6900000000</td>\n",
       "      <td>0.608</td>\n",
       "      <td>0.021855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>pythia-6.9b-deduped</td>\n",
       "      <td>0.215396</td>\n",
       "      <td>0.006357</td>\n",
       "      <td>0.215240</td>\n",
       "      <td>0.011524</td>\n",
       "      <td>0.288889</td>\n",
       "      <td>0.039155</td>\n",
       "      <td>0.245283</td>\n",
       "      <td>0.026480</td>\n",
       "      <td>...</td>\n",
       "      <td>0.242775</td>\n",
       "      <td>0.032693</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.044620</td>\n",
       "      <td>0.334559</td>\n",
       "      <td>0.028662</td>\n",
       "      <td>Pythia</td>\n",
       "      <td>6900000000</td>\n",
       "      <td>0.608</td>\n",
       "      <td>0.021855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>pythia-70m-deduped</td>\n",
       "      <td>0.318671</td>\n",
       "      <td>0.007205</td>\n",
       "      <td>0.274941</td>\n",
       "      <td>0.012519</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0.033557</td>\n",
       "      <td>0.211321</td>\n",
       "      <td>0.025126</td>\n",
       "      <td>...</td>\n",
       "      <td>0.208092</td>\n",
       "      <td>0.030953</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.046057</td>\n",
       "      <td>0.183824</td>\n",
       "      <td>0.023529</td>\n",
       "      <td>Pythia</td>\n",
       "      <td>70000000</td>\n",
       "      <td>0.544</td>\n",
       "      <td>0.022296</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0           model_name  medmcqa/acc  medmcqa/acc_stderr  \\\n",
       "0            0           Mamba-1.4b     0.235477            0.006561   \n",
       "1            1           Mamba-130m     0.320344            0.007215   \n",
       "2            2           Mamba-2.8b     0.257471            0.006761   \n",
       "3            3           Mamba-370m     0.323213            0.007232   \n",
       "4            4           Mamba-790m     0.314129            0.007178   \n",
       "5            5              OLMo-1B     0.262013            0.006800   \n",
       "6            6              OLMo-7B     0.240258            0.006607   \n",
       "7            7         Qwen1.5-0.5B     0.329429            0.007268   \n",
       "8            8         Qwen1.5-1.8B     0.368157            0.007458   \n",
       "9            9          Qwen1.5-14B     0.531676            0.007716   \n",
       "10          10          Qwen1.5-14B     0.531676            0.007716   \n",
       "11          11           Qwen1.5-4B     0.436768            0.007670   \n",
       "12          12           Qwen1.5-7B     0.502271            0.007732   \n",
       "13          13  pythia-1.4b-deduped     0.311499            0.007161   \n",
       "14          14   pythia-12b-deduped     0.251255            0.006707   \n",
       "15          15   pythia-12b-deduped     0.251255            0.006707   \n",
       "16          16  pythia-160m-deduped     0.316758            0.007194   \n",
       "17          17    pythia-1b-deduped     0.304566            0.007117   \n",
       "18          18  pythia-2.8b-deduped     0.261057            0.006792   \n",
       "19          19  pythia-410m-deduped     0.320822            0.007218   \n",
       "20          20  pythia-6.9b-deduped     0.215396            0.006357   \n",
       "21          21  pythia-6.9b-deduped     0.215396            0.006357   \n",
       "22          22   pythia-70m-deduped     0.318671            0.007205   \n",
       "\n",
       "    medqa_4options/acc  medqa_4options/acc_stderr  mmlu_anatomy/acc  \\\n",
       "0             0.228594                   0.011774          0.237037   \n",
       "1             0.276512                   0.012541          0.185185   \n",
       "2             0.251375                   0.012163          0.251852   \n",
       "3             0.270228                   0.012451          0.274074   \n",
       "4             0.266300                   0.012394          0.200000   \n",
       "5             0.274156                   0.012508          0.318519   \n",
       "6             0.239592                   0.011968          0.288889   \n",
       "7             0.315789                   0.013033          0.348148   \n",
       "8             0.348782                   0.013363          0.400000   \n",
       "9             0.542812                   0.013968          0.651852   \n",
       "10            0.542812                   0.013968          0.651852   \n",
       "11            0.414768                   0.013814          0.488889   \n",
       "12            0.487824                   0.014015          0.518519   \n",
       "13            0.257659                   0.012263          0.207407   \n",
       "14            0.245090                   0.012061          0.251852   \n",
       "15            0.245090                   0.012061          0.251852   \n",
       "16            0.281225                   0.012606          0.192593   \n",
       "17            0.237235                   0.011927          0.207407   \n",
       "18            0.257659                   0.012263          0.281481   \n",
       "19            0.271799                   0.012474          0.185185   \n",
       "20            0.215240                   0.011524          0.288889   \n",
       "21            0.215240                   0.011524          0.288889   \n",
       "22            0.274941                   0.012519          0.185185   \n",
       "\n",
       "    mmlu_anatomy/acc_stderr  mmlu_clinical_knowledge/acc  \\\n",
       "0                  0.036737                     0.196226   \n",
       "1                  0.033557                     0.211321   \n",
       "2                  0.037499                     0.324528   \n",
       "3                  0.038533                     0.218868   \n",
       "4                  0.034555                     0.222642   \n",
       "5                  0.040248                     0.200000   \n",
       "6                  0.039155                     0.267925   \n",
       "7                  0.041153                     0.403774   \n",
       "8                  0.042321                     0.479245   \n",
       "9                  0.041153                     0.735849   \n",
       "10                 0.041153                     0.735849   \n",
       "11                 0.043183                     0.592453   \n",
       "12                 0.043164                     0.649057   \n",
       "13                 0.035026                     0.222642   \n",
       "14                 0.037499                     0.256604   \n",
       "15                 0.037499                     0.256604   \n",
       "16                 0.034065                     0.211321   \n",
       "17                 0.035026                     0.271698   \n",
       "18                 0.038850                     0.256604   \n",
       "19                 0.033557                     0.222642   \n",
       "20                 0.039155                     0.245283   \n",
       "21                 0.039155                     0.245283   \n",
       "22                 0.033557                     0.211321   \n",
       "\n",
       "    mmlu_clinical_knowledge/acc_stderr  ...  mmlu_college_medicine/acc  \\\n",
       "0                             0.024442  ...                   0.219653   \n",
       "1                             0.025126  ...                   0.208092   \n",
       "2                             0.028816  ...                   0.375723   \n",
       "3                             0.025448  ...                   0.202312   \n",
       "4                             0.025604  ...                   0.265896   \n",
       "5                             0.024618  ...                   0.225434   \n",
       "6                             0.027257  ...                   0.329480   \n",
       "7                             0.030198  ...                   0.346821   \n",
       "8                             0.030746  ...                   0.456647   \n",
       "9                             0.027134  ...                   0.676301   \n",
       "10                            0.027134  ...                   0.676301   \n",
       "11                            0.030242  ...                   0.537572   \n",
       "12                            0.029374  ...                   0.606936   \n",
       "13                            0.025604  ...                   0.225434   \n",
       "14                            0.026881  ...                   0.265896   \n",
       "15                            0.026881  ...                   0.265896   \n",
       "16                            0.025126  ...                   0.208092   \n",
       "17                            0.027378  ...                   0.248555   \n",
       "18                            0.026881  ...                   0.236994   \n",
       "19                            0.025604  ...                   0.236994   \n",
       "20                            0.026480  ...                   0.242775   \n",
       "21                            0.026480  ...                   0.242775   \n",
       "22                            0.025126  ...                   0.208092   \n",
       "\n",
       "    mmlu_college_medicine/acc_stderr  mmlu_medical_genetics/acc  \\\n",
       "0                           0.031568                       0.30   \n",
       "1                           0.030953                       0.30   \n",
       "2                           0.036928                       0.30   \n",
       "3                           0.030631                       0.30   \n",
       "4                           0.033688                       0.33   \n",
       "5                           0.031862                       0.24   \n",
       "6                           0.035839                       0.32   \n",
       "7                           0.036291                       0.46   \n",
       "8                           0.037981                       0.58   \n",
       "9                           0.035676                       0.77   \n",
       "10                          0.035676                       0.77   \n",
       "11                          0.038017                       0.59   \n",
       "12                          0.037242                       0.69   \n",
       "13                          0.031862                       0.23   \n",
       "14                          0.033688                       0.36   \n",
       "15                          0.033688                       0.36   \n",
       "16                          0.030953                       0.30   \n",
       "17                          0.032953                       0.30   \n",
       "18                          0.032424                       0.27   \n",
       "19                          0.032424                       0.30   \n",
       "20                          0.032693                       0.27   \n",
       "21                          0.032693                       0.27   \n",
       "22                          0.030953                       0.30   \n",
       "\n",
       "    mmlu_medical_genetics/acc_stderr  mmlu_professional_medicine/acc  \\\n",
       "0                           0.046057                        0.250000   \n",
       "1                           0.046057                        0.191176   \n",
       "2                           0.046057                        0.393382   \n",
       "3                           0.046057                        0.150735   \n",
       "4                           0.047258                        0.238971   \n",
       "5                           0.042923                        0.176471   \n",
       "6                           0.046883                        0.216912   \n",
       "7                           0.050091                        0.275735   \n",
       "8                           0.049604                        0.485294   \n",
       "9                           0.042295                        0.713235   \n",
       "10                          0.042295                        0.713235   \n",
       "11                          0.049431                        0.477941   \n",
       "12                          0.046482                        0.580882   \n",
       "13                          0.042295                        0.187500   \n",
       "14                          0.048242                        0.150735   \n",
       "15                          0.048242                        0.150735   \n",
       "16                          0.046057                        0.191176   \n",
       "17                          0.046057                        0.187500   \n",
       "18                          0.044620                        0.382353   \n",
       "19                          0.046057                        0.183824   \n",
       "20                          0.044620                        0.334559   \n",
       "21                          0.044620                        0.334559   \n",
       "22                          0.046057                        0.183824   \n",
       "\n",
       "    mmlu_professional_medicine/acc_stderr  model_family  param_count  \\\n",
       "0                                0.026304         Mamba   1400000000   \n",
       "1                                0.023887         Mamba    130000000   \n",
       "2                                0.029674         Mamba   2800000000   \n",
       "3                                0.021734         Mamba    370000000   \n",
       "4                                0.025905         Mamba    790000000   \n",
       "5                                0.023157          OLMo   1000000000   \n",
       "6                                0.025036          OLMo   7000000000   \n",
       "7                                0.027146          Qwen    500000000   \n",
       "8                                0.030360          Qwen   1800000000   \n",
       "9                                0.027472          Qwen  14000000000   \n",
       "10                               0.027472          Qwen  14000000000   \n",
       "11                               0.030343          Qwen   4000000000   \n",
       "12                               0.029973          Qwen   7000000000   \n",
       "13                               0.023710        Pythia   1400000000   \n",
       "14                               0.021734        Pythia  12000000000   \n",
       "15                               0.021734        Pythia  12000000000   \n",
       "16                               0.023887        Pythia    160000000   \n",
       "17                               0.023710        Pythia   1000000000   \n",
       "18                               0.029520        Pythia   2800000000   \n",
       "19                               0.023529        Pythia    410000000   \n",
       "20                               0.028662        Pythia   6900000000   \n",
       "21                               0.028662        Pythia   6900000000   \n",
       "22                               0.023529        Pythia     70000000   \n",
       "\n",
       "   pubmedqa/acc  pubmedqa/acc_stderr  \n",
       "0         0.652             0.021324  \n",
       "1         0.530             0.022343  \n",
       "2         0.734             0.019781  \n",
       "3         0.530             0.022343  \n",
       "4         0.660             0.021206  \n",
       "5         0.592             0.022001  \n",
       "6         0.690             0.020704  \n",
       "7         0.620             0.021729  \n",
       "8         0.520             0.022365  \n",
       "9         0.764             0.019009  \n",
       "10        0.764             0.019009  \n",
       "11        0.664             0.021145  \n",
       "12        0.678             0.020917  \n",
       "13        0.584             0.022065  \n",
       "14        0.700             0.020514  \n",
       "15        0.700             0.020514  \n",
       "16        0.446             0.022252  \n",
       "17        0.506             0.022381  \n",
       "18        0.640             0.021488  \n",
       "19        0.542             0.022304  \n",
       "20        0.608             0.021855  \n",
       "21        0.608             0.021855  \n",
       "22        0.544             0.022296  \n",
       "\n",
       "[23 rows x 22 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#MultiMedQA evaluations across model scale. Raw eval data cleaned using data-cleaner.py in processing folder\n",
    "import pandas as pd\n",
    "scale_evals = pd.read_csv('../eval-results/wandb-logs/cleaned/acc_scale_results.csv')\n",
    "scale_evals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Family</th>\n",
       "      <th>Task</th>\n",
       "      <th>Slope Coefficient</th>\n",
       "      <th>P-value</th>\n",
       "      <th>R^2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mamba</td>\n",
       "      <td>Medmcqa</td>\n",
       "      <td>-0.095008</td>\n",
       "      <td>0.124749</td>\n",
       "      <td>0.598614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mamba</td>\n",
       "      <td>Medqa 4Options</td>\n",
       "      <td>-0.046384</td>\n",
       "      <td>0.167476</td>\n",
       "      <td>0.522841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mamba</td>\n",
       "      <td>Mmlu Anatomy</td>\n",
       "      <td>0.067453</td>\n",
       "      <td>0.398219</td>\n",
       "      <td>0.243512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mamba</td>\n",
       "      <td>Mmlu Clinical Knowledge</td>\n",
       "      <td>0.095133</td>\n",
       "      <td>0.309719</td>\n",
       "      <td>0.331503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mamba</td>\n",
       "      <td>Mmlu College Biology</td>\n",
       "      <td>-0.084192</td>\n",
       "      <td>0.006800</td>\n",
       "      <td>0.937258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Mamba</td>\n",
       "      <td>Mmlu College Medicine</td>\n",
       "      <td>0.163580</td>\n",
       "      <td>0.138800</td>\n",
       "      <td>0.572384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Mamba</td>\n",
       "      <td>Mmlu Medical Genetics</td>\n",
       "      <td>0.002446</td>\n",
       "      <td>0.913208</td>\n",
       "      <td>0.004654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Mamba</td>\n",
       "      <td>Mmlu Professional Medicine</td>\n",
       "      <td>0.245517</td>\n",
       "      <td>0.090186</td>\n",
       "      <td>0.670091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Mamba</td>\n",
       "      <td>Pubmedqa</td>\n",
       "      <td>0.113127</td>\n",
       "      <td>0.022653</td>\n",
       "      <td>0.862306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Qwen</td>\n",
       "      <td>Medmcqa</td>\n",
       "      <td>0.153585</td>\n",
       "      <td>0.000349</td>\n",
       "      <td>0.969629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Qwen</td>\n",
       "      <td>Medqa 4Options</td>\n",
       "      <td>0.173915</td>\n",
       "      <td>0.000581</td>\n",
       "      <td>0.960888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Qwen</td>\n",
       "      <td>Mmlu Anatomy</td>\n",
       "      <td>0.192322</td>\n",
       "      <td>0.000529</td>\n",
       "      <td>0.962674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Qwen</td>\n",
       "      <td>Mmlu Clinical Knowledge</td>\n",
       "      <td>0.185895</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.990167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Qwen</td>\n",
       "      <td>Mmlu College Biology</td>\n",
       "      <td>0.241821</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.939523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Qwen</td>\n",
       "      <td>Mmlu College Medicine</td>\n",
       "      <td>0.200453</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.997306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Qwen</td>\n",
       "      <td>Mmlu Medical Genetics</td>\n",
       "      <td>0.151379</td>\n",
       "      <td>0.000350</td>\n",
       "      <td>0.969588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Qwen</td>\n",
       "      <td>Mmlu Professional Medicine</td>\n",
       "      <td>0.265157</td>\n",
       "      <td>0.001181</td>\n",
       "      <td>0.944401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Qwen</td>\n",
       "      <td>Pubmedqa</td>\n",
       "      <td>0.083410</td>\n",
       "      <td>0.086506</td>\n",
       "      <td>0.561203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Pythia</td>\n",
       "      <td>Medmcqa</td>\n",
       "      <td>-0.072522</td>\n",
       "      <td>0.003122</td>\n",
       "      <td>0.684911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Pythia</td>\n",
       "      <td>Medqa 4Options</td>\n",
       "      <td>-0.038877</td>\n",
       "      <td>0.012162</td>\n",
       "      <td>0.565111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Pythia</td>\n",
       "      <td>Mmlu Anatomy</td>\n",
       "      <td>0.086625</td>\n",
       "      <td>0.001687</td>\n",
       "      <td>0.728266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Pythia</td>\n",
       "      <td>Mmlu Clinical Knowledge</td>\n",
       "      <td>0.037346</td>\n",
       "      <td>0.011728</td>\n",
       "      <td>0.568789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Pythia</td>\n",
       "      <td>Mmlu College Biology</td>\n",
       "      <td>-0.060905</td>\n",
       "      <td>0.006221</td>\n",
       "      <td>0.628618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Pythia</td>\n",
       "      <td>Mmlu College Medicine</td>\n",
       "      <td>0.040972</td>\n",
       "      <td>0.001049</td>\n",
       "      <td>0.757804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Pythia</td>\n",
       "      <td>Mmlu Medical Genetics</td>\n",
       "      <td>0.013575</td>\n",
       "      <td>0.614117</td>\n",
       "      <td>0.033250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Pythia</td>\n",
       "      <td>Mmlu Professional Medicine</td>\n",
       "      <td>0.041474</td>\n",
       "      <td>0.543804</td>\n",
       "      <td>0.047835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Pythia</td>\n",
       "      <td>Pubmedqa</td>\n",
       "      <td>0.063938</td>\n",
       "      <td>0.003558</td>\n",
       "      <td>0.674895</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Model Family                        Task  Slope Coefficient   P-value  \\\n",
       "0         Mamba                     Medmcqa          -0.095008  0.124749   \n",
       "1         Mamba              Medqa 4Options          -0.046384  0.167476   \n",
       "2         Mamba                Mmlu Anatomy           0.067453  0.398219   \n",
       "3         Mamba     Mmlu Clinical Knowledge           0.095133  0.309719   \n",
       "4         Mamba        Mmlu College Biology          -0.084192  0.006800   \n",
       "5         Mamba       Mmlu College Medicine           0.163580  0.138800   \n",
       "6         Mamba       Mmlu Medical Genetics           0.002446  0.913208   \n",
       "7         Mamba  Mmlu Professional Medicine           0.245517  0.090186   \n",
       "8         Mamba                    Pubmedqa           0.113127  0.022653   \n",
       "9          Qwen                     Medmcqa           0.153585  0.000349   \n",
       "10         Qwen              Medqa 4Options           0.173915  0.000581   \n",
       "11         Qwen                Mmlu Anatomy           0.192322  0.000529   \n",
       "12         Qwen     Mmlu Clinical Knowledge           0.185895  0.000036   \n",
       "13         Qwen        Mmlu College Biology           0.241821  0.001400   \n",
       "14         Qwen       Mmlu College Medicine           0.200453  0.000003   \n",
       "15         Qwen       Mmlu Medical Genetics           0.151379  0.000350   \n",
       "16         Qwen  Mmlu Professional Medicine           0.265157  0.001181   \n",
       "17         Qwen                    Pubmedqa           0.083410  0.086506   \n",
       "18       Pythia                     Medmcqa          -0.072522  0.003122   \n",
       "19       Pythia              Medqa 4Options          -0.038877  0.012162   \n",
       "20       Pythia                Mmlu Anatomy           0.086625  0.001687   \n",
       "21       Pythia     Mmlu Clinical Knowledge           0.037346  0.011728   \n",
       "22       Pythia        Mmlu College Biology          -0.060905  0.006221   \n",
       "23       Pythia       Mmlu College Medicine           0.040972  0.001049   \n",
       "24       Pythia       Mmlu Medical Genetics           0.013575  0.614117   \n",
       "25       Pythia  Mmlu Professional Medicine           0.041474  0.543804   \n",
       "26       Pythia                    Pubmedqa           0.063938  0.003558   \n",
       "\n",
       "         R^2  \n",
       "0   0.598614  \n",
       "1   0.522841  \n",
       "2   0.243512  \n",
       "3   0.331503  \n",
       "4   0.937258  \n",
       "5   0.572384  \n",
       "6   0.004654  \n",
       "7   0.670091  \n",
       "8   0.862306  \n",
       "9   0.969629  \n",
       "10  0.960888  \n",
       "11  0.962674  \n",
       "12  0.990167  \n",
       "13  0.939523  \n",
       "14  0.997306  \n",
       "15  0.969588  \n",
       "16  0.944401  \n",
       "17  0.561203  \n",
       "18  0.684911  \n",
       "19  0.565111  \n",
       "20  0.728266  \n",
       "21  0.568789  \n",
       "22  0.628618  \n",
       "23  0.757804  \n",
       "24  0.033250  \n",
       "25  0.047835  \n",
       "26  0.674895  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Log-Log Regression Analysis of Model Scale vs Task Accuracy\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "data = scale_evals.copy()\n",
    "\n",
    "#remove OLMo from df\n",
    "data = data[data['model_family'] != 'OLMo']\n",
    "\n",
    "data['log_param_count'] = np.log10(data['param_count'])\n",
    "\n",
    "\n",
    "\n",
    "tasks = ['medmcqa/acc', 'medqa_4options/acc', 'mmlu_anatomy/acc', \n",
    "         'mmlu_clinical_knowledge/acc', 'mmlu_college_biology/acc', \n",
    "         'mmlu_college_medicine/acc', 'mmlu_medical_genetics/acc', \n",
    "         'mmlu_professional_medicine/acc', 'pubmedqa/acc']\n",
    "\n",
    "for task in tasks:\n",
    "    data[f'log_{task}'] = np.log10(data[task])\n",
    "\n",
    "# Function to perform log-log regression and return slope, p-value, and R^2\n",
    "def log_log_regression(x, y):\n",
    "    x = sm.add_constant(x)  # Adds a constant term to the predictor\n",
    "    model = sm.OLS(y, x).fit()\n",
    "    return model.params[1], model.pvalues[1], model.rsquared\n",
    "\n",
    "# Prepare the results table\n",
    "results = []\n",
    "\n",
    "for model_family in data['model_family'].unique():\n",
    "    for task in tasks:\n",
    "        task_label = task.replace('/acc', '').replace('_', ' ').title()\n",
    "        subset = data[data['model_family'] == model_family]\n",
    "        x = subset['log_param_count']\n",
    "        y = subset[f'log_{task}']\n",
    "        slope, p_value, r_squared = log_log_regression(x, y)\n",
    "        results.append([model_family, task_label, slope, p_value, r_squared])\n",
    "\n",
    "results_df_table4 = pd.DataFrame(results, columns=['Model Family', 'Task', 'Slope Coefficient', 'P-value', 'R^2'])\n",
    "\n",
    "\n",
    "results_df_table4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Family</th>\n",
       "      <th>Slope Coefficient</th>\n",
       "      <th>P-value</th>\n",
       "      <th>R^2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mamba</td>\n",
       "      <td>0.051297</td>\n",
       "      <td>0.194186</td>\n",
       "      <td>0.480747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Qwen</td>\n",
       "      <td>0.183104</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.989919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pythia</td>\n",
       "      <td>0.012403</td>\n",
       "      <td>0.058374</td>\n",
       "      <td>0.378393</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Model Family  Slope Coefficient   P-value       R^2\n",
       "0        Mamba           0.051297  0.194186  0.480747\n",
       "1         Qwen           0.183104  0.000038  0.989919\n",
       "2       Pythia           0.012403  0.058374  0.378393"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Log-Log Regression Analysis of Model Scale vs Average Task Accuracy\n",
    "data['avg_accuracy'] = data[[f'log_{task}' for task in tasks]].mean(axis=1)\n",
    "\n",
    "# Function to perform log-log regression and return slope, p-value, and R^2 for average accuracy\n",
    "def log_log_avg_accuracy_regression(data):\n",
    "    results = []\n",
    "    for model_family in data['model_family'].unique():\n",
    "        subset = data[data['model_family'] == model_family]\n",
    "        x = subset['log_param_count']\n",
    "        y = subset['avg_accuracy']\n",
    "        slope, p_value, r_squared = log_log_regression(x, y)\n",
    "        results.append([model_family, slope, p_value, r_squared])\n",
    "    return pd.DataFrame(results, columns=['Model Family', 'Slope Coefficient', 'P-value', 'R^2'])\n",
    "\n",
    "# Perform the regression analysis\n",
    "avg_accuracy_results = log_log_avg_accuracy_regression(data)\n",
    "avg_accuracy_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>cli_configs.model_args</th>\n",
       "      <th>medmcqa/acc</th>\n",
       "      <th>medmcqa/acc_stderr</th>\n",
       "      <th>medqa_4options/acc</th>\n",
       "      <th>medqa_4options/acc_stderr</th>\n",
       "      <th>mmlu_anatomy/acc</th>\n",
       "      <th>mmlu_anatomy/acc_stderr</th>\n",
       "      <th>mmlu_clinical_knowledge/acc</th>\n",
       "      <th>mmlu_clinical_knowledge/acc_stderr</th>\n",
       "      <th>mmlu_college_biology/acc</th>\n",
       "      <th>mmlu_college_biology/acc_stderr</th>\n",
       "      <th>mmlu_college_medicine/acc</th>\n",
       "      <th>mmlu_college_medicine/acc_stderr</th>\n",
       "      <th>mmlu_medical_genetics/acc</th>\n",
       "      <th>mmlu_medical_genetics/acc_stderr</th>\n",
       "      <th>mmlu_professional_medicine/acc</th>\n",
       "      <th>mmlu_professional_medicine/acc_stderr</th>\n",
       "      <th>pubmedqa/acc</th>\n",
       "      <th>pubmedqa/acc_stderr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Step128000</td>\n",
       "      <td>pretrained=EleutherAI/pythia-70m-deduped,revis...</td>\n",
       "      <td>0.318193</td>\n",
       "      <td>0.007203</td>\n",
       "      <td>0.277298</td>\n",
       "      <td>0.012552</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0.033557</td>\n",
       "      <td>0.211321</td>\n",
       "      <td>0.025126</td>\n",
       "      <td>0.263889</td>\n",
       "      <td>0.036857</td>\n",
       "      <td>0.213873</td>\n",
       "      <td>0.031265</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.046482</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.023710</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.022318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Step64000</td>\n",
       "      <td>pretrained=EleutherAI/pythia-70m-deduped,revis...</td>\n",
       "      <td>0.318671</td>\n",
       "      <td>0.007205</td>\n",
       "      <td>0.277298</td>\n",
       "      <td>0.012552</td>\n",
       "      <td>0.192593</td>\n",
       "      <td>0.034065</td>\n",
       "      <td>0.215094</td>\n",
       "      <td>0.025288</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0.038010</td>\n",
       "      <td>0.208092</td>\n",
       "      <td>0.030953</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.046482</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.023710</td>\n",
       "      <td>0.536</td>\n",
       "      <td>0.022325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Step32000</td>\n",
       "      <td>pretrained=EleutherAI/pythia-70m-deduped,revis...</td>\n",
       "      <td>0.319866</td>\n",
       "      <td>0.007213</td>\n",
       "      <td>0.278083</td>\n",
       "      <td>0.012563</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0.033557</td>\n",
       "      <td>0.211321</td>\n",
       "      <td>0.025126</td>\n",
       "      <td>0.263889</td>\n",
       "      <td>0.036857</td>\n",
       "      <td>0.208092</td>\n",
       "      <td>0.030953</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.046057</td>\n",
       "      <td>0.191176</td>\n",
       "      <td>0.023887</td>\n",
       "      <td>0.434</td>\n",
       "      <td>0.022187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Step16000</td>\n",
       "      <td>pretrained=EleutherAI/pythia-70m-deduped,revis...</td>\n",
       "      <td>0.319627</td>\n",
       "      <td>0.007211</td>\n",
       "      <td>0.275727</td>\n",
       "      <td>0.012530</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.034555</td>\n",
       "      <td>0.215094</td>\n",
       "      <td>0.025288</td>\n",
       "      <td>0.256944</td>\n",
       "      <td>0.036539</td>\n",
       "      <td>0.208092</td>\n",
       "      <td>0.030953</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.046057</td>\n",
       "      <td>0.183824</td>\n",
       "      <td>0.023529</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.022271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Step8000</td>\n",
       "      <td>pretrained=EleutherAI/pythia-70m-deduped,revis...</td>\n",
       "      <td>0.319388</td>\n",
       "      <td>0.007210</td>\n",
       "      <td>0.279654</td>\n",
       "      <td>0.012585</td>\n",
       "      <td>0.192593</td>\n",
       "      <td>0.034065</td>\n",
       "      <td>0.215094</td>\n",
       "      <td>0.025288</td>\n",
       "      <td>0.263889</td>\n",
       "      <td>0.036857</td>\n",
       "      <td>0.219653</td>\n",
       "      <td>0.031568</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.046482</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.023710</td>\n",
       "      <td>0.554</td>\n",
       "      <td>0.022252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Step4000</td>\n",
       "      <td>pretrained=EleutherAI/pythia-70m-deduped,revis...</td>\n",
       "      <td>0.321061</td>\n",
       "      <td>0.007220</td>\n",
       "      <td>0.277298</td>\n",
       "      <td>0.012552</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0.033557</td>\n",
       "      <td>0.215094</td>\n",
       "      <td>0.025288</td>\n",
       "      <td>0.256944</td>\n",
       "      <td>0.036539</td>\n",
       "      <td>0.213873</td>\n",
       "      <td>0.031265</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.046057</td>\n",
       "      <td>0.183824</td>\n",
       "      <td>0.023529</td>\n",
       "      <td>0.552</td>\n",
       "      <td>0.022262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Step2000</td>\n",
       "      <td>pretrained=EleutherAI/pythia-70m-deduped,revis...</td>\n",
       "      <td>0.314129</td>\n",
       "      <td>0.007178</td>\n",
       "      <td>0.278083</td>\n",
       "      <td>0.012563</td>\n",
       "      <td>0.192593</td>\n",
       "      <td>0.034065</td>\n",
       "      <td>0.215094</td>\n",
       "      <td>0.025288</td>\n",
       "      <td>0.284722</td>\n",
       "      <td>0.037738</td>\n",
       "      <td>0.231214</td>\n",
       "      <td>0.032147</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.046482</td>\n",
       "      <td>0.183824</td>\n",
       "      <td>0.023529</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.022318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Step128000</td>\n",
       "      <td>pretrained=EleutherAI/pythia-160m-deduped,revi...</td>\n",
       "      <td>0.320344</td>\n",
       "      <td>0.007215</td>\n",
       "      <td>0.273370</td>\n",
       "      <td>0.012497</td>\n",
       "      <td>0.192593</td>\n",
       "      <td>0.034065</td>\n",
       "      <td>0.211321</td>\n",
       "      <td>0.025126</td>\n",
       "      <td>0.256944</td>\n",
       "      <td>0.036539</td>\n",
       "      <td>0.208092</td>\n",
       "      <td>0.030953</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.046057</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.023710</td>\n",
       "      <td>0.476</td>\n",
       "      <td>0.022357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Step64000</td>\n",
       "      <td>pretrained=EleutherAI/pythia-160m-deduped,revi...</td>\n",
       "      <td>0.321301</td>\n",
       "      <td>0.007221</td>\n",
       "      <td>0.275727</td>\n",
       "      <td>0.012530</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0.033557</td>\n",
       "      <td>0.215094</td>\n",
       "      <td>0.025288</td>\n",
       "      <td>0.263889</td>\n",
       "      <td>0.036857</td>\n",
       "      <td>0.213873</td>\n",
       "      <td>0.031265</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.046057</td>\n",
       "      <td>0.194853</td>\n",
       "      <td>0.024061</td>\n",
       "      <td>0.404</td>\n",
       "      <td>0.021967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Step32000</td>\n",
       "      <td>pretrained=EleutherAI/pythia-160m-deduped,revi...</td>\n",
       "      <td>0.320344</td>\n",
       "      <td>0.007215</td>\n",
       "      <td>0.279654</td>\n",
       "      <td>0.012585</td>\n",
       "      <td>0.192593</td>\n",
       "      <td>0.034065</td>\n",
       "      <td>0.211321</td>\n",
       "      <td>0.025126</td>\n",
       "      <td>0.263889</td>\n",
       "      <td>0.036857</td>\n",
       "      <td>0.208092</td>\n",
       "      <td>0.030953</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.046057</td>\n",
       "      <td>0.191176</td>\n",
       "      <td>0.023887</td>\n",
       "      <td>0.548</td>\n",
       "      <td>0.022280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Step16000</td>\n",
       "      <td>pretrained=EleutherAI/pythia-160m-deduped,revi...</td>\n",
       "      <td>0.317715</td>\n",
       "      <td>0.007200</td>\n",
       "      <td>0.276512</td>\n",
       "      <td>0.012541</td>\n",
       "      <td>0.192593</td>\n",
       "      <td>0.034065</td>\n",
       "      <td>0.211321</td>\n",
       "      <td>0.025126</td>\n",
       "      <td>0.270833</td>\n",
       "      <td>0.037162</td>\n",
       "      <td>0.213873</td>\n",
       "      <td>0.031265</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.046057</td>\n",
       "      <td>0.194853</td>\n",
       "      <td>0.024061</td>\n",
       "      <td>0.554</td>\n",
       "      <td>0.022252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Step8000</td>\n",
       "      <td>pretrained=EleutherAI/pythia-160m-deduped,revi...</td>\n",
       "      <td>0.319388</td>\n",
       "      <td>0.007210</td>\n",
       "      <td>0.275727</td>\n",
       "      <td>0.012530</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0.033557</td>\n",
       "      <td>0.218868</td>\n",
       "      <td>0.025448</td>\n",
       "      <td>0.270833</td>\n",
       "      <td>0.037162</td>\n",
       "      <td>0.213873</td>\n",
       "      <td>0.031265</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.046057</td>\n",
       "      <td>0.191176</td>\n",
       "      <td>0.023887</td>\n",
       "      <td>0.552</td>\n",
       "      <td>0.022262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Step4000</td>\n",
       "      <td>pretrained=EleutherAI/pythia-160m-deduped,revi...</td>\n",
       "      <td>0.321301</td>\n",
       "      <td>0.007221</td>\n",
       "      <td>0.275727</td>\n",
       "      <td>0.012530</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0.033557</td>\n",
       "      <td>0.218868</td>\n",
       "      <td>0.025448</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.037456</td>\n",
       "      <td>0.208092</td>\n",
       "      <td>0.030953</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.046883</td>\n",
       "      <td>0.191176</td>\n",
       "      <td>0.023887</td>\n",
       "      <td>0.558</td>\n",
       "      <td>0.022232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Step2000</td>\n",
       "      <td>pretrained=EleutherAI/pythia-160m-deduped,revi...</td>\n",
       "      <td>0.321301</td>\n",
       "      <td>0.007221</td>\n",
       "      <td>0.277298</td>\n",
       "      <td>0.012552</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0.033557</td>\n",
       "      <td>0.211321</td>\n",
       "      <td>0.025126</td>\n",
       "      <td>0.256944</td>\n",
       "      <td>0.036539</td>\n",
       "      <td>0.208092</td>\n",
       "      <td>0.030953</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.046057</td>\n",
       "      <td>0.183824</td>\n",
       "      <td>0.023529</td>\n",
       "      <td>0.540</td>\n",
       "      <td>0.022311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Step128000</td>\n",
       "      <td>pretrained=EleutherAI/pythia-410m-deduped,revi...</td>\n",
       "      <td>0.320105</td>\n",
       "      <td>0.007214</td>\n",
       "      <td>0.273370</td>\n",
       "      <td>0.012497</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0.033557</td>\n",
       "      <td>0.226415</td>\n",
       "      <td>0.025758</td>\n",
       "      <td>0.256944</td>\n",
       "      <td>0.036539</td>\n",
       "      <td>0.184971</td>\n",
       "      <td>0.029606</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.046057</td>\n",
       "      <td>0.183824</td>\n",
       "      <td>0.023529</td>\n",
       "      <td>0.514</td>\n",
       "      <td>0.022374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Step64000</td>\n",
       "      <td>pretrained=EleutherAI/pythia-410m-deduped,revi...</td>\n",
       "      <td>0.318671</td>\n",
       "      <td>0.007205</td>\n",
       "      <td>0.276512</td>\n",
       "      <td>0.012541</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0.033557</td>\n",
       "      <td>0.215094</td>\n",
       "      <td>0.025288</td>\n",
       "      <td>0.256944</td>\n",
       "      <td>0.036539</td>\n",
       "      <td>0.202312</td>\n",
       "      <td>0.030631</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.046057</td>\n",
       "      <td>0.183824</td>\n",
       "      <td>0.023529</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.022343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Step32000</td>\n",
       "      <td>pretrained=EleutherAI/pythia-410m-deduped,revi...</td>\n",
       "      <td>0.318432</td>\n",
       "      <td>0.007204</td>\n",
       "      <td>0.274941</td>\n",
       "      <td>0.012519</td>\n",
       "      <td>0.192593</td>\n",
       "      <td>0.034065</td>\n",
       "      <td>0.215094</td>\n",
       "      <td>0.025288</td>\n",
       "      <td>0.256944</td>\n",
       "      <td>0.036539</td>\n",
       "      <td>0.213873</td>\n",
       "      <td>0.031265</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.046057</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.023710</td>\n",
       "      <td>0.468</td>\n",
       "      <td>0.022337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Step16000</td>\n",
       "      <td>pretrained=EleutherAI/pythia-410m-deduped,revi...</td>\n",
       "      <td>0.308630</td>\n",
       "      <td>0.007143</td>\n",
       "      <td>0.277298</td>\n",
       "      <td>0.012552</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0.033557</td>\n",
       "      <td>0.222642</td>\n",
       "      <td>0.025604</td>\n",
       "      <td>0.270833</td>\n",
       "      <td>0.037162</td>\n",
       "      <td>0.202312</td>\n",
       "      <td>0.030631</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.046482</td>\n",
       "      <td>0.224265</td>\n",
       "      <td>0.025337</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.022365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Step8000</td>\n",
       "      <td>pretrained=EleutherAI/pythia-410m-deduped,revi...</td>\n",
       "      <td>0.311021</td>\n",
       "      <td>0.007158</td>\n",
       "      <td>0.276512</td>\n",
       "      <td>0.012541</td>\n",
       "      <td>0.214815</td>\n",
       "      <td>0.035479</td>\n",
       "      <td>0.215094</td>\n",
       "      <td>0.025288</td>\n",
       "      <td>0.284722</td>\n",
       "      <td>0.037738</td>\n",
       "      <td>0.219653</td>\n",
       "      <td>0.031568</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.045605</td>\n",
       "      <td>0.194853</td>\n",
       "      <td>0.024061</td>\n",
       "      <td>0.380</td>\n",
       "      <td>0.021729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Step4000</td>\n",
       "      <td>pretrained=EleutherAI/pythia-410m-deduped,revi...</td>\n",
       "      <td>0.316758</td>\n",
       "      <td>0.007194</td>\n",
       "      <td>0.276512</td>\n",
       "      <td>0.012541</td>\n",
       "      <td>0.192593</td>\n",
       "      <td>0.034065</td>\n",
       "      <td>0.218868</td>\n",
       "      <td>0.025448</td>\n",
       "      <td>0.263889</td>\n",
       "      <td>0.036857</td>\n",
       "      <td>0.219653</td>\n",
       "      <td>0.031568</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.045605</td>\n",
       "      <td>0.198529</td>\n",
       "      <td>0.024231</td>\n",
       "      <td>0.556</td>\n",
       "      <td>0.022242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Step2000</td>\n",
       "      <td>pretrained=EleutherAI/pythia-410m-deduped,revi...</td>\n",
       "      <td>0.322496</td>\n",
       "      <td>0.007228</td>\n",
       "      <td>0.277298</td>\n",
       "      <td>0.012552</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0.033557</td>\n",
       "      <td>0.215094</td>\n",
       "      <td>0.025288</td>\n",
       "      <td>0.256944</td>\n",
       "      <td>0.036539</td>\n",
       "      <td>0.208092</td>\n",
       "      <td>0.030953</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.046057</td>\n",
       "      <td>0.183824</td>\n",
       "      <td>0.023529</td>\n",
       "      <td>0.502</td>\n",
       "      <td>0.022383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Step128000</td>\n",
       "      <td>pretrained=EleutherAI/pythia-1b-deduped,revisi...</td>\n",
       "      <td>0.316280</td>\n",
       "      <td>0.007191</td>\n",
       "      <td>0.242734</td>\n",
       "      <td>0.012021</td>\n",
       "      <td>0.214815</td>\n",
       "      <td>0.035479</td>\n",
       "      <td>0.245283</td>\n",
       "      <td>0.026480</td>\n",
       "      <td>0.263889</td>\n",
       "      <td>0.036857</td>\n",
       "      <td>0.196532</td>\n",
       "      <td>0.030300</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.045126</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.023710</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.022343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Step64000</td>\n",
       "      <td>pretrained=EleutherAI/pythia-1b-deduped,revisi...</td>\n",
       "      <td>0.313172</td>\n",
       "      <td>0.007172</td>\n",
       "      <td>0.246661</td>\n",
       "      <td>0.012087</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.034555</td>\n",
       "      <td>0.237736</td>\n",
       "      <td>0.026200</td>\n",
       "      <td>0.229167</td>\n",
       "      <td>0.035147</td>\n",
       "      <td>0.271676</td>\n",
       "      <td>0.033918</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.045605</td>\n",
       "      <td>0.264706</td>\n",
       "      <td>0.026800</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.022109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Step32000</td>\n",
       "      <td>pretrained=EleutherAI/pythia-1b-deduped,revisi...</td>\n",
       "      <td>0.293569</td>\n",
       "      <td>0.007042</td>\n",
       "      <td>0.219167</td>\n",
       "      <td>0.011599</td>\n",
       "      <td>0.192593</td>\n",
       "      <td>0.034065</td>\n",
       "      <td>0.313208</td>\n",
       "      <td>0.028545</td>\n",
       "      <td>0.215278</td>\n",
       "      <td>0.034371</td>\n",
       "      <td>0.254335</td>\n",
       "      <td>0.033206</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.044620</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.028156</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.022271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Step16000</td>\n",
       "      <td>pretrained=EleutherAI/pythia-1b-deduped,revisi...</td>\n",
       "      <td>0.292613</td>\n",
       "      <td>0.007035</td>\n",
       "      <td>0.274941</td>\n",
       "      <td>0.012519</td>\n",
       "      <td>0.288889</td>\n",
       "      <td>0.039155</td>\n",
       "      <td>0.222642</td>\n",
       "      <td>0.025604</td>\n",
       "      <td>0.284722</td>\n",
       "      <td>0.037738</td>\n",
       "      <td>0.248555</td>\n",
       "      <td>0.032953</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.045605</td>\n",
       "      <td>0.180147</td>\n",
       "      <td>0.023345</td>\n",
       "      <td>0.554</td>\n",
       "      <td>0.022252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Step8000</td>\n",
       "      <td>pretrained=EleutherAI/pythia-1b-deduped,revisi...</td>\n",
       "      <td>0.310782</td>\n",
       "      <td>0.007157</td>\n",
       "      <td>0.268657</td>\n",
       "      <td>0.012428</td>\n",
       "      <td>0.237037</td>\n",
       "      <td>0.036737</td>\n",
       "      <td>0.218868</td>\n",
       "      <td>0.025448</td>\n",
       "      <td>0.298611</td>\n",
       "      <td>0.038271</td>\n",
       "      <td>0.196532</td>\n",
       "      <td>0.030300</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.044620</td>\n",
       "      <td>0.202206</td>\n",
       "      <td>0.024398</td>\n",
       "      <td>0.544</td>\n",
       "      <td>0.022296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Step4000</td>\n",
       "      <td>pretrained=EleutherAI/pythia-1b-deduped,revisi...</td>\n",
       "      <td>0.322018</td>\n",
       "      <td>0.007225</td>\n",
       "      <td>0.277298</td>\n",
       "      <td>0.012552</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0.033557</td>\n",
       "      <td>0.215094</td>\n",
       "      <td>0.025288</td>\n",
       "      <td>0.256944</td>\n",
       "      <td>0.036539</td>\n",
       "      <td>0.213873</td>\n",
       "      <td>0.031265</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.046057</td>\n",
       "      <td>0.183824</td>\n",
       "      <td>0.023529</td>\n",
       "      <td>0.540</td>\n",
       "      <td>0.022311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Step2000</td>\n",
       "      <td>pretrained=EleutherAI/pythia-1b-deduped,revisi...</td>\n",
       "      <td>0.321779</td>\n",
       "      <td>0.007224</td>\n",
       "      <td>0.276512</td>\n",
       "      <td>0.012541</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0.033557</td>\n",
       "      <td>0.215094</td>\n",
       "      <td>0.025288</td>\n",
       "      <td>0.263889</td>\n",
       "      <td>0.036857</td>\n",
       "      <td>0.219653</td>\n",
       "      <td>0.031568</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.046057</td>\n",
       "      <td>0.194853</td>\n",
       "      <td>0.024061</td>\n",
       "      <td>0.552</td>\n",
       "      <td>0.022262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Step128000</td>\n",
       "      <td>pretrained=EleutherAI/pythia-2.8b-deduped,revi...</td>\n",
       "      <td>0.252211</td>\n",
       "      <td>0.006716</td>\n",
       "      <td>0.252946</td>\n",
       "      <td>0.012188</td>\n",
       "      <td>0.229630</td>\n",
       "      <td>0.036334</td>\n",
       "      <td>0.286792</td>\n",
       "      <td>0.027835</td>\n",
       "      <td>0.236111</td>\n",
       "      <td>0.035514</td>\n",
       "      <td>0.254335</td>\n",
       "      <td>0.033206</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.046057</td>\n",
       "      <td>0.272059</td>\n",
       "      <td>0.027033</td>\n",
       "      <td>0.632</td>\n",
       "      <td>0.021589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Step64000</td>\n",
       "      <td>pretrained=EleutherAI/pythia-2.8b-deduped,revi...</td>\n",
       "      <td>0.273488</td>\n",
       "      <td>0.006893</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.012347</td>\n",
       "      <td>0.318519</td>\n",
       "      <td>0.040248</td>\n",
       "      <td>0.226415</td>\n",
       "      <td>0.025758</td>\n",
       "      <td>0.284722</td>\n",
       "      <td>0.037738</td>\n",
       "      <td>0.225434</td>\n",
       "      <td>0.031862</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.044620</td>\n",
       "      <td>0.180147</td>\n",
       "      <td>0.023345</td>\n",
       "      <td>0.642</td>\n",
       "      <td>0.021461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Step32000</td>\n",
       "      <td>pretrained=EleutherAI/pythia-2.8b-deduped,revi...</td>\n",
       "      <td>0.312455</td>\n",
       "      <td>0.007167</td>\n",
       "      <td>0.257659</td>\n",
       "      <td>0.012263</td>\n",
       "      <td>0.251852</td>\n",
       "      <td>0.037499</td>\n",
       "      <td>0.271698</td>\n",
       "      <td>0.027378</td>\n",
       "      <td>0.326389</td>\n",
       "      <td>0.039211</td>\n",
       "      <td>0.248555</td>\n",
       "      <td>0.032953</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.045126</td>\n",
       "      <td>0.194853</td>\n",
       "      <td>0.024061</td>\n",
       "      <td>0.574</td>\n",
       "      <td>0.022137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Step16000</td>\n",
       "      <td>pretrained=EleutherAI/pythia-2.8b-deduped,revi...</td>\n",
       "      <td>0.311260</td>\n",
       "      <td>0.007160</td>\n",
       "      <td>0.270228</td>\n",
       "      <td>0.012451</td>\n",
       "      <td>0.207407</td>\n",
       "      <td>0.035026</td>\n",
       "      <td>0.230189</td>\n",
       "      <td>0.025908</td>\n",
       "      <td>0.326389</td>\n",
       "      <td>0.039211</td>\n",
       "      <td>0.225434</td>\n",
       "      <td>0.031862</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.043519</td>\n",
       "      <td>0.169118</td>\n",
       "      <td>0.022771</td>\n",
       "      <td>0.562</td>\n",
       "      <td>0.022210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Step8000</td>\n",
       "      <td>pretrained=EleutherAI/pythia-2.8b-deduped,revi...</td>\n",
       "      <td>0.286158</td>\n",
       "      <td>0.006989</td>\n",
       "      <td>0.271013</td>\n",
       "      <td>0.012463</td>\n",
       "      <td>0.244444</td>\n",
       "      <td>0.037125</td>\n",
       "      <td>0.233962</td>\n",
       "      <td>0.026055</td>\n",
       "      <td>0.305556</td>\n",
       "      <td>0.038521</td>\n",
       "      <td>0.231214</td>\n",
       "      <td>0.032147</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.042295</td>\n",
       "      <td>0.169118</td>\n",
       "      <td>0.022771</td>\n",
       "      <td>0.554</td>\n",
       "      <td>0.022252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Step4000</td>\n",
       "      <td>pretrained=EleutherAI/pythia-2.8b-deduped,revi...</td>\n",
       "      <td>0.321061</td>\n",
       "      <td>0.007220</td>\n",
       "      <td>0.277298</td>\n",
       "      <td>0.012552</td>\n",
       "      <td>0.192593</td>\n",
       "      <td>0.034065</td>\n",
       "      <td>0.215094</td>\n",
       "      <td>0.025288</td>\n",
       "      <td>0.263889</td>\n",
       "      <td>0.036857</td>\n",
       "      <td>0.208092</td>\n",
       "      <td>0.030953</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.046057</td>\n",
       "      <td>0.180147</td>\n",
       "      <td>0.023345</td>\n",
       "      <td>0.492</td>\n",
       "      <td>0.022380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Step2000</td>\n",
       "      <td>pretrained=EleutherAI/pythia-2.8b-deduped,revi...</td>\n",
       "      <td>0.322018</td>\n",
       "      <td>0.007225</td>\n",
       "      <td>0.276512</td>\n",
       "      <td>0.012541</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0.033557</td>\n",
       "      <td>0.215094</td>\n",
       "      <td>0.025288</td>\n",
       "      <td>0.270833</td>\n",
       "      <td>0.037162</td>\n",
       "      <td>0.213873</td>\n",
       "      <td>0.031265</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.046057</td>\n",
       "      <td>0.191176</td>\n",
       "      <td>0.023887</td>\n",
       "      <td>0.522</td>\n",
       "      <td>0.022361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Step128000</td>\n",
       "      <td>pretrained=EleutherAI/pythia-6.9b-deduped,revi...</td>\n",
       "      <td>0.276596</td>\n",
       "      <td>0.006917</td>\n",
       "      <td>0.249804</td>\n",
       "      <td>0.012138</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.034555</td>\n",
       "      <td>0.245283</td>\n",
       "      <td>0.026480</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.036210</td>\n",
       "      <td>0.173410</td>\n",
       "      <td>0.028868</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.046883</td>\n",
       "      <td>0.194853</td>\n",
       "      <td>0.024061</td>\n",
       "      <td>0.622</td>\n",
       "      <td>0.021707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Step64000</td>\n",
       "      <td>pretrained=EleutherAI/pythia-6.9b-deduped,revi...</td>\n",
       "      <td>0.256514</td>\n",
       "      <td>0.006753</td>\n",
       "      <td>0.273370</td>\n",
       "      <td>0.012497</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0.033557</td>\n",
       "      <td>0.218868</td>\n",
       "      <td>0.025448</td>\n",
       "      <td>0.256944</td>\n",
       "      <td>0.036539</td>\n",
       "      <td>0.219653</td>\n",
       "      <td>0.031568</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.046057</td>\n",
       "      <td>0.194853</td>\n",
       "      <td>0.024061</td>\n",
       "      <td>0.638</td>\n",
       "      <td>0.021514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Step32000</td>\n",
       "      <td>pretrained=EleutherAI/pythia-6.9b-deduped,revi...</td>\n",
       "      <td>0.223046</td>\n",
       "      <td>0.006437</td>\n",
       "      <td>0.267871</td>\n",
       "      <td>0.012417</td>\n",
       "      <td>0.229630</td>\n",
       "      <td>0.036334</td>\n",
       "      <td>0.230189</td>\n",
       "      <td>0.025908</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.037456</td>\n",
       "      <td>0.236994</td>\n",
       "      <td>0.032424</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.044620</td>\n",
       "      <td>0.180147</td>\n",
       "      <td>0.023345</td>\n",
       "      <td>0.614</td>\n",
       "      <td>0.021794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Step16000</td>\n",
       "      <td>pretrained=EleutherAI/pythia-6.9b-deduped,revi...</td>\n",
       "      <td>0.240736</td>\n",
       "      <td>0.006611</td>\n",
       "      <td>0.235664</td>\n",
       "      <td>0.011900</td>\n",
       "      <td>0.281481</td>\n",
       "      <td>0.038850</td>\n",
       "      <td>0.218868</td>\n",
       "      <td>0.025448</td>\n",
       "      <td>0.229167</td>\n",
       "      <td>0.035147</td>\n",
       "      <td>0.254335</td>\n",
       "      <td>0.033206</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.042923</td>\n",
       "      <td>0.209559</td>\n",
       "      <td>0.024723</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.022163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Step8000</td>\n",
       "      <td>pretrained=EleutherAI/pythia-6.9b-deduped,revi...</td>\n",
       "      <td>0.314368</td>\n",
       "      <td>0.007179</td>\n",
       "      <td>0.276512</td>\n",
       "      <td>0.012541</td>\n",
       "      <td>0.251852</td>\n",
       "      <td>0.037499</td>\n",
       "      <td>0.249057</td>\n",
       "      <td>0.026616</td>\n",
       "      <td>0.298611</td>\n",
       "      <td>0.038271</td>\n",
       "      <td>0.219653</td>\n",
       "      <td>0.031568</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.047258</td>\n",
       "      <td>0.180147</td>\n",
       "      <td>0.023345</td>\n",
       "      <td>0.566</td>\n",
       "      <td>0.022187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Step4000</td>\n",
       "      <td>pretrained=EleutherAI/pythia-6.9b-deduped,revi...</td>\n",
       "      <td>0.313890</td>\n",
       "      <td>0.007176</td>\n",
       "      <td>0.276512</td>\n",
       "      <td>0.012541</td>\n",
       "      <td>0.177778</td>\n",
       "      <td>0.033028</td>\n",
       "      <td>0.222642</td>\n",
       "      <td>0.025604</td>\n",
       "      <td>0.256944</td>\n",
       "      <td>0.036539</td>\n",
       "      <td>0.208092</td>\n",
       "      <td>0.030953</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.046482</td>\n",
       "      <td>0.209559</td>\n",
       "      <td>0.024723</td>\n",
       "      <td>0.498</td>\n",
       "      <td>0.022383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Step2000</td>\n",
       "      <td>pretrained=EleutherAI/pythia-6.9b-deduped,revi...</td>\n",
       "      <td>0.321301</td>\n",
       "      <td>0.007221</td>\n",
       "      <td>0.277298</td>\n",
       "      <td>0.012552</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0.033557</td>\n",
       "      <td>0.215094</td>\n",
       "      <td>0.025288</td>\n",
       "      <td>0.256944</td>\n",
       "      <td>0.036539</td>\n",
       "      <td>0.213873</td>\n",
       "      <td>0.031265</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.046057</td>\n",
       "      <td>0.180147</td>\n",
       "      <td>0.023345</td>\n",
       "      <td>0.548</td>\n",
       "      <td>0.022280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Step512000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.237150</td>\n",
       "      <td>0.006577</td>\n",
       "      <td>0.218382</td>\n",
       "      <td>0.011584</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.040723</td>\n",
       "      <td>0.249057</td>\n",
       "      <td>0.026616</td>\n",
       "      <td>0.256944</td>\n",
       "      <td>0.036539</td>\n",
       "      <td>0.341040</td>\n",
       "      <td>0.036147</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.046057</td>\n",
       "      <td>0.224265</td>\n",
       "      <td>0.025337</td>\n",
       "      <td>0.698</td>\n",
       "      <td>0.020553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Step256000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.261774</td>\n",
       "      <td>0.006798</td>\n",
       "      <td>0.225452</td>\n",
       "      <td>0.011717</td>\n",
       "      <td>0.244444</td>\n",
       "      <td>0.037125</td>\n",
       "      <td>0.241509</td>\n",
       "      <td>0.026341</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.033961</td>\n",
       "      <td>0.213873</td>\n",
       "      <td>0.031265</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.043519</td>\n",
       "      <td>0.202206</td>\n",
       "      <td>0.024398</td>\n",
       "      <td>0.682</td>\n",
       "      <td>0.020848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Step128000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.301219</td>\n",
       "      <td>0.007094</td>\n",
       "      <td>0.269442</td>\n",
       "      <td>0.012440</td>\n",
       "      <td>0.214815</td>\n",
       "      <td>0.035479</td>\n",
       "      <td>0.237736</td>\n",
       "      <td>0.026200</td>\n",
       "      <td>0.229167</td>\n",
       "      <td>0.035147</td>\n",
       "      <td>0.260116</td>\n",
       "      <td>0.033450</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.047610</td>\n",
       "      <td>0.194853</td>\n",
       "      <td>0.024061</td>\n",
       "      <td>0.640</td>\n",
       "      <td>0.021488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Step64000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.276596</td>\n",
       "      <td>0.006917</td>\n",
       "      <td>0.270228</td>\n",
       "      <td>0.012451</td>\n",
       "      <td>0.311111</td>\n",
       "      <td>0.039993</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.024618</td>\n",
       "      <td>0.284722</td>\n",
       "      <td>0.037738</td>\n",
       "      <td>0.225434</td>\n",
       "      <td>0.031862</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.044084</td>\n",
       "      <td>0.161765</td>\n",
       "      <td>0.022369</td>\n",
       "      <td>0.598</td>\n",
       "      <td>0.021949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Step32000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.323691</td>\n",
       "      <td>0.007235</td>\n",
       "      <td>0.277298</td>\n",
       "      <td>0.012552</td>\n",
       "      <td>0.237037</td>\n",
       "      <td>0.036737</td>\n",
       "      <td>0.207547</td>\n",
       "      <td>0.024960</td>\n",
       "      <td>0.236111</td>\n",
       "      <td>0.035514</td>\n",
       "      <td>0.248555</td>\n",
       "      <td>0.032953</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.047937</td>\n",
       "      <td>0.238971</td>\n",
       "      <td>0.025905</td>\n",
       "      <td>0.556</td>\n",
       "      <td>0.022242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Step16000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.307674</td>\n",
       "      <td>0.007137</td>\n",
       "      <td>0.271799</td>\n",
       "      <td>0.012474</td>\n",
       "      <td>0.177778</td>\n",
       "      <td>0.033028</td>\n",
       "      <td>0.222642</td>\n",
       "      <td>0.025604</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.036210</td>\n",
       "      <td>0.231214</td>\n",
       "      <td>0.032147</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.046482</td>\n",
       "      <td>0.202206</td>\n",
       "      <td>0.024398</td>\n",
       "      <td>0.532</td>\n",
       "      <td>0.022337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Step8000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.312216</td>\n",
       "      <td>0.007166</td>\n",
       "      <td>0.277298</td>\n",
       "      <td>0.012552</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.034555</td>\n",
       "      <td>0.215094</td>\n",
       "      <td>0.025288</td>\n",
       "      <td>0.256944</td>\n",
       "      <td>0.036539</td>\n",
       "      <td>0.213873</td>\n",
       "      <td>0.031265</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.046057</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.023710</td>\n",
       "      <td>0.504</td>\n",
       "      <td>0.022382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Step4000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.299307</td>\n",
       "      <td>0.007082</td>\n",
       "      <td>0.272584</td>\n",
       "      <td>0.012485</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.035914</td>\n",
       "      <td>0.222642</td>\n",
       "      <td>0.025604</td>\n",
       "      <td>0.270833</td>\n",
       "      <td>0.037162</td>\n",
       "      <td>0.219653</td>\n",
       "      <td>0.031568</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.046057</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.023710</td>\n",
       "      <td>0.452</td>\n",
       "      <td>0.022280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Step2000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.308391</td>\n",
       "      <td>0.007141</td>\n",
       "      <td>0.248233</td>\n",
       "      <td>0.012112</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.034555</td>\n",
       "      <td>0.211321</td>\n",
       "      <td>0.025126</td>\n",
       "      <td>0.256944</td>\n",
       "      <td>0.036539</td>\n",
       "      <td>0.196532</td>\n",
       "      <td>0.030300</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.046057</td>\n",
       "      <td>0.205882</td>\n",
       "      <td>0.024562</td>\n",
       "      <td>0.422</td>\n",
       "      <td>0.022109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Step1000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.298829</td>\n",
       "      <td>0.007078</td>\n",
       "      <td>0.276512</td>\n",
       "      <td>0.012541</td>\n",
       "      <td>0.192593</td>\n",
       "      <td>0.034065</td>\n",
       "      <td>0.215094</td>\n",
       "      <td>0.025288</td>\n",
       "      <td>0.256944</td>\n",
       "      <td>0.036539</td>\n",
       "      <td>0.213873</td>\n",
       "      <td>0.031265</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.046482</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.023710</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.021488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Step0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.261774</td>\n",
       "      <td>0.006798</td>\n",
       "      <td>0.235664</td>\n",
       "      <td>0.011900</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.040723</td>\n",
       "      <td>0.260377</td>\n",
       "      <td>0.027009</td>\n",
       "      <td>0.194444</td>\n",
       "      <td>0.033096</td>\n",
       "      <td>0.231214</td>\n",
       "      <td>0.032147</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.042923</td>\n",
       "      <td>0.180147</td>\n",
       "      <td>0.023345</td>\n",
       "      <td>0.382</td>\n",
       "      <td>0.021751</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Name                             cli_configs.model_args  \\\n",
       "0   Step128000  pretrained=EleutherAI/pythia-70m-deduped,revis...   \n",
       "1    Step64000  pretrained=EleutherAI/pythia-70m-deduped,revis...   \n",
       "2    Step32000  pretrained=EleutherAI/pythia-70m-deduped,revis...   \n",
       "3    Step16000  pretrained=EleutherAI/pythia-70m-deduped,revis...   \n",
       "4     Step8000  pretrained=EleutherAI/pythia-70m-deduped,revis...   \n",
       "5     Step4000  pretrained=EleutherAI/pythia-70m-deduped,revis...   \n",
       "6     Step2000  pretrained=EleutherAI/pythia-70m-deduped,revis...   \n",
       "0   Step128000  pretrained=EleutherAI/pythia-160m-deduped,revi...   \n",
       "1    Step64000  pretrained=EleutherAI/pythia-160m-deduped,revi...   \n",
       "2    Step32000  pretrained=EleutherAI/pythia-160m-deduped,revi...   \n",
       "3    Step16000  pretrained=EleutherAI/pythia-160m-deduped,revi...   \n",
       "4     Step8000  pretrained=EleutherAI/pythia-160m-deduped,revi...   \n",
       "5     Step4000  pretrained=EleutherAI/pythia-160m-deduped,revi...   \n",
       "6     Step2000  pretrained=EleutherAI/pythia-160m-deduped,revi...   \n",
       "0   Step128000  pretrained=EleutherAI/pythia-410m-deduped,revi...   \n",
       "1    Step64000  pretrained=EleutherAI/pythia-410m-deduped,revi...   \n",
       "2    Step32000  pretrained=EleutherAI/pythia-410m-deduped,revi...   \n",
       "3    Step16000  pretrained=EleutherAI/pythia-410m-deduped,revi...   \n",
       "4     Step8000  pretrained=EleutherAI/pythia-410m-deduped,revi...   \n",
       "5     Step4000  pretrained=EleutherAI/pythia-410m-deduped,revi...   \n",
       "6     Step2000  pretrained=EleutherAI/pythia-410m-deduped,revi...   \n",
       "0   Step128000  pretrained=EleutherAI/pythia-1b-deduped,revisi...   \n",
       "1    Step64000  pretrained=EleutherAI/pythia-1b-deduped,revisi...   \n",
       "2    Step32000  pretrained=EleutherAI/pythia-1b-deduped,revisi...   \n",
       "3    Step16000  pretrained=EleutherAI/pythia-1b-deduped,revisi...   \n",
       "4     Step8000  pretrained=EleutherAI/pythia-1b-deduped,revisi...   \n",
       "5     Step4000  pretrained=EleutherAI/pythia-1b-deduped,revisi...   \n",
       "6     Step2000  pretrained=EleutherAI/pythia-1b-deduped,revisi...   \n",
       "0   Step128000  pretrained=EleutherAI/pythia-2.8b-deduped,revi...   \n",
       "1    Step64000  pretrained=EleutherAI/pythia-2.8b-deduped,revi...   \n",
       "2    Step32000  pretrained=EleutherAI/pythia-2.8b-deduped,revi...   \n",
       "3    Step16000  pretrained=EleutherAI/pythia-2.8b-deduped,revi...   \n",
       "4     Step8000  pretrained=EleutherAI/pythia-2.8b-deduped,revi...   \n",
       "5     Step4000  pretrained=EleutherAI/pythia-2.8b-deduped,revi...   \n",
       "6     Step2000  pretrained=EleutherAI/pythia-2.8b-deduped,revi...   \n",
       "0   Step128000  pretrained=EleutherAI/pythia-6.9b-deduped,revi...   \n",
       "1    Step64000  pretrained=EleutherAI/pythia-6.9b-deduped,revi...   \n",
       "2    Step32000  pretrained=EleutherAI/pythia-6.9b-deduped,revi...   \n",
       "3    Step16000  pretrained=EleutherAI/pythia-6.9b-deduped,revi...   \n",
       "4     Step8000  pretrained=EleutherAI/pythia-6.9b-deduped,revi...   \n",
       "5     Step4000  pretrained=EleutherAI/pythia-6.9b-deduped,revi...   \n",
       "6     Step2000  pretrained=EleutherAI/pythia-6.9b-deduped,revi...   \n",
       "0   Step512000                                                NaN   \n",
       "1   Step256000                                                NaN   \n",
       "2   Step128000                                                NaN   \n",
       "3    Step64000                                                NaN   \n",
       "4    Step32000                                                NaN   \n",
       "5    Step16000                                                NaN   \n",
       "6     Step8000                                                NaN   \n",
       "7     Step4000                                                NaN   \n",
       "8     Step2000                                                NaN   \n",
       "9     Step1000                                                NaN   \n",
       "10       Step0                                                NaN   \n",
       "\n",
       "    medmcqa/acc  medmcqa/acc_stderr  medqa_4options/acc  \\\n",
       "0      0.318193            0.007203            0.277298   \n",
       "1      0.318671            0.007205            0.277298   \n",
       "2      0.319866            0.007213            0.278083   \n",
       "3      0.319627            0.007211            0.275727   \n",
       "4      0.319388            0.007210            0.279654   \n",
       "5      0.321061            0.007220            0.277298   \n",
       "6      0.314129            0.007178            0.278083   \n",
       "0      0.320344            0.007215            0.273370   \n",
       "1      0.321301            0.007221            0.275727   \n",
       "2      0.320344            0.007215            0.279654   \n",
       "3      0.317715            0.007200            0.276512   \n",
       "4      0.319388            0.007210            0.275727   \n",
       "5      0.321301            0.007221            0.275727   \n",
       "6      0.321301            0.007221            0.277298   \n",
       "0      0.320105            0.007214            0.273370   \n",
       "1      0.318671            0.007205            0.276512   \n",
       "2      0.318432            0.007204            0.274941   \n",
       "3      0.308630            0.007143            0.277298   \n",
       "4      0.311021            0.007158            0.276512   \n",
       "5      0.316758            0.007194            0.276512   \n",
       "6      0.322496            0.007228            0.277298   \n",
       "0      0.316280            0.007191            0.242734   \n",
       "1      0.313172            0.007172            0.246661   \n",
       "2      0.293569            0.007042            0.219167   \n",
       "3      0.292613            0.007035            0.274941   \n",
       "4      0.310782            0.007157            0.268657   \n",
       "5      0.322018            0.007225            0.277298   \n",
       "6      0.321779            0.007224            0.276512   \n",
       "0      0.252211            0.006716            0.252946   \n",
       "1      0.273488            0.006893            0.263158   \n",
       "2      0.312455            0.007167            0.257659   \n",
       "3      0.311260            0.007160            0.270228   \n",
       "4      0.286158            0.006989            0.271013   \n",
       "5      0.321061            0.007220            0.277298   \n",
       "6      0.322018            0.007225            0.276512   \n",
       "0      0.276596            0.006917            0.249804   \n",
       "1      0.256514            0.006753            0.273370   \n",
       "2      0.223046            0.006437            0.267871   \n",
       "3      0.240736            0.006611            0.235664   \n",
       "4      0.314368            0.007179            0.276512   \n",
       "5      0.313890            0.007176            0.276512   \n",
       "6      0.321301            0.007221            0.277298   \n",
       "0      0.237150            0.006577            0.218382   \n",
       "1      0.261774            0.006798            0.225452   \n",
       "2      0.301219            0.007094            0.269442   \n",
       "3      0.276596            0.006917            0.270228   \n",
       "4      0.323691            0.007235            0.277298   \n",
       "5      0.307674            0.007137            0.271799   \n",
       "6      0.312216            0.007166            0.277298   \n",
       "7      0.299307            0.007082            0.272584   \n",
       "8      0.308391            0.007141            0.248233   \n",
       "9      0.298829            0.007078            0.276512   \n",
       "10     0.261774            0.006798            0.235664   \n",
       "\n",
       "    medqa_4options/acc_stderr  mmlu_anatomy/acc  mmlu_anatomy/acc_stderr  \\\n",
       "0                    0.012552          0.185185                 0.033557   \n",
       "1                    0.012552          0.192593                 0.034065   \n",
       "2                    0.012563          0.185185                 0.033557   \n",
       "3                    0.012530          0.200000                 0.034555   \n",
       "4                    0.012585          0.192593                 0.034065   \n",
       "5                    0.012552          0.185185                 0.033557   \n",
       "6                    0.012563          0.192593                 0.034065   \n",
       "0                    0.012497          0.192593                 0.034065   \n",
       "1                    0.012530          0.185185                 0.033557   \n",
       "2                    0.012585          0.192593                 0.034065   \n",
       "3                    0.012541          0.192593                 0.034065   \n",
       "4                    0.012530          0.185185                 0.033557   \n",
       "5                    0.012530          0.185185                 0.033557   \n",
       "6                    0.012552          0.185185                 0.033557   \n",
       "0                    0.012497          0.185185                 0.033557   \n",
       "1                    0.012541          0.185185                 0.033557   \n",
       "2                    0.012519          0.192593                 0.034065   \n",
       "3                    0.012552          0.185185                 0.033557   \n",
       "4                    0.012541          0.214815                 0.035479   \n",
       "5                    0.012541          0.192593                 0.034065   \n",
       "6                    0.012552          0.185185                 0.033557   \n",
       "0                    0.012021          0.214815                 0.035479   \n",
       "1                    0.012087          0.200000                 0.034555   \n",
       "2                    0.011599          0.192593                 0.034065   \n",
       "3                    0.012519          0.288889                 0.039155   \n",
       "4                    0.012428          0.237037                 0.036737   \n",
       "5                    0.012552          0.185185                 0.033557   \n",
       "6                    0.012541          0.185185                 0.033557   \n",
       "0                    0.012188          0.229630                 0.036334   \n",
       "1                    0.012347          0.318519                 0.040248   \n",
       "2                    0.012263          0.251852                 0.037499   \n",
       "3                    0.012451          0.207407                 0.035026   \n",
       "4                    0.012463          0.244444                 0.037125   \n",
       "5                    0.012552          0.192593                 0.034065   \n",
       "6                    0.012541          0.185185                 0.033557   \n",
       "0                    0.012138          0.200000                 0.034555   \n",
       "1                    0.012497          0.185185                 0.033557   \n",
       "2                    0.012417          0.229630                 0.036334   \n",
       "3                    0.011900          0.281481                 0.038850   \n",
       "4                    0.012541          0.251852                 0.037499   \n",
       "5                    0.012541          0.177778                 0.033028   \n",
       "6                    0.012552          0.185185                 0.033557   \n",
       "0                    0.011584          0.333333                 0.040723   \n",
       "1                    0.011717          0.244444                 0.037125   \n",
       "2                    0.012440          0.214815                 0.035479   \n",
       "3                    0.012451          0.311111                 0.039993   \n",
       "4                    0.012552          0.237037                 0.036737   \n",
       "5                    0.012474          0.177778                 0.033028   \n",
       "6                    0.012552          0.200000                 0.034555   \n",
       "7                    0.012485          0.222222                 0.035914   \n",
       "8                    0.012112          0.200000                 0.034555   \n",
       "9                    0.012541          0.192593                 0.034065   \n",
       "10                   0.011900          0.333333                 0.040723   \n",
       "\n",
       "    mmlu_clinical_knowledge/acc  mmlu_clinical_knowledge/acc_stderr  \\\n",
       "0                      0.211321                            0.025126   \n",
       "1                      0.215094                            0.025288   \n",
       "2                      0.211321                            0.025126   \n",
       "3                      0.215094                            0.025288   \n",
       "4                      0.215094                            0.025288   \n",
       "5                      0.215094                            0.025288   \n",
       "6                      0.215094                            0.025288   \n",
       "0                      0.211321                            0.025126   \n",
       "1                      0.215094                            0.025288   \n",
       "2                      0.211321                            0.025126   \n",
       "3                      0.211321                            0.025126   \n",
       "4                      0.218868                            0.025448   \n",
       "5                      0.218868                            0.025448   \n",
       "6                      0.211321                            0.025126   \n",
       "0                      0.226415                            0.025758   \n",
       "1                      0.215094                            0.025288   \n",
       "2                      0.215094                            0.025288   \n",
       "3                      0.222642                            0.025604   \n",
       "4                      0.215094                            0.025288   \n",
       "5                      0.218868                            0.025448   \n",
       "6                      0.215094                            0.025288   \n",
       "0                      0.245283                            0.026480   \n",
       "1                      0.237736                            0.026200   \n",
       "2                      0.313208                            0.028545   \n",
       "3                      0.222642                            0.025604   \n",
       "4                      0.218868                            0.025448   \n",
       "5                      0.215094                            0.025288   \n",
       "6                      0.215094                            0.025288   \n",
       "0                      0.286792                            0.027835   \n",
       "1                      0.226415                            0.025758   \n",
       "2                      0.271698                            0.027378   \n",
       "3                      0.230189                            0.025908   \n",
       "4                      0.233962                            0.026055   \n",
       "5                      0.215094                            0.025288   \n",
       "6                      0.215094                            0.025288   \n",
       "0                      0.245283                            0.026480   \n",
       "1                      0.218868                            0.025448   \n",
       "2                      0.230189                            0.025908   \n",
       "3                      0.218868                            0.025448   \n",
       "4                      0.249057                            0.026616   \n",
       "5                      0.222642                            0.025604   \n",
       "6                      0.215094                            0.025288   \n",
       "0                      0.249057                            0.026616   \n",
       "1                      0.241509                            0.026341   \n",
       "2                      0.237736                            0.026200   \n",
       "3                      0.200000                            0.024618   \n",
       "4                      0.207547                            0.024960   \n",
       "5                      0.222642                            0.025604   \n",
       "6                      0.215094                            0.025288   \n",
       "7                      0.222642                            0.025604   \n",
       "8                      0.211321                            0.025126   \n",
       "9                      0.215094                            0.025288   \n",
       "10                     0.260377                            0.027009   \n",
       "\n",
       "    mmlu_college_biology/acc  mmlu_college_biology/acc_stderr  \\\n",
       "0                   0.263889                         0.036857   \n",
       "1                   0.291667                         0.038010   \n",
       "2                   0.263889                         0.036857   \n",
       "3                   0.256944                         0.036539   \n",
       "4                   0.263889                         0.036857   \n",
       "5                   0.256944                         0.036539   \n",
       "6                   0.284722                         0.037738   \n",
       "0                   0.256944                         0.036539   \n",
       "1                   0.263889                         0.036857   \n",
       "2                   0.263889                         0.036857   \n",
       "3                   0.270833                         0.037162   \n",
       "4                   0.270833                         0.037162   \n",
       "5                   0.277778                         0.037456   \n",
       "6                   0.256944                         0.036539   \n",
       "0                   0.256944                         0.036539   \n",
       "1                   0.256944                         0.036539   \n",
       "2                   0.256944                         0.036539   \n",
       "3                   0.270833                         0.037162   \n",
       "4                   0.284722                         0.037738   \n",
       "5                   0.263889                         0.036857   \n",
       "6                   0.256944                         0.036539   \n",
       "0                   0.263889                         0.036857   \n",
       "1                   0.229167                         0.035147   \n",
       "2                   0.215278                         0.034371   \n",
       "3                   0.284722                         0.037738   \n",
       "4                   0.298611                         0.038271   \n",
       "5                   0.256944                         0.036539   \n",
       "6                   0.263889                         0.036857   \n",
       "0                   0.236111                         0.035514   \n",
       "1                   0.284722                         0.037738   \n",
       "2                   0.326389                         0.039211   \n",
       "3                   0.326389                         0.039211   \n",
       "4                   0.305556                         0.038521   \n",
       "5                   0.263889                         0.036857   \n",
       "6                   0.270833                         0.037162   \n",
       "0                   0.250000                         0.036210   \n",
       "1                   0.256944                         0.036539   \n",
       "2                   0.277778                         0.037456   \n",
       "3                   0.229167                         0.035147   \n",
       "4                   0.298611                         0.038271   \n",
       "5                   0.256944                         0.036539   \n",
       "6                   0.256944                         0.036539   \n",
       "0                   0.256944                         0.036539   \n",
       "1                   0.208333                         0.033961   \n",
       "2                   0.229167                         0.035147   \n",
       "3                   0.284722                         0.037738   \n",
       "4                   0.236111                         0.035514   \n",
       "5                   0.250000                         0.036210   \n",
       "6                   0.256944                         0.036539   \n",
       "7                   0.270833                         0.037162   \n",
       "8                   0.256944                         0.036539   \n",
       "9                   0.256944                         0.036539   \n",
       "10                  0.194444                         0.033096   \n",
       "\n",
       "    mmlu_college_medicine/acc  mmlu_college_medicine/acc_stderr  \\\n",
       "0                    0.213873                          0.031265   \n",
       "1                    0.208092                          0.030953   \n",
       "2                    0.208092                          0.030953   \n",
       "3                    0.208092                          0.030953   \n",
       "4                    0.219653                          0.031568   \n",
       "5                    0.213873                          0.031265   \n",
       "6                    0.231214                          0.032147   \n",
       "0                    0.208092                          0.030953   \n",
       "1                    0.213873                          0.031265   \n",
       "2                    0.208092                          0.030953   \n",
       "3                    0.213873                          0.031265   \n",
       "4                    0.213873                          0.031265   \n",
       "5                    0.208092                          0.030953   \n",
       "6                    0.208092                          0.030953   \n",
       "0                    0.184971                          0.029606   \n",
       "1                    0.202312                          0.030631   \n",
       "2                    0.213873                          0.031265   \n",
       "3                    0.202312                          0.030631   \n",
       "4                    0.219653                          0.031568   \n",
       "5                    0.219653                          0.031568   \n",
       "6                    0.208092                          0.030953   \n",
       "0                    0.196532                          0.030300   \n",
       "1                    0.271676                          0.033918   \n",
       "2                    0.254335                          0.033206   \n",
       "3                    0.248555                          0.032953   \n",
       "4                    0.196532                          0.030300   \n",
       "5                    0.213873                          0.031265   \n",
       "6                    0.219653                          0.031568   \n",
       "0                    0.254335                          0.033206   \n",
       "1                    0.225434                          0.031862   \n",
       "2                    0.248555                          0.032953   \n",
       "3                    0.225434                          0.031862   \n",
       "4                    0.231214                          0.032147   \n",
       "5                    0.208092                          0.030953   \n",
       "6                    0.213873                          0.031265   \n",
       "0                    0.173410                          0.028868   \n",
       "1                    0.219653                          0.031568   \n",
       "2                    0.236994                          0.032424   \n",
       "3                    0.254335                          0.033206   \n",
       "4                    0.219653                          0.031568   \n",
       "5                    0.208092                          0.030953   \n",
       "6                    0.213873                          0.031265   \n",
       "0                    0.341040                          0.036147   \n",
       "1                    0.213873                          0.031265   \n",
       "2                    0.260116                          0.033450   \n",
       "3                    0.225434                          0.031862   \n",
       "4                    0.248555                          0.032953   \n",
       "5                    0.231214                          0.032147   \n",
       "6                    0.213873                          0.031265   \n",
       "7                    0.219653                          0.031568   \n",
       "8                    0.196532                          0.030300   \n",
       "9                    0.213873                          0.031265   \n",
       "10                   0.231214                          0.032147   \n",
       "\n",
       "    mmlu_medical_genetics/acc  mmlu_medical_genetics/acc_stderr  \\\n",
       "0                        0.31                          0.046482   \n",
       "1                        0.31                          0.046482   \n",
       "2                        0.30                          0.046057   \n",
       "3                        0.30                          0.046057   \n",
       "4                        0.31                          0.046482   \n",
       "5                        0.30                          0.046057   \n",
       "6                        0.31                          0.046482   \n",
       "0                        0.30                          0.046057   \n",
       "1                        0.30                          0.046057   \n",
       "2                        0.30                          0.046057   \n",
       "3                        0.30                          0.046057   \n",
       "4                        0.30                          0.046057   \n",
       "5                        0.32                          0.046883   \n",
       "6                        0.30                          0.046057   \n",
       "0                        0.30                          0.046057   \n",
       "1                        0.30                          0.046057   \n",
       "2                        0.30                          0.046057   \n",
       "3                        0.31                          0.046482   \n",
       "4                        0.29                          0.045605   \n",
       "5                        0.29                          0.045605   \n",
       "6                        0.30                          0.046057   \n",
       "0                        0.28                          0.045126   \n",
       "1                        0.29                          0.045605   \n",
       "2                        0.27                          0.044620   \n",
       "3                        0.29                          0.045605   \n",
       "4                        0.27                          0.044620   \n",
       "5                        0.30                          0.046057   \n",
       "6                        0.30                          0.046057   \n",
       "0                        0.30                          0.046057   \n",
       "1                        0.27                          0.044620   \n",
       "2                        0.28                          0.045126   \n",
       "3                        0.25                          0.043519   \n",
       "4                        0.23                          0.042295   \n",
       "5                        0.30                          0.046057   \n",
       "6                        0.30                          0.046057   \n",
       "0                        0.32                          0.046883   \n",
       "1                        0.30                          0.046057   \n",
       "2                        0.27                          0.044620   \n",
       "3                        0.24                          0.042923   \n",
       "4                        0.33                          0.047258   \n",
       "5                        0.31                          0.046482   \n",
       "6                        0.30                          0.046057   \n",
       "0                        0.30                          0.046057   \n",
       "1                        0.25                          0.043519   \n",
       "2                        0.34                          0.047610   \n",
       "3                        0.26                          0.044084   \n",
       "4                        0.35                          0.047937   \n",
       "5                        0.31                          0.046482   \n",
       "6                        0.30                          0.046057   \n",
       "7                        0.30                          0.046057   \n",
       "8                        0.30                          0.046057   \n",
       "9                        0.31                          0.046482   \n",
       "10                       0.24                          0.042923   \n",
       "\n",
       "    mmlu_professional_medicine/acc  mmlu_professional_medicine/acc_stderr  \\\n",
       "0                         0.187500                               0.023710   \n",
       "1                         0.187500                               0.023710   \n",
       "2                         0.191176                               0.023887   \n",
       "3                         0.183824                               0.023529   \n",
       "4                         0.187500                               0.023710   \n",
       "5                         0.183824                               0.023529   \n",
       "6                         0.183824                               0.023529   \n",
       "0                         0.187500                               0.023710   \n",
       "1                         0.194853                               0.024061   \n",
       "2                         0.191176                               0.023887   \n",
       "3                         0.194853                               0.024061   \n",
       "4                         0.191176                               0.023887   \n",
       "5                         0.191176                               0.023887   \n",
       "6                         0.183824                               0.023529   \n",
       "0                         0.183824                               0.023529   \n",
       "1                         0.183824                               0.023529   \n",
       "2                         0.187500                               0.023710   \n",
       "3                         0.224265                               0.025337   \n",
       "4                         0.194853                               0.024061   \n",
       "5                         0.198529                               0.024231   \n",
       "6                         0.183824                               0.023529   \n",
       "0                         0.187500                               0.023710   \n",
       "1                         0.264706                               0.026800   \n",
       "2                         0.312500                               0.028156   \n",
       "3                         0.180147                               0.023345   \n",
       "4                         0.202206                               0.024398   \n",
       "5                         0.183824                               0.023529   \n",
       "6                         0.194853                               0.024061   \n",
       "0                         0.272059                               0.027033   \n",
       "1                         0.180147                               0.023345   \n",
       "2                         0.194853                               0.024061   \n",
       "3                         0.169118                               0.022771   \n",
       "4                         0.169118                               0.022771   \n",
       "5                         0.180147                               0.023345   \n",
       "6                         0.191176                               0.023887   \n",
       "0                         0.194853                               0.024061   \n",
       "1                         0.194853                               0.024061   \n",
       "2                         0.180147                               0.023345   \n",
       "3                         0.209559                               0.024723   \n",
       "4                         0.180147                               0.023345   \n",
       "5                         0.209559                               0.024723   \n",
       "6                         0.180147                               0.023345   \n",
       "0                         0.224265                               0.025337   \n",
       "1                         0.202206                               0.024398   \n",
       "2                         0.194853                               0.024061   \n",
       "3                         0.161765                               0.022369   \n",
       "4                         0.238971                               0.025905   \n",
       "5                         0.202206                               0.024398   \n",
       "6                         0.187500                               0.023710   \n",
       "7                         0.187500                               0.023710   \n",
       "8                         0.205882                               0.024562   \n",
       "9                         0.187500                               0.023710   \n",
       "10                        0.180147                               0.023345   \n",
       "\n",
       "    pubmedqa/acc  pubmedqa/acc_stderr  \n",
       "0          0.538             0.022318  \n",
       "1          0.536             0.022325  \n",
       "2          0.434             0.022187  \n",
       "3          0.550             0.022271  \n",
       "4          0.554             0.022252  \n",
       "5          0.552             0.022262  \n",
       "6          0.538             0.022318  \n",
       "0          0.476             0.022357  \n",
       "1          0.404             0.021967  \n",
       "2          0.548             0.022280  \n",
       "3          0.554             0.022252  \n",
       "4          0.552             0.022262  \n",
       "5          0.558             0.022232  \n",
       "6          0.540             0.022311  \n",
       "0          0.514             0.022374  \n",
       "1          0.530             0.022343  \n",
       "2          0.468             0.022337  \n",
       "3          0.480             0.022365  \n",
       "4          0.380             0.021729  \n",
       "5          0.556             0.022242  \n",
       "6          0.502             0.022383  \n",
       "0          0.530             0.022343  \n",
       "1          0.578             0.022109  \n",
       "2          0.550             0.022271  \n",
       "3          0.554             0.022252  \n",
       "4          0.544             0.022296  \n",
       "5          0.540             0.022311  \n",
       "6          0.552             0.022262  \n",
       "0          0.632             0.021589  \n",
       "1          0.642             0.021461  \n",
       "2          0.574             0.022137  \n",
       "3          0.562             0.022210  \n",
       "4          0.554             0.022252  \n",
       "5          0.492             0.022380  \n",
       "6          0.522             0.022361  \n",
       "0          0.622             0.021707  \n",
       "1          0.638             0.021514  \n",
       "2          0.614             0.021794  \n",
       "3          0.570             0.022163  \n",
       "4          0.566             0.022187  \n",
       "5          0.498             0.022383  \n",
       "6          0.548             0.022280  \n",
       "0          0.698             0.020553  \n",
       "1          0.682             0.020848  \n",
       "2          0.640             0.021488  \n",
       "3          0.598             0.021949  \n",
       "4          0.556             0.022242  \n",
       "5          0.532             0.022337  \n",
       "6          0.504             0.022382  \n",
       "7          0.452             0.022280  \n",
       "8          0.422             0.022109  \n",
       "9          0.360             0.021488  \n",
       "10         0.382             0.021751  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#MultiMedQA evaluations across Pythia model scale and intermediate checkpoints, along with OLMo 7B\n",
    "#Table 6 is a combination of data from the following files:\n",
    "Pythia70M_checkpoint_evals = pd.read_csv('../eval-results/wandb-logs/wandb_pythia_70m_dynamics.csv')\n",
    "Pythia160M_checkpoint_evals = pd.read_csv('../eval-results/wandb-logs/wandb_pythia_160m_dynamics.csv')\n",
    "Pythia410M_checkpoint_evals = pd.read_csv('../eval-results/wandb-logs/wandb_pythia_410m_dynamics.csv')\n",
    "Pythia1B_checkpoint_evals = pd.read_csv('../eval-results/wandb-logs/wandb_pythia_1b_dynamics.csv')\n",
    "Pythia2_8B_checkpoint_evals = pd.read_csv('../eval-results/wandb-logs/wandb_pythia_2-8b_dynamics.csv')\n",
    "Pythia6_9B_checkpoint_evals = pd.read_csv('../eval-results/wandb-logs/wandb_pythia_6-9b_dynamics.csv')\n",
    "OLMo7B_checkpoint_evals = pd.read_csv('../eval-results/wandb-logs/wandb_OLMo7B_dynamics.csv')\n",
    "\n",
    "# Combine the dataframes\n",
    "combined_checkpoint_evals = pd.concat([Pythia70M_checkpoint_evals, Pythia160M_checkpoint_evals, Pythia410M_checkpoint_evals, Pythia1B_checkpoint_evals, Pythia2_8B_checkpoint_evals, Pythia6_9B_checkpoint_evals, OLMo7B_checkpoint_evals])\n",
    "\n",
    "#Filter to only include relevant columns\n",
    "\n",
    "combined_checkpoint_evals = combined_checkpoint_evals[['Name', 'cli_configs.model_args', 'medmcqa/acc', 'medmcqa/acc_stderr', 'medqa_4options/acc', 'medqa_4options/acc_stderr', 'mmlu_anatomy/acc', 'mmlu_anatomy/acc_stderr'  ,'mmlu_clinical_knowledge/acc', 'mmlu_clinical_knowledge/acc_stderr' , 'mmlu_college_biology/acc', 'mmlu_college_biology/acc_stderr','mmlu_college_medicine/acc', 'mmlu_college_medicine/acc_stderr','mmlu_medical_genetics/acc','mmlu_medical_genetics/acc_stderr' ,'mmlu_professional_medicine/acc','mmlu_professional_medicine/acc_stderr' ,'pubmedqa/acc', 'pubmedqa/acc_stderr']]\n",
    "combined_checkpoint_evals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Task</th>\n",
       "      <th>Slope Coef for ParamSize</th>\n",
       "      <th>Corrected P-value for ParamSize</th>\n",
       "      <th>Slope Coef for TokensSeen</th>\n",
       "      <th>Corrected P-value for TokensSeen</th>\n",
       "      <th>Interaction Term Coef</th>\n",
       "      <th>Corrected Interaction Term P-value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>medqa_4options/acc</td>\n",
       "      <td>0.128872</td>\n",
       "      <td>0.222961</td>\n",
       "      <td>0.103852</td>\n",
       "      <td>0.234130</td>\n",
       "      <td>-0.013433</td>\n",
       "      <td>0.216491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>medmcqa/acc</td>\n",
       "      <td>0.326367</td>\n",
       "      <td>0.016653</td>\n",
       "      <td>0.279611</td>\n",
       "      <td>0.016653</td>\n",
       "      <td>-0.033918</td>\n",
       "      <td>0.016653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pubmedqa/acc</td>\n",
       "      <td>-0.467243</td>\n",
       "      <td>0.018419</td>\n",
       "      <td>-0.405790</td>\n",
       "      <td>0.016653</td>\n",
       "      <td>0.046929</td>\n",
       "      <td>0.016653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mmlu_anatomy/acc</td>\n",
       "      <td>-0.220740</td>\n",
       "      <td>0.552654</td>\n",
       "      <td>-0.198917</td>\n",
       "      <td>0.527284</td>\n",
       "      <td>0.024666</td>\n",
       "      <td>0.477813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mmlu_clinical_knowledge/acc</td>\n",
       "      <td>-0.213527</td>\n",
       "      <td>0.222961</td>\n",
       "      <td>-0.177788</td>\n",
       "      <td>0.222961</td>\n",
       "      <td>0.022411</td>\n",
       "      <td>0.216491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>mmlu_college_biology/acc</td>\n",
       "      <td>0.056359</td>\n",
       "      <td>0.829696</td>\n",
       "      <td>0.036012</td>\n",
       "      <td>0.865097</td>\n",
       "      <td>-0.005213</td>\n",
       "      <td>0.829696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>mmlu_college_medicine/acc</td>\n",
       "      <td>-0.087478</td>\n",
       "      <td>0.719230</td>\n",
       "      <td>-0.084906</td>\n",
       "      <td>0.682164</td>\n",
       "      <td>0.009475</td>\n",
       "      <td>0.682164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>mmlu_medical_genetics/acc</td>\n",
       "      <td>0.003566</td>\n",
       "      <td>0.976314</td>\n",
       "      <td>0.013637</td>\n",
       "      <td>0.926770</td>\n",
       "      <td>-0.001892</td>\n",
       "      <td>0.926770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mmlu_professional_medicine/acc</td>\n",
       "      <td>-0.161777</td>\n",
       "      <td>0.675603</td>\n",
       "      <td>-0.121813</td>\n",
       "      <td>0.682164</td>\n",
       "      <td>0.016103</td>\n",
       "      <td>0.675603</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Task  Slope Coef for ParamSize  \\\n",
       "0              medqa_4options/acc                  0.128872   \n",
       "1                     medmcqa/acc                  0.326367   \n",
       "2                    pubmedqa/acc                 -0.467243   \n",
       "3                mmlu_anatomy/acc                 -0.220740   \n",
       "4     mmlu_clinical_knowledge/acc                 -0.213527   \n",
       "5        mmlu_college_biology/acc                  0.056359   \n",
       "6       mmlu_college_medicine/acc                 -0.087478   \n",
       "7       mmlu_medical_genetics/acc                  0.003566   \n",
       "8  mmlu_professional_medicine/acc                 -0.161777   \n",
       "\n",
       "   Corrected P-value for ParamSize  Slope Coef for TokensSeen  \\\n",
       "0                         0.222961                   0.103852   \n",
       "1                         0.016653                   0.279611   \n",
       "2                         0.018419                  -0.405790   \n",
       "3                         0.552654                  -0.198917   \n",
       "4                         0.222961                  -0.177788   \n",
       "5                         0.829696                   0.036012   \n",
       "6                         0.719230                  -0.084906   \n",
       "7                         0.976314                   0.013637   \n",
       "8                         0.675603                  -0.121813   \n",
       "\n",
       "   Corrected P-value for TokensSeen  Interaction Term Coef  \\\n",
       "0                          0.234130              -0.013433   \n",
       "1                          0.016653              -0.033918   \n",
       "2                          0.016653               0.046929   \n",
       "3                          0.527284               0.024666   \n",
       "4                          0.222961               0.022411   \n",
       "5                          0.865097              -0.005213   \n",
       "6                          0.682164               0.009475   \n",
       "7                          0.926770              -0.001892   \n",
       "8                          0.682164               0.016103   \n",
       "\n",
       "   Corrected Interaction Term P-value  \n",
       "0                            0.216491  \n",
       "1                            0.016653  \n",
       "2                            0.016653  \n",
       "3                            0.477813  \n",
       "4                            0.216491  \n",
       "5                            0.829696  \n",
       "6                            0.682164  \n",
       "7                            0.926770  \n",
       "8                            0.675603  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.sandbox.stats.multicomp import multipletests\n",
    "\n",
    "# Load and prepare data\n",
    "def load_and_prepare_data(file_paths, model_names):\n",
    "    all_data = []\n",
    "    tokens_per_step = 2097152\n",
    "    for file_path, model_name in zip(file_paths, model_names):\n",
    "        df = pd.read_csv(file_path)\n",
    "        if 'Name' in df.columns:\n",
    "            df['Step'] = df['Name'].str.extract('(\\d+)').astype('int64')\n",
    "        df['Model'] = model_name\n",
    "        accuracy_columns = [col for col in df.columns if '/acc' in col]\n",
    "        relevant_columns = ['Step', 'Model'] + accuracy_columns\n",
    "        df = df[relevant_columns]\n",
    "        df['tokens_seen'] = df['Step'] * tokens_per_step\n",
    "        all_data.append(df)\n",
    "    combined_df = pd.concat(all_data, ignore_index=True)\n",
    "    combined_df['Log10_Step'] = np.log10(combined_df['Step'])\n",
    "    combined_df['Log10_tokens_seen'] = np.log10(combined_df['tokens_seen'])\n",
    "    return combined_df\n",
    "\n",
    "file_paths = [\n",
    "    \"../eval-results/wandb-logs/wandb_pythia_70m_dynamics.csv\",\n",
    "    \"../eval-results/wandb-logs/wandb_pythia_160m_dynamics.csv\",\n",
    "    \"../eval-results/wandb-logs/wandb_pythia_410m_dynamics.csv\",\n",
    "    \"../eval-results/wandb-logs/wandb_pythia_1b_dynamics.csv\",\n",
    "    \"../eval-results/wandb-logs/wandb_pythia_2-8b_dynamics.csv\",\n",
    "    \"../eval-results/wandb-logs/wandb_pythia_6-9b_dynamics.csv\"\n",
    "]\n",
    "model_names = [\"Pythia 70M\", \"Pythia 160M\", \"Pythia 410M\", \"Pythia 1B\", \"Pythia 2.8B\", \"Pythia 6.9B\"]\n",
    "\n",
    "combined_df = load_and_prepare_data(file_paths, model_names)\n",
    "\n",
    "# Melting DataFrame\n",
    "task_names = [\n",
    "    \"medqa_4options/acc\", \"medmcqa/acc\", \"pubmedqa/acc\", \"mmlu_anatomy/acc\", \"mmlu_clinical_knowledge/acc\",\n",
    "    \"mmlu_college_biology/acc\", \"mmlu_college_medicine/acc\", \"mmlu_medical_genetics/acc\", \"mmlu_professional_medicine/acc\"\n",
    "]\n",
    "melted_df = combined_df.melt(id_vars=['tokens_seen', 'Model'], value_vars=task_names, var_name='Task', value_name='Accuracy')\n",
    "\n",
    "# Reorder the DataFrame based on model size\n",
    "model_order = [\"Pythia 70M\", \"Pythia 160M\", \"Pythia 410M\", \"Pythia 1B\", \"Pythia 2.8B\", \"Pythia 6.9B\"]\n",
    "melted_df['Model'] = pd.Categorical(melted_df['Model'], categories=model_order, ordered=True)\n",
    "\n",
    "# Create a dictionary to map model names to their parameter sizes\n",
    "param_sizes = {\n",
    "    \"Pythia 70M\": 70000000,\n",
    "    \"Pythia 160M\": 160000000,\n",
    "    \"Pythia 410M\": 410000000,\n",
    "    \"Pythia 1B\": 1000000000,\n",
    "    \"Pythia 2.8B\": 2800000000,\n",
    "    \"Pythia 6.9B\": 6900000000\n",
    "}\n",
    "\n",
    "# Create a new column for parameter size\n",
    "melted_df['ParamSize'] = melted_df['Model'].map(param_sizes).astype('int64')\n",
    "\n",
    "# Take the log10 of the parameter size and training steps\n",
    "melted_df['Log10_ParamSize'] = np.log10(melted_df['ParamSize'])\n",
    "# Take the log10 of the tokens seen\n",
    "melted_df['Log10_TokensSeen'] = np.log10(melted_df['tokens_seen'])\n",
    "\n",
    "melted_df['Log10_Accuracy'] = np.log10(melted_df['Accuracy'])\n",
    "\n",
    "\n",
    "# Fit a separate regression model for each task and extract coefficients and p-values\n",
    "results_list = []\n",
    "\n",
    "for task in tasks:\n",
    "    task_data = melted_df[melted_df['Task'] == task]\n",
    "    model = smf.ols(formula='Log10_Accuracy ~ Log10_ParamSize * Log10_TokensSeen', data=task_data)\n",
    "    results = model.fit()\n",
    "    \n",
    "    # Extract coefficients and p-values\n",
    "    coefs = results.params\n",
    "    pvals = results.pvalues\n",
    "    \n",
    "    # Append results to list\n",
    "    results_list.append({\n",
    "        'Task': task,\n",
    "        'Slope Coef for ParamSize': coefs['Log10_ParamSize'],\n",
    "        'P-value for ParamSize': pvals['Log10_ParamSize'],\n",
    "        'Slope Coef for TokensSeen': coefs['Log10_TokensSeen'],\n",
    "        'P-value for TokensSeen': pvals['Log10_TokensSeen'],\n",
    "        'Interaction Term Coef': coefs['Log10_ParamSize:Log10_TokensSeen'],\n",
    "        'Interaction Term P-value': pvals['Log10_ParamSize:Log10_TokensSeen']\n",
    "    })\n",
    "\n",
    "# Create a DataFrame from the results list\n",
    "results_df_table7 = pd.DataFrame(results_list)\n",
    "\n",
    "# Correct for multiple comparisons\n",
    "p_values_to_correct = results_df_table7[['P-value for ParamSize', 'P-value for TokensSeen', 'Interaction Term P-value']].values.flatten()\n",
    "_, pvals_corrected, _, _ = multipletests(p_values_to_correct, method='fdr_bh')\n",
    "\n",
    "# Assign corrected p-values back to the DataFrame\n",
    "results_df_table7['Corrected P-value for ParamSize'] = pvals_corrected[0::3]\n",
    "results_df_table7['Corrected P-value for TokensSeen'] = pvals_corrected[1::3]\n",
    "results_df_table7['Corrected Interaction Term P-value'] = pvals_corrected[2::3]\n",
    "\n",
    "# Reorder columns for final output\n",
    "results_df_table7 = results_df_table7[[\n",
    "    'Task',\n",
    "    'Slope Coef for ParamSize',\n",
    "    'Corrected P-value for ParamSize',\n",
    "    'Slope Coef for TokensSeen',\n",
    "    'Corrected P-value for TokensSeen',\n",
    "    'Interaction Term Coef',\n",
    "    'Corrected Interaction Term P-value'\n",
    "]]\n",
    "\n",
    "results_df_table7\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Slope</th>\n",
       "      <th>Slope_pValue</th>\n",
       "      <th>Task</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pythia 70M</td>\n",
       "      <td>-0.015609</td>\n",
       "      <td>0.559706</td>\n",
       "      <td>pubmedqa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pythia 160M</td>\n",
       "      <td>-0.053154</td>\n",
       "      <td>0.108142</td>\n",
       "      <td>pubmedqa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pythia 410M</td>\n",
       "      <td>0.009449</td>\n",
       "      <td>0.807181</td>\n",
       "      <td>pubmedqa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pythia 1B</td>\n",
       "      <td>0.001286</td>\n",
       "      <td>0.878947</td>\n",
       "      <td>pubmedqa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pythia 2.8B</td>\n",
       "      <td>0.058808</td>\n",
       "      <td>0.003295</td>\n",
       "      <td>pubmedqa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Pythia 6.9B</td>\n",
       "      <td>0.049303</td>\n",
       "      <td>0.013574</td>\n",
       "      <td>pubmedqa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pythia 70M</td>\n",
       "      <td>0.001294</td>\n",
       "      <td>0.546643</td>\n",
       "      <td>medmcqa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pythia 160M</td>\n",
       "      <td>-0.000307</td>\n",
       "      <td>0.812795</td>\n",
       "      <td>medmcqa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pythia 410M</td>\n",
       "      <td>0.000684</td>\n",
       "      <td>0.890472</td>\n",
       "      <td>medmcqa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pythia 1B</td>\n",
       "      <td>-0.008470</td>\n",
       "      <td>0.489431</td>\n",
       "      <td>medmcqa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pythia 2.8B</td>\n",
       "      <td>-0.049765</td>\n",
       "      <td>0.031111</td>\n",
       "      <td>medmcqa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Pythia 6.9B</td>\n",
       "      <td>-0.061642</td>\n",
       "      <td>0.123961</td>\n",
       "      <td>medmcqa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Model     Slope  Slope_pValue      Task\n",
       "0   Pythia 70M -0.015609      0.559706  pubmedqa\n",
       "1  Pythia 160M -0.053154      0.108142  pubmedqa\n",
       "2  Pythia 410M  0.009449      0.807181  pubmedqa\n",
       "3    Pythia 1B  0.001286      0.878947  pubmedqa\n",
       "4  Pythia 2.8B  0.058808      0.003295  pubmedqa\n",
       "5  Pythia 6.9B  0.049303      0.013574  pubmedqa\n",
       "0   Pythia 70M  0.001294      0.546643   medmcqa\n",
       "1  Pythia 160M -0.000307      0.812795   medmcqa\n",
       "2  Pythia 410M  0.000684      0.890472   medmcqa\n",
       "3    Pythia 1B -0.008470      0.489431   medmcqa\n",
       "4  Pythia 2.8B -0.049765      0.031111   medmcqa\n",
       "5  Pythia 6.9B -0.061642      0.123961   medmcqa"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Log-Log Regression for each pythia models accuracy on pubmedqa and medmcqa\n",
    "pubmedqa_data = melted_df[melted_df['Task'] == 'pubmedqa/acc']\n",
    "medmcqa_data = melted_df[melted_df['Task'] == 'medmcqa/acc']\n",
    "\n",
    "\n",
    "# Define a function to perform regression and extract results\n",
    "def analyze_regression(data):\n",
    "    results = []\n",
    "    models = data['Model'].unique()\n",
    "    for model in models:\n",
    "        subset = data[data['Model'] == model]\n",
    "        x = sm.add_constant(subset['Log10_TokensSeen'])  # Add constant for the intercept\n",
    "        y = subset['Log10_Accuracy']\n",
    "        model_fit = sm.OLS(y, x).fit()\n",
    "        # Extract the intercept, slope, and their p-values\n",
    "        intercept, slope = model_fit.params\n",
    "        intercept_pvalue, slope_pvalue = model_fit.pvalues\n",
    "        results.append({'Model': model, 'Slope': slope, 'Slope_pValue': slope_pvalue})\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Analyze both datasets\n",
    "pubmedqa_results = analyze_regression(pubmedqa_data)\n",
    "medmcqa_results = analyze_regression(medmcqa_data)\n",
    "\n",
    "#Combine the results and add column for task\n",
    "pubmedqa_results['Task'] = 'pubmedqa'\n",
    "medmcqa_results['Task'] = 'medmcqa'\n",
    "\n",
    "combined_results_table8 = pd.concat([pubmedqa_results, medmcqa_results])\n",
    "combined_results_table8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nikol\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\arraylike.py:402: RuntimeWarning: divide by zero encountered in log10\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Task</th>\n",
       "      <th>Model</th>\n",
       "      <th>Slope Coefficient</th>\n",
       "      <th>P-Value</th>\n",
       "      <th>R_squared</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>medqa_4options/acc</td>\n",
       "      <td>OLMo 7B</td>\n",
       "      <td>-0.025473</td>\n",
       "      <td>0.0679</td>\n",
       "      <td>0.357467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>medmcqa/acc</td>\n",
       "      <td>OLMo 7B</td>\n",
       "      <td>-0.030678</td>\n",
       "      <td>0.0311</td>\n",
       "      <td>0.460162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pubmedqa/acc</td>\n",
       "      <td>OLMo 7B</td>\n",
       "      <td>0.101560</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.971521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mmlu_anatomy/acc</td>\n",
       "      <td>OLMo 7B</td>\n",
       "      <td>0.068073</td>\n",
       "      <td>0.0245</td>\n",
       "      <td>0.488472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mmlu_clinical_knowledge/acc</td>\n",
       "      <td>OLMo 7B</td>\n",
       "      <td>0.020055</td>\n",
       "      <td>0.0690</td>\n",
       "      <td>0.355323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>mmlu_college_biology/acc</td>\n",
       "      <td>OLMo 7B</td>\n",
       "      <td>-0.017946</td>\n",
       "      <td>0.2198</td>\n",
       "      <td>0.181341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>mmlu_college_medicine/acc</td>\n",
       "      <td>OLMo 7B</td>\n",
       "      <td>0.051300</td>\n",
       "      <td>0.0262</td>\n",
       "      <td>0.480793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>mmlu_medical_genetics/acc</td>\n",
       "      <td>OLMo 7B</td>\n",
       "      <td>-0.010960</td>\n",
       "      <td>0.5347</td>\n",
       "      <td>0.049982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mmlu_professional_medicine/acc</td>\n",
       "      <td>OLMo 7B</td>\n",
       "      <td>0.012257</td>\n",
       "      <td>0.5040</td>\n",
       "      <td>0.057644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>medqa_4options/acc</td>\n",
       "      <td>Pythia 6.9B</td>\n",
       "      <td>-0.018954</td>\n",
       "      <td>0.3126</td>\n",
       "      <td>0.201303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>medmcqa/acc</td>\n",
       "      <td>Pythia 6.9B</td>\n",
       "      <td>-0.061642</td>\n",
       "      <td>0.1240</td>\n",
       "      <td>0.405677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>pubmedqa/acc</td>\n",
       "      <td>Pythia 6.9B</td>\n",
       "      <td>0.049303</td>\n",
       "      <td>0.0136</td>\n",
       "      <td>0.735611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>mmlu_anatomy/acc</td>\n",
       "      <td>Pythia 6.9B</td>\n",
       "      <td>0.011343</td>\n",
       "      <td>0.8375</td>\n",
       "      <td>0.009245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>mmlu_clinical_knowledge/acc</td>\n",
       "      <td>Pythia 6.9B</td>\n",
       "      <td>0.014480</td>\n",
       "      <td>0.4130</td>\n",
       "      <td>0.137409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>mmlu_college_biology/acc</td>\n",
       "      <td>Pythia 6.9B</td>\n",
       "      <td>-0.007961</td>\n",
       "      <td>0.7590</td>\n",
       "      <td>0.020579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>mmlu_college_medicine/acc</td>\n",
       "      <td>Pythia 6.9B</td>\n",
       "      <td>-0.022931</td>\n",
       "      <td>0.5317</td>\n",
       "      <td>0.082729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>mmlu_medical_genetics/acc</td>\n",
       "      <td>Pythia 6.9B</td>\n",
       "      <td>-0.003742</td>\n",
       "      <td>0.9136</td>\n",
       "      <td>0.002596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>mmlu_professional_medicine/acc</td>\n",
       "      <td>Pythia 6.9B</td>\n",
       "      <td>0.004632</td>\n",
       "      <td>0.8282</td>\n",
       "      <td>0.010342</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Task        Model  Slope Coefficient P-Value  \\\n",
       "0               medqa_4options/acc      OLMo 7B          -0.025473  0.0679   \n",
       "1                      medmcqa/acc      OLMo 7B          -0.030678  0.0311   \n",
       "2                     pubmedqa/acc      OLMo 7B           0.101560  0.0000   \n",
       "3                 mmlu_anatomy/acc      OLMo 7B           0.068073  0.0245   \n",
       "4      mmlu_clinical_knowledge/acc      OLMo 7B           0.020055  0.0690   \n",
       "5         mmlu_college_biology/acc      OLMo 7B          -0.017946  0.2198   \n",
       "6        mmlu_college_medicine/acc      OLMo 7B           0.051300  0.0262   \n",
       "7        mmlu_medical_genetics/acc      OLMo 7B          -0.010960  0.5347   \n",
       "8   mmlu_professional_medicine/acc      OLMo 7B           0.012257  0.5040   \n",
       "9               medqa_4options/acc  Pythia 6.9B          -0.018954  0.3126   \n",
       "10                     medmcqa/acc  Pythia 6.9B          -0.061642  0.1240   \n",
       "11                    pubmedqa/acc  Pythia 6.9B           0.049303  0.0136   \n",
       "12                mmlu_anatomy/acc  Pythia 6.9B           0.011343  0.8375   \n",
       "13     mmlu_clinical_knowledge/acc  Pythia 6.9B           0.014480  0.4130   \n",
       "14        mmlu_college_biology/acc  Pythia 6.9B          -0.007961  0.7590   \n",
       "15       mmlu_college_medicine/acc  Pythia 6.9B          -0.022931  0.5317   \n",
       "16       mmlu_medical_genetics/acc  Pythia 6.9B          -0.003742  0.9136   \n",
       "17  mmlu_professional_medicine/acc  Pythia 6.9B           0.004632  0.8282   \n",
       "\n",
       "    R_squared  \n",
       "0    0.357467  \n",
       "1    0.460162  \n",
       "2    0.971521  \n",
       "3    0.488472  \n",
       "4    0.355323  \n",
       "5    0.181341  \n",
       "6    0.480793  \n",
       "7    0.049982  \n",
       "8    0.057644  \n",
       "9    0.201303  \n",
       "10   0.405677  \n",
       "11   0.735611  \n",
       "12   0.009245  \n",
       "13   0.137409  \n",
       "14   0.020579  \n",
       "15   0.082729  \n",
       "16   0.002596  \n",
       "17   0.010342  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "file_olmo = '../eval-results/wandb-logs/wandb_OLMo7B_dynamics.csv'\n",
    "file_pythia = '../eval-results/wandb-logs/wandb_pythia_6-9b_dynamics.csv'\n",
    "\n",
    "olmo_df = pd.read_csv(file_olmo)\n",
    "pythia_df = pd.read_csv(file_pythia)\n",
    "\n",
    "# Extract the step number and select only accuracy-related columns\n",
    "olmo_df['Step'] = olmo_df['Name'].str.extract('(\\d+)').astype('int64')\n",
    "pythia_df['Step'] = pythia_df['Name'].str.extract('(\\d+)').astype('int64')\n",
    "\n",
    "olmo_df['Model'] = 'OLMo 7B'\n",
    "pythia_df['Model'] = 'Pythia 6.9B'\n",
    "\n",
    "# Define the accuracy columns to be selected\n",
    "accuracy_columns = [\n",
    "    \"medqa_4options/acc\", \"medmcqa/acc\", \"pubmedqa/acc\", \"mmlu_anatomy/acc\", \"mmlu_clinical_knowledge/acc\",\n",
    "    \"mmlu_college_biology/acc\", \"mmlu_college_medicine/acc\", \"mmlu_medical_genetics/acc\", \"mmlu_professional_medicine/acc\"\n",
    "]\n",
    "\n",
    "# Filter the dataframes\n",
    "olmo_relevant = olmo_df[['Step', 'Model'] + accuracy_columns]\n",
    "pythia_relevant = pythia_df[['Step', 'Model'] + accuracy_columns]\n",
    "\n",
    "# Combine the dataframes\n",
    "combined_df = pd.concat([olmo_relevant, pythia_relevant], ignore_index=True)\n",
    "\n",
    "# Calculate tokens seen and log-transform the relevant columns\n",
    "tokens_per_step = 2097152\n",
    "combined_df['tokens_seen'] = combined_df['Step'] * tokens_per_step\n",
    "combined_df['Log10_tokens_seen'] = np.log10(combined_df['tokens_seen'])\n",
    "\n",
    "# Melt the dataframe for easier analysis\n",
    "melted_df = combined_df.melt(id_vars=['tokens_seen', 'Log10_tokens_seen', 'Model'], value_vars=accuracy_columns, var_name='Task', value_name='Accuracy')\n",
    "\n",
    "# Convert 'Accuracy' to log10 scale for regression\n",
    "melted_df['Log10_Accuracy'] = np.log10(melted_df['Accuracy'])\n",
    "\n",
    "# Function to perform log-log regression\n",
    "def analyze_regression(df, model_name):\n",
    "    results_list = []\n",
    "    tasks = df['Task'].unique()\n",
    "    for task in tasks:\n",
    "        task_data = df[(df['Task'] == task) & (df['Model'] == model_name)]\n",
    "        if not task_data.empty:\n",
    "            model = smf.ols(formula='Log10_Accuracy ~ Log10_tokens_seen', data=task_data)\n",
    "            results = model.fit()\n",
    "            results_list.append({\n",
    "                'Task': task,\n",
    "                'Model': model_name,\n",
    "                'Slope Coefficient': results.params['Log10_tokens_seen'],\n",
    "                'P-Value': f\"{results.pvalues['Log10_tokens_seen']:.4f}\",\n",
    "                'R_squared': results.rsquared\n",
    "            })\n",
    "    return pd.DataFrame(results_list)\n",
    "\n",
    "#remove rows where tokens_seen is 0 in the melted_df\n",
    "melted_df = melted_df[melted_df['tokens_seen'] != 0]\n",
    "\n",
    "# Perform regression analysis for OLMo 7B and Pythia 6.9B\n",
    "olmo7b_results = analyze_regression(melted_df, 'OLMo 7B')\n",
    "pythia6_9B_results = analyze_regression(melted_df, 'Pythia 6.9B')\n",
    "\n",
    "\n",
    "\n",
    "# Combine results\n",
    "final_results = pd.concat([olmo7b_results, pythia6_9B_results], ignore_index=True)\n",
    "\n",
    "# Display the results\n",
    "\n",
    "\n",
    "final_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>medmcqa/acc</th>\n",
       "      <th>medmcqa/acc_stderr</th>\n",
       "      <th>medqa_4options/acc</th>\n",
       "      <th>medqa_4options/acc_stderr</th>\n",
       "      <th>mmlu_anatomy/acc</th>\n",
       "      <th>mmlu_anatomy/acc_stderr</th>\n",
       "      <th>mmlu_clinical_knowledge/acc</th>\n",
       "      <th>mmlu_clinical_knowledge/acc_stderr</th>\n",
       "      <th>mmlu_college_biology/acc</th>\n",
       "      <th>mmlu_college_biology/acc_stderr</th>\n",
       "      <th>mmlu_college_medicine/acc</th>\n",
       "      <th>mmlu_college_medicine/acc_stderr</th>\n",
       "      <th>mmlu_medical_genetics/acc</th>\n",
       "      <th>mmlu_medical_genetics/acc_stderr</th>\n",
       "      <th>mmlu_professional_medicine/acc</th>\n",
       "      <th>mmlu_professional_medicine/acc_stderr</th>\n",
       "      <th>pubmedqa/acc</th>\n",
       "      <th>pubmedqa/acc_stderr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>OLMo-1B</td>\n",
       "      <td>0.262013</td>\n",
       "      <td>0.006800</td>\n",
       "      <td>0.274156</td>\n",
       "      <td>0.012508</td>\n",
       "      <td>0.318519</td>\n",
       "      <td>0.040248</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.024618</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.037456</td>\n",
       "      <td>0.225434</td>\n",
       "      <td>0.031862</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.042923</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.023157</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.022001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>OLMo-7B</td>\n",
       "      <td>0.240258</td>\n",
       "      <td>0.006607</td>\n",
       "      <td>0.239592</td>\n",
       "      <td>0.011968</td>\n",
       "      <td>0.288889</td>\n",
       "      <td>0.039155</td>\n",
       "      <td>0.267925</td>\n",
       "      <td>0.027257</td>\n",
       "      <td>0.298611</td>\n",
       "      <td>0.038271</td>\n",
       "      <td>0.329480</td>\n",
       "      <td>0.035839</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.046883</td>\n",
       "      <td>0.216912</td>\n",
       "      <td>0.025036</td>\n",
       "      <td>0.690</td>\n",
       "      <td>0.020704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>pythia-1b-deduped</td>\n",
       "      <td>0.304566</td>\n",
       "      <td>0.007117</td>\n",
       "      <td>0.237235</td>\n",
       "      <td>0.011927</td>\n",
       "      <td>0.207407</td>\n",
       "      <td>0.035026</td>\n",
       "      <td>0.271698</td>\n",
       "      <td>0.027378</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.036210</td>\n",
       "      <td>0.248555</td>\n",
       "      <td>0.032953</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.046057</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.023710</td>\n",
       "      <td>0.506</td>\n",
       "      <td>0.022381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>pythia-6.9b-deduped</td>\n",
       "      <td>0.215396</td>\n",
       "      <td>0.006357</td>\n",
       "      <td>0.215240</td>\n",
       "      <td>0.011524</td>\n",
       "      <td>0.288889</td>\n",
       "      <td>0.039155</td>\n",
       "      <td>0.245283</td>\n",
       "      <td>0.026480</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.033961</td>\n",
       "      <td>0.242775</td>\n",
       "      <td>0.032693</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.044620</td>\n",
       "      <td>0.334559</td>\n",
       "      <td>0.028662</td>\n",
       "      <td>0.608</td>\n",
       "      <td>0.021855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OpenLlama7B</td>\n",
       "      <td>0.259861</td>\n",
       "      <td>0.006782</td>\n",
       "      <td>0.268657</td>\n",
       "      <td>0.012428</td>\n",
       "      <td>0.303704</td>\n",
       "      <td>0.039726</td>\n",
       "      <td>0.309434</td>\n",
       "      <td>0.028450</td>\n",
       "      <td>0.243056</td>\n",
       "      <td>0.035869</td>\n",
       "      <td>0.225434</td>\n",
       "      <td>0.031862</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.045126</td>\n",
       "      <td>0.198529</td>\n",
       "      <td>0.024231</td>\n",
       "      <td>0.736</td>\n",
       "      <td>0.019733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OpenLlama3B</td>\n",
       "      <td>0.306479</td>\n",
       "      <td>0.007129</td>\n",
       "      <td>0.267871</td>\n",
       "      <td>0.012417</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0.033557</td>\n",
       "      <td>0.233962</td>\n",
       "      <td>0.026055</td>\n",
       "      <td>0.256944</td>\n",
       "      <td>0.036539</td>\n",
       "      <td>0.196532</td>\n",
       "      <td>0.030300</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.047610</td>\n",
       "      <td>0.213235</td>\n",
       "      <td>0.024881</td>\n",
       "      <td>0.720</td>\n",
       "      <td>0.020100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             model_name  medmcqa/acc  medmcqa/acc_stderr  medqa_4options/acc  \\\n",
       "5               OLMo-1B     0.262013            0.006800            0.274156   \n",
       "6               OLMo-7B     0.240258            0.006607            0.239592   \n",
       "17    pythia-1b-deduped     0.304566            0.007117            0.237235   \n",
       "20  pythia-6.9b-deduped     0.215396            0.006357            0.215240   \n",
       "1           OpenLlama7B     0.259861            0.006782            0.268657   \n",
       "2           OpenLlama3B     0.306479            0.007129            0.267871   \n",
       "\n",
       "    medqa_4options/acc_stderr  mmlu_anatomy/acc  mmlu_anatomy/acc_stderr  \\\n",
       "5                    0.012508          0.318519                 0.040248   \n",
       "6                    0.011968          0.288889                 0.039155   \n",
       "17                   0.011927          0.207407                 0.035026   \n",
       "20                   0.011524          0.288889                 0.039155   \n",
       "1                    0.012428          0.303704                 0.039726   \n",
       "2                    0.012417          0.185185                 0.033557   \n",
       "\n",
       "    mmlu_clinical_knowledge/acc  mmlu_clinical_knowledge/acc_stderr  \\\n",
       "5                      0.200000                            0.024618   \n",
       "6                      0.267925                            0.027257   \n",
       "17                     0.271698                            0.027378   \n",
       "20                     0.245283                            0.026480   \n",
       "1                      0.309434                            0.028450   \n",
       "2                      0.233962                            0.026055   \n",
       "\n",
       "    mmlu_college_biology/acc  mmlu_college_biology/acc_stderr  \\\n",
       "5                   0.277778                         0.037456   \n",
       "6                   0.298611                         0.038271   \n",
       "17                  0.250000                         0.036210   \n",
       "20                  0.208333                         0.033961   \n",
       "1                   0.243056                         0.035869   \n",
       "2                   0.256944                         0.036539   \n",
       "\n",
       "    mmlu_college_medicine/acc  mmlu_college_medicine/acc_stderr  \\\n",
       "5                    0.225434                          0.031862   \n",
       "6                    0.329480                          0.035839   \n",
       "17                   0.248555                          0.032953   \n",
       "20                   0.242775                          0.032693   \n",
       "1                    0.225434                          0.031862   \n",
       "2                    0.196532                          0.030300   \n",
       "\n",
       "    mmlu_medical_genetics/acc  mmlu_medical_genetics/acc_stderr  \\\n",
       "5                        0.24                          0.042923   \n",
       "6                        0.32                          0.046883   \n",
       "17                       0.30                          0.046057   \n",
       "20                       0.27                          0.044620   \n",
       "1                        0.28                          0.045126   \n",
       "2                        0.34                          0.047610   \n",
       "\n",
       "    mmlu_professional_medicine/acc  mmlu_professional_medicine/acc_stderr  \\\n",
       "5                         0.176471                               0.023157   \n",
       "6                         0.216912                               0.025036   \n",
       "17                        0.187500                               0.023710   \n",
       "20                        0.334559                               0.028662   \n",
       "1                         0.198529                               0.024231   \n",
       "2                         0.213235                               0.024881   \n",
       "\n",
       "    pubmedqa/acc  pubmedqa/acc_stderr  \n",
       "5          0.592             0.022001  \n",
       "6          0.690             0.020704  \n",
       "17         0.506             0.022381  \n",
       "20         0.608             0.021855  \n",
       "1          0.736             0.019733  \n",
       "2          0.720             0.020100  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#MultiMedQA evaluations across model pre-trained on different open-source corpora. \n",
    "#Table displays select results from the following csv files:\n",
    "\n",
    "corpus_evals = pd.read_csv('../eval-results/wandb-logs/cleaned/acc_scale_results.csv')\n",
    "corpus_evals = corpus_evals[corpus_evals['model_name'].str.contains('pythia-6.9b-deduped|pythia-1b-deduped|OLMo')]\n",
    "corpus_evals = corpus_evals.drop_duplicates(subset=['model_name'])\n",
    "\n",
    "openllama_evals = pd.read_csv('../eval-results/wandb-logs/wandb_export_results_updated.csv'    )\n",
    "openllama_evals = openllama_evals[openllama_evals['Name'].str.contains('OpenLlama')]\n",
    "openllama_evals.rename(columns={'Name': 'model_name'}, inplace=True)\n",
    "\n",
    "combined_corpus_evals = pd.concat([corpus_evals, openllama_evals])\n",
    "combined_corpus_evals = combined_corpus_evals.dropna(axis=1, how='any')\n",
    "\n",
    "combined_corpus_evals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total counts (NER question context): Dolma    935014406346\n",
      "Pile     115040931873\n",
      "RPJ      403350594489\n",
      "dtype: int64\n",
      "Total counts (NER question): Dolma    379119470939\n",
      "Pile      45275511089\n",
      "RPJ      173028233524\n",
      "dtype: int64\n",
      "Total counts (MeSH): Dolma    24623505658\n",
      "Pile      2402225335\n",
      "RPJ      11595477221\n",
      "dtype: int64\n",
      "Normalized counts per million (NER question context): Dolma    304777.513932\n",
      "Pile     300133.407794\n",
      "RPJ      291029.724575\n",
      "dtype: float64\n",
      "Normalized counts per million (NER question): Dolma    123577.871155\n",
      "Pile     118120.509035\n",
      "RPJ      124845.134318\n",
      "dtype: float64\n",
      "Normalized counts per million (MeSH): Dolma    8026.283646\n",
      "Pile     6267.230840\n",
      "RPJ      8366.489570\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Table that displays total and normalized/standardized biomedical term counts across pre-training corpora\n",
    "\n",
    "#total tokens in each corpus\n",
    "total_tokens = {\n",
    "    'Dolma': 3067858892487,\n",
    "    'Pile': 383299322520,\n",
    "    'RPJ': 1385942948192\n",
    "}\n",
    "\n",
    "# Loading raw counts\n",
    "ner_context_counts = pd.read_csv('../termfreq-results/biomedical_ner_context_infinigram_counts_merged.csv') \n",
    "ner_question_counts = pd.read_csv('../termfreq-results/biomedical_ner_infinigram_counts_merged.csv')\n",
    "mesh_counts = pd.read_csv('../termfreq-results/mesh_infinigram_counts_merged.csv') \n",
    "\n",
    "total_counts_ner_context = ner_context_counts.iloc[:, 1:].sum()\n",
    "total_counts_ner = ner_question_counts.iloc[:, 1:].sum()\n",
    "total_counts_mesh = mesh_counts.iloc[:, 1:].sum()\n",
    "\n",
    "# Normalize the counts per million tokens\n",
    "normalized_counts_per_million_ner_context = total_counts_ner_context / pd.Series(total_tokens) * 1e6\n",
    "normalized_counts_per_million_ner = total_counts_ner / pd.Series(total_tokens) * 1e6\n",
    "normalized_counts_per_million_mesh = total_counts_mesh / pd.Series(total_tokens) * 1e6\n",
    "\n",
    "print(\"Total counts (NER question context):\", total_counts_ner_context)\n",
    "print(\"Total counts (NER question):\", total_counts_ner)\n",
    "print(\"Total counts (MeSH):\", total_counts_mesh)\n",
    "print(\"Normalized counts per million (NER question context):\", normalized_counts_per_million_ner_context)\n",
    "print(\"Normalized counts per million (NER question):\", normalized_counts_per_million_ner)\n",
    "print(\"Normalized counts per million (MeSH):\", normalized_counts_per_million_mesh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Name</th>\n",
       "      <th>model_family</th>\n",
       "      <th>param_count</th>\n",
       "      <th>medmcqa/acc</th>\n",
       "      <th>medmcqa/acc_stderr</th>\n",
       "      <th>medqa_4options/acc</th>\n",
       "      <th>medqa_4options/acc_stderr</th>\n",
       "      <th>mmlu_anatomy/acc</th>\n",
       "      <th>mmlu_anatomy/acc_stderr</th>\n",
       "      <th>...</th>\n",
       "      <th>mmlu_college_biology/acc</th>\n",
       "      <th>mmlu_college_biology/acc_stderr</th>\n",
       "      <th>mmlu_college_medicine/acc</th>\n",
       "      <th>mmlu_college_medicine/acc_stderr</th>\n",
       "      <th>mmlu_medical_genetics/acc</th>\n",
       "      <th>mmlu_medical_genetics/acc_stderr</th>\n",
       "      <th>mmlu_professional_medicine/acc</th>\n",
       "      <th>mmlu_professional_medicine/acc_stderr</th>\n",
       "      <th>pubmedqa/acc</th>\n",
       "      <th>pubmedqa/acc_stderr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>Paloma1b-Falcon-RefinedWeb</td>\n",
       "      <td>Paloma</td>\n",
       "      <td>1000000000</td>\n",
       "      <td>0.284963</td>\n",
       "      <td>0.006980</td>\n",
       "      <td>0.278869</td>\n",
       "      <td>0.012574</td>\n",
       "      <td>0.251852</td>\n",
       "      <td>0.037499</td>\n",
       "      <td>...</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.036210</td>\n",
       "      <td>0.202312</td>\n",
       "      <td>0.030631</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.046057</td>\n",
       "      <td>0.209559</td>\n",
       "      <td>0.024723</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.022221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>Paloma1b-RedPajama</td>\n",
       "      <td>Paloma</td>\n",
       "      <td>1000000000</td>\n",
       "      <td>0.301458</td>\n",
       "      <td>0.007096</td>\n",
       "      <td>0.267086</td>\n",
       "      <td>0.012405</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.035914</td>\n",
       "      <td>...</td>\n",
       "      <td>0.180556</td>\n",
       "      <td>0.032166</td>\n",
       "      <td>0.294798</td>\n",
       "      <td>0.034766</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.047258</td>\n",
       "      <td>0.253676</td>\n",
       "      <td>0.026431</td>\n",
       "      <td>0.598</td>\n",
       "      <td>0.021949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>Paloma1b-MC4</td>\n",
       "      <td>Paloma</td>\n",
       "      <td>1000000000</td>\n",
       "      <td>0.287593</td>\n",
       "      <td>0.006999</td>\n",
       "      <td>0.272584</td>\n",
       "      <td>0.012485</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.038202</td>\n",
       "      <td>...</td>\n",
       "      <td>0.236111</td>\n",
       "      <td>0.035514</td>\n",
       "      <td>0.225434</td>\n",
       "      <td>0.031862</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.048524</td>\n",
       "      <td>0.191176</td>\n",
       "      <td>0.023887</td>\n",
       "      <td>0.590</td>\n",
       "      <td>0.022017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>Paloma1b-C4</td>\n",
       "      <td>Paloma</td>\n",
       "      <td>1000000000</td>\n",
       "      <td>0.317236</td>\n",
       "      <td>0.007197</td>\n",
       "      <td>0.277298</td>\n",
       "      <td>0.012552</td>\n",
       "      <td>0.237037</td>\n",
       "      <td>0.036737</td>\n",
       "      <td>...</td>\n",
       "      <td>0.243056</td>\n",
       "      <td>0.035869</td>\n",
       "      <td>0.208092</td>\n",
       "      <td>0.030953</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.046057</td>\n",
       "      <td>0.191176</td>\n",
       "      <td>0.023887</td>\n",
       "      <td>0.574</td>\n",
       "      <td>0.022137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>Paloma1b-Pile</td>\n",
       "      <td>Paloma</td>\n",
       "      <td>1000000000</td>\n",
       "      <td>0.277313</td>\n",
       "      <td>0.006923</td>\n",
       "      <td>0.278083</td>\n",
       "      <td>0.012563</td>\n",
       "      <td>0.325926</td>\n",
       "      <td>0.040491</td>\n",
       "      <td>...</td>\n",
       "      <td>0.298611</td>\n",
       "      <td>0.038271</td>\n",
       "      <td>0.231214</td>\n",
       "      <td>0.032147</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.042295</td>\n",
       "      <td>0.143382</td>\n",
       "      <td>0.021289</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.022109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>13</td>\n",
       "      <td>Paloma1b-Dolma</td>\n",
       "      <td>Paloma</td>\n",
       "      <td>1000000000</td>\n",
       "      <td>0.301697</td>\n",
       "      <td>0.007098</td>\n",
       "      <td>0.233307</td>\n",
       "      <td>0.011859</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0.033557</td>\n",
       "      <td>...</td>\n",
       "      <td>0.256944</td>\n",
       "      <td>0.036539</td>\n",
       "      <td>0.225434</td>\n",
       "      <td>0.031862</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.046057</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.023710</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.021931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                        Name model_family  param_count  \\\n",
       "0           8  Paloma1b-Falcon-RefinedWeb       Paloma   1000000000   \n",
       "1           9          Paloma1b-RedPajama       Paloma   1000000000   \n",
       "2          10                Paloma1b-MC4       Paloma   1000000000   \n",
       "3          11                 Paloma1b-C4       Paloma   1000000000   \n",
       "4          12               Paloma1b-Pile       Paloma   1000000000   \n",
       "5          13              Paloma1b-Dolma       Paloma   1000000000   \n",
       "\n",
       "   medmcqa/acc  medmcqa/acc_stderr  medqa_4options/acc  \\\n",
       "0     0.284963            0.006980            0.278869   \n",
       "1     0.301458            0.007096            0.267086   \n",
       "2     0.287593            0.006999            0.272584   \n",
       "3     0.317236            0.007197            0.277298   \n",
       "4     0.277313            0.006923            0.278083   \n",
       "5     0.301697            0.007098            0.233307   \n",
       "\n",
       "   medqa_4options/acc_stderr  mmlu_anatomy/acc  mmlu_anatomy/acc_stderr  ...  \\\n",
       "0                   0.012574          0.251852                 0.037499  ...   \n",
       "1                   0.012405          0.222222                 0.035914  ...   \n",
       "2                   0.012485          0.266667                 0.038202  ...   \n",
       "3                   0.012552          0.237037                 0.036737  ...   \n",
       "4                   0.012563          0.325926                 0.040491  ...   \n",
       "5                   0.011859          0.185185                 0.033557  ...   \n",
       "\n",
       "   mmlu_college_biology/acc  mmlu_college_biology/acc_stderr  \\\n",
       "0                  0.250000                         0.036210   \n",
       "1                  0.180556                         0.032166   \n",
       "2                  0.236111                         0.035514   \n",
       "3                  0.243056                         0.035869   \n",
       "4                  0.298611                         0.038271   \n",
       "5                  0.256944                         0.036539   \n",
       "\n",
       "   mmlu_college_medicine/acc  mmlu_college_medicine/acc_stderr  \\\n",
       "0                   0.202312                          0.030631   \n",
       "1                   0.294798                          0.034766   \n",
       "2                   0.225434                          0.031862   \n",
       "3                   0.208092                          0.030953   \n",
       "4                   0.231214                          0.032147   \n",
       "5                   0.225434                          0.031862   \n",
       "\n",
       "   mmlu_medical_genetics/acc  mmlu_medical_genetics/acc_stderr  \\\n",
       "0                       0.30                          0.046057   \n",
       "1                       0.33                          0.047258   \n",
       "2                       0.37                          0.048524   \n",
       "3                       0.30                          0.046057   \n",
       "4                       0.23                          0.042295   \n",
       "5                       0.30                          0.046057   \n",
       "\n",
       "   mmlu_professional_medicine/acc  mmlu_professional_medicine/acc_stderr  \\\n",
       "0                        0.209559                               0.024723   \n",
       "1                        0.253676                               0.026431   \n",
       "2                        0.191176                               0.023887   \n",
       "3                        0.191176                               0.023887   \n",
       "4                        0.143382                               0.021289   \n",
       "5                        0.187500                               0.023710   \n",
       "\n",
       "   pubmedqa/acc  pubmedqa/acc_stderr  \n",
       "0         0.560             0.022221  \n",
       "1         0.598             0.021949  \n",
       "2         0.590             0.022017  \n",
       "3         0.574             0.022137  \n",
       "4         0.578             0.022109  \n",
       "5         0.600             0.021931  \n",
       "\n",
       "[6 rows x 22 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This table displays MultiMedQA results across Paloma1B models, to highlight the impact of different pre-training corpora.\n",
    "#The data was cleaned from wandb exports using data-cleaner.py in the processing folder\n",
    "paloma_evals = pd.read_csv('../eval-results/wandb-logs/cleaned/acc_data_results.csv')\n",
    "paloma_evals\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
