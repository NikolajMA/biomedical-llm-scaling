{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tables from paper\n",
    "This notebook contains the code to recreate all the analyses and results presented in tables in the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 1\n",
    "Table 1 is an overview of the format and size of MultiMedQA datasets. This data is extracted from the paper of each respective dataset, and does not require any additional processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['step0-tokens0B', 'step1000-tokens4B', 'step2000-tokens9B', 'step3000-tokens13B', 'step4000-tokens18B', 'step5000-tokens22B', 'step6000-tokens27B', 'step7000-tokens31B', 'step8000-tokens35B', 'step9000-tokens40B', 'step10000-tokens44B', 'step11000-tokens49B', 'step12000-tokens53B', 'step13000-tokens58B', 'step14000-tokens62B', 'step15000-tokens66B', 'step16000-tokens71B', 'step17000-tokens75B', 'step18000-tokens80B', 'step19000-tokens84B', 'step20000-tokens88B', 'step21000-tokens93B', 'step22000-tokens97B', 'step23000-tokens102B', 'step24000-tokens106B', 'step25000-tokens111B', 'step26000-tokens115B', 'step27000-tokens119B', 'step28000-tokens124B', 'step29000-tokens128B', 'step30000-tokens133B', 'step31000-tokens137B', 'step32000-tokens142B', 'step33000-tokens146B', 'step34000-tokens150B', 'step35000-tokens155B', 'step36000-tokens159B', 'step37000-tokens164B', 'step38000-tokens168B', 'step39000-tokens173B', 'step40000-tokens177B', 'step41000-tokens181B', 'step42000-tokens186B', 'step43000-tokens190B', 'step44000-tokens195B', 'step45000-tokens199B', 'step46000-tokens203B', 'step47000-tokens208B', 'step48000-tokens212B', 'step49000-tokens217B', 'step50000-tokens221B', 'step51000-tokens226B', 'step52000-tokens230B', 'step53000-tokens234B', 'step54000-tokens239B', 'step55000-tokens243B', 'step56000-tokens248B', 'step57000-tokens252B', 'step58000-tokens257B', 'step59000-tokens261B', 'step60000-tokens265B', 'step61000-tokens270B', 'step62000-tokens274B', 'step63000-tokens279B', 'step64000-tokens283B', 'step65000-tokens288B', 'step66000-tokens292B', 'step67000-tokens296B', 'step68000-tokens301B', 'step69000-tokens305B', 'step70000-tokens310B', 'step71000-tokens314B', 'step72000-tokens319B', 'step73000-tokens323B', 'step74000-tokens327B', 'step75000-tokens332B', 'step76000-tokens336B', 'step77000-tokens341B', 'step78000-tokens345B', 'step79000-tokens349B', 'step80000-tokens354B', 'step81000-tokens358B', 'step82000-tokens363B', 'step83000-tokens367B', 'step84000-tokens372B', 'step85000-tokens376B', 'step86000-tokens380B', 'step87000-tokens385B', 'step88000-tokens389B', 'step89000-tokens394B', 'step90000-tokens398B', 'step91000-tokens403B', 'step92000-tokens407B', 'step93000-tokens411B', 'step94000-tokens416B', 'step95000-tokens420B', 'step96000-tokens425B', 'step97000-tokens429B', 'step98000-tokens434B', 'step99000-tokens438B', 'step100000-tokens442B', 'step101000-tokens447B', 'step102000-tokens451B', 'step103000-tokens456B', 'step104000-tokens460B', 'step105000-tokens464B', 'step106000-tokens469B', 'step107000-tokens473B', 'step108000-tokens478B', 'step109000-tokens482B', 'step110000-tokens487B', 'step111000-tokens491B', 'step112000-tokens495B', 'step113000-tokens500B', 'step114000-tokens504B', 'step115000-tokens509B', 'step116000-tokens513B', 'step117000-tokens518B', 'step118000-tokens522B', 'step119000-tokens526B', 'step120000-tokens531B', 'step121000-tokens535B', 'step122000-tokens540B', 'step123000-tokens544B', 'step124000-tokens549B', 'step125000-tokens553B', 'step126000-tokens557B', 'step127000-tokens562B', 'step128000-tokens566B', 'step129000-tokens571B', 'step130000-tokens575B', 'step131000-tokens580B', 'step132000-tokens584B', 'step133000-tokens588B', 'step134000-tokens593B', 'step135000-tokens597B', 'step136000-tokens602B', 'step137000-tokens606B', 'step138000-tokens610B', 'step139000-tokens615B', 'step140000-tokens619B', 'step141000-tokens624B', 'step142000-tokens628B', 'step143000-tokens633B', 'step144000-tokens637B', 'step145000-tokens641B', 'step146000-tokens646B', 'step147000-tokens650B', 'step148000-tokens655B', 'step149000-tokens659B', 'step150000-tokens664B', 'step151000-tokens668B', 'step152000-tokens672B', 'step153000-tokens677B', 'step154000-tokens681B', 'step155000-tokens686B', 'step156000-tokens690B', 'step157000-tokens695B', 'step158000-tokens699B', 'step159000-tokens703B', 'step160000-tokens708B', 'step161000-tokens712B', 'step162000-tokens717B', 'step163000-tokens721B', 'step164000-tokens725B', 'step165000-tokens730B', 'step166000-tokens734B', 'step167000-tokens739B', 'step168000-tokens743B', 'step169000-tokens748B', 'step170000-tokens752B', 'step171000-tokens756B', 'step172000-tokens761B', 'step173000-tokens765B', 'step174000-tokens770B', 'step175000-tokens774B', 'step176000-tokens779B', 'step177000-tokens783B', 'step178000-tokens787B', 'step179000-tokens792B', 'step180000-tokens796B', 'step181000-tokens801B', 'step182000-tokens805B', 'step183000-tokens810B', 'step184000-tokens814B', 'step185000-tokens818B', 'step186000-tokens823B', 'step187000-tokens827B', 'step188000-tokens832B', 'step189000-tokens836B', 'step190000-tokens840B', 'step191000-tokens845B', 'step192000-tokens849B', 'step193000-tokens854B', 'step194000-tokens858B', 'step195000-tokens863B', 'step196000-tokens867B', 'step197000-tokens871B', 'step198000-tokens876B', 'step199000-tokens880B', 'step200000-tokens885B', 'step201000-tokens889B', 'step202000-tokens894B', 'step203000-tokens898B', 'step204000-tokens902B', 'step205000-tokens907B', 'step206000-tokens911B', 'step207000-tokens916B', 'step208000-tokens920B', 'step209000-tokens925B', 'step210000-tokens929B', 'step211000-tokens933B', 'step212000-tokens938B', 'step213000-tokens942B', 'step214000-tokens947B', 'step215000-tokens951B', 'step216000-tokens956B', 'step217000-tokens960B', 'step218000-tokens964B', 'step219000-tokens969B', 'step220000-tokens973B', 'step221000-tokens978B', 'step222000-tokens982B', 'step223000-tokens986B', 'step224000-tokens991B', 'step225000-tokens995B', 'step226000-tokens1000B', 'step227000-tokens1004B', 'step228000-tokens1009B', 'step229000-tokens1013B', 'step230000-tokens1017B', 'step231000-tokens1022B', 'step232000-tokens1026B', 'step233000-tokens1031B', 'step234000-tokens1035B', 'step235000-tokens1040B', 'step236000-tokens1044B', 'step237000-tokens1048B', 'step238000-tokens1053B', 'step239000-tokens1057B', 'step240000-tokens1062B', 'step241000-tokens1066B', 'step242000-tokens1071B', 'step243000-tokens1075B', 'step244000-tokens1079B', 'step245000-tokens1084B', 'step246000-tokens1088B', 'step247000-tokens1093B', 'step248000-tokens1097B', 'step249000-tokens1101B', 'step250000-tokens1106B', 'step251000-tokens1110B', 'step252000-tokens1115B', 'step253000-tokens1119B', 'step254000-tokens1124B', 'step255000-tokens1128B', 'step256000-tokens1132B', 'step257000-tokens1137B', 'step258000-tokens1141B', 'step259000-tokens1146B', 'step260000-tokens1150B', 'step261000-tokens1155B', 'step262000-tokens1159B', 'step263000-tokens1163B', 'step264000-tokens1168B', 'step265000-tokens1172B', 'step266000-tokens1177B', 'step267000-tokens1181B', 'step268000-tokens1186B', 'step269000-tokens1190B', 'step270000-tokens1194B', 'step271000-tokens1199B', 'step272000-tokens1203B', 'step273000-tokens1208B', 'step274000-tokens1212B', 'step275000-tokens1217B', 'step276000-tokens1221B', 'step277000-tokens1225B', 'step278000-tokens1230B', 'step279000-tokens1234B', 'step280000-tokens1239B', 'step281000-tokens1243B', 'step282000-tokens1247B', 'step283000-tokens1252B', 'step284000-tokens1256B', 'step285000-tokens1261B', 'step286000-tokens1265B', 'step287000-tokens1270B', 'step288000-tokens1274B', 'step289000-tokens1278B', 'step290000-tokens1283B', 'step291000-tokens1287B', 'step292000-tokens1292B', 'step293000-tokens1296B', 'step294000-tokens1301B', 'step295000-tokens1305B', 'step296000-tokens1309B', 'step297000-tokens1314B', 'step298000-tokens1318B', 'step299000-tokens1323B', 'step300000-tokens1327B', 'step301000-tokens1332B', 'step302000-tokens1336B', 'step303000-tokens1340B', 'step304000-tokens1345B', 'step305000-tokens1349B', 'step306000-tokens1354B', 'step307000-tokens1358B', 'step308000-tokens1362B', 'step309000-tokens1367B', 'step310000-tokens1371B', 'step311000-tokens1376B', 'step312000-tokens1380B', 'step313000-tokens1385B', 'step314000-tokens1389B', 'step315000-tokens1393B', 'step316000-tokens1398B', 'step317000-tokens1402B', 'step318000-tokens1407B', 'step319000-tokens1411B', 'step320000-tokens1416B', 'step321000-tokens1420B', 'step322000-tokens1424B', 'step323000-tokens1429B', 'step324000-tokens1433B', 'step325000-tokens1438B', 'step326000-tokens1442B', 'step327000-tokens1447B', 'step328000-tokens1451B', 'step329000-tokens1455B', 'step330000-tokens1460B', 'step331000-tokens1464B', 'step332000-tokens1469B', 'step333000-tokens1473B', 'step334000-tokens1478B', 'step335000-tokens1482B', 'step336000-tokens1486B', 'step337000-tokens1491B', 'step338000-tokens1495B', 'step339000-tokens1500B', 'step340000-tokens1504B', 'step341000-tokens1508B', 'step342000-tokens1513B', 'step343000-tokens1517B', 'step344000-tokens1522B', 'step345000-tokens1526B', 'step346000-tokens1531B', 'step347000-tokens1535B', 'step348000-tokens1539B', 'step349000-tokens1544B', 'step350000-tokens1548B', 'step351000-tokens1553B', 'step352000-tokens1557B', 'step353000-tokens1562B', 'step354000-tokens1566B', 'step355000-tokens1570B', 'step356000-tokens1575B', 'step357000-tokens1579B', 'step358000-tokens1584B', 'step359000-tokens1588B', 'step360000-tokens1593B', 'step361000-tokens1597B', 'step362000-tokens1601B', 'step363000-tokens1606B', 'step364000-tokens1610B', 'step365000-tokens1615B', 'step366000-tokens1619B', 'step367000-tokens1623B', 'step368000-tokens1628B', 'step369000-tokens1632B', 'step370000-tokens1637B', 'step371000-tokens1641B', 'step372000-tokens1646B', 'step373000-tokens1650B', 'step374000-tokens1654B', 'step375000-tokens1659B', 'step376000-tokens1663B', 'step377000-tokens1668B', 'step378000-tokens1672B', 'step379000-tokens1677B', 'step380000-tokens1681B', 'step381000-tokens1685B', 'step382000-tokens1690B', 'step383000-tokens1694B', 'step384000-tokens1699B', 'step385000-tokens1703B', 'step386000-tokens1708B', 'step387000-tokens1712B', 'step388000-tokens1716B', 'step389000-tokens1721B', 'step390000-tokens1725B', 'step391000-tokens1730B', 'step392000-tokens1734B', 'step393000-tokens1739B', 'step394000-tokens1743B', 'step395000-tokens1747B', 'step396000-tokens1752B', 'step397000-tokens1756B', 'step398000-tokens1761B', 'step399000-tokens1765B', 'step400000-tokens1769B', 'step401000-tokens1774B', 'step402000-tokens1778B', 'step403000-tokens1783B', 'step404000-tokens1787B', 'step405000-tokens1792B', 'step406000-tokens1796B', 'step407000-tokens1800B', 'step408000-tokens1805B', 'step409000-tokens1809B', 'step410000-tokens1814B', 'step411000-tokens1818B', 'step412000-tokens1823B', 'step413000-tokens1827B', 'step414000-tokens1831B', 'step415000-tokens1836B', 'step416000-tokens1840B', 'step417000-tokens1845B', 'step418000-tokens1849B', 'step419000-tokens1854B', 'step420000-tokens1858B', 'step421000-tokens1862B', 'step422000-tokens1867B', 'step423000-tokens1871B', 'step424000-tokens1876B', 'step425000-tokens1880B', 'step426000-tokens1884B', 'step427000-tokens1889B', 'step428000-tokens1893B', 'step429000-tokens1898B', 'step430000-tokens1902B', 'step431000-tokens1907B', 'step432000-tokens1911B', 'step433000-tokens1915B', 'step434000-tokens1920B', 'step435000-tokens1924B', 'step436000-tokens1929B', 'step437000-tokens1933B', 'step438000-tokens1938B', 'step439000-tokens1942B', 'step440000-tokens1946B', 'step441000-tokens1951B', 'step442000-tokens1955B', 'step443000-tokens1960B', 'step444000-tokens1964B', 'step445000-tokens1969B', 'step446000-tokens1973B', 'step447000-tokens1977B', 'step448000-tokens1982B', 'step449000-tokens1986B', 'step450000-tokens1991B', 'step451000-tokens1995B', 'step452000-tokens2000B', 'step453000-tokens2004B', 'step454000-tokens2008B', 'step455000-tokens2013B', 'step456000-tokens2017B', 'step457000-tokens2022B', 'step458000-tokens2026B', 'step459000-tokens2030B', 'step460000-tokens2035B', 'step461000-tokens2039B', 'step462000-tokens2044B', 'step463000-tokens2048B', 'step464000-tokens2053B', 'step465000-tokens2057B', 'step466000-tokens2061B', 'step467000-tokens2066B', 'step468000-tokens2070B', 'step469000-tokens2075B', 'step470000-tokens2079B', 'step471000-tokens2084B', 'step472000-tokens2088B', 'step473000-tokens2092B', 'step474000-tokens2097B', 'step475000-tokens2101B', 'step476000-tokens2106B', 'step477000-tokens2110B', 'step478000-tokens2115B', 'step479000-tokens2119B', 'step480000-tokens2123B', 'step481000-tokens2128B', 'step482000-tokens2132B', 'step483000-tokens2137B', 'step484000-tokens2141B', 'step485000-tokens2145B', 'step486000-tokens2150B', 'step487000-tokens2154B', 'step488000-tokens2159B', 'step489000-tokens2163B', 'step490000-tokens2168B', 'step491000-tokens2172B', 'step492000-tokens2176B', 'step493000-tokens2181B', 'step494000-tokens2185B', 'step495000-tokens2190B', 'step496000-tokens2194B', 'step497000-tokens2199B', 'step498000-tokens2203B', 'step499000-tokens2207B', 'step500000-tokens2212B', 'step501000-tokens2216B', 'step502000-tokens2221B', 'step503000-tokens2225B', 'step504000-tokens2230B', 'step505000-tokens2234B', 'step506000-tokens2238B', 'step507000-tokens2243B', 'step508000-tokens2247B', 'step509000-tokens2252B', 'step510000-tokens2256B', 'step511000-tokens2261B', 'step512000-tokens2265B', 'step513000-tokens2269B', 'step514000-tokens2274B', 'step515000-tokens2278B', 'step516000-tokens2283B', 'step517000-tokens2287B', 'step518000-tokens2291B', 'step519000-tokens2296B', 'step520000-tokens2300B', 'step521000-tokens2305B', 'step522000-tokens2309B', 'step523000-tokens2314B', 'step524000-tokens2318B', 'step525000-tokens2322B', 'step526000-tokens2327B', 'step527000-tokens2331B', 'step528000-tokens2336B', 'step529000-tokens2340B', 'step530000-tokens2345B', 'step531000-tokens2349B', 'step532000-tokens2353B', 'step533000-tokens2358B', 'step534000-tokens2362B', 'step535000-tokens2367B', 'step536000-tokens2371B', 'step537000-tokens2376B', 'step538000-tokens2380B', 'step539000-tokens2384B', 'step540000-tokens2389B', 'step541000-tokens2393B', 'step542000-tokens2398B', 'step543000-tokens2402B', 'step544000-tokens2406B', 'step545000-tokens2411B', 'step546000-tokens2415B', 'step547000-tokens2420B', 'step548000-tokens2424B', 'step549000-tokens2429B', 'step550000-tokens2433B', 'step551000-tokens2437B', 'step552000-tokens2442B', 'step553000-tokens2446B', 'step554000-tokens2451B', 'step555000-tokens2455B', 'step556000-tokens2460B', 'step557000-tokens2464B', 'main']\n"
     ]
    }
   ],
   "source": [
    "#Overview of intermediate pre-training steps used to evaluate OLMo 7B. \n",
    "# The specific revisions are noted in the paper\n",
    "\n",
    "from huggingface_hub import list_repo_refs\n",
    "\n",
    "out = list_repo_refs(\"allenai/OLMo-7B\")\n",
    "branches = [b.name for b in out.branches]\n",
    "\n",
    "# Extract the step number from the branch name\n",
    "def get_step_number(branch_name):\n",
    "    return int(branch_name.split('-')[0].replace('step', ''))\n",
    "\n",
    "sorted_branches = sorted(branches, key=lambda x: get_step_number(x) if x != \"main\" else float('inf'))\n",
    "\n",
    "print(sorted_branches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Unnamed: 0           model_name  medmcqa/acc  medmcqa/acc_stderr  \\\n",
      "0            0           Mamba-1.4b     0.235477            0.006561   \n",
      "1            1           Mamba-130m     0.320344            0.007215   \n",
      "2            2           Mamba-2.8b     0.257471            0.006761   \n",
      "3            3           Mamba-370m     0.323213            0.007232   \n",
      "4            4           Mamba-790m     0.314129            0.007178   \n",
      "5            5              OLMo-1B     0.262013            0.006800   \n",
      "6            6              OLMo-7B     0.240258            0.006607   \n",
      "7            7         Qwen1.5-0.5B     0.329429            0.007268   \n",
      "8            8         Qwen1.5-1.8B     0.368157            0.007458   \n",
      "9            9          Qwen1.5-14B     0.531676            0.007716   \n",
      "10          10          Qwen1.5-14B     0.531676            0.007716   \n",
      "11          11           Qwen1.5-4B     0.436768            0.007670   \n",
      "12          12           Qwen1.5-7B     0.502271            0.007732   \n",
      "13          13  pythia-1.4b-deduped     0.311499            0.007161   \n",
      "14          14   pythia-12b-deduped     0.251255            0.006707   \n",
      "15          15   pythia-12b-deduped     0.251255            0.006707   \n",
      "16          16  pythia-160m-deduped     0.316758            0.007194   \n",
      "17          17    pythia-1b-deduped     0.304566            0.007117   \n",
      "18          18  pythia-2.8b-deduped     0.261057            0.006792   \n",
      "19          19  pythia-410m-deduped     0.320822            0.007218   \n",
      "20          20  pythia-6.9b-deduped     0.215396            0.006357   \n",
      "21          21  pythia-6.9b-deduped     0.215396            0.006357   \n",
      "22          22   pythia-70m-deduped     0.318671            0.007205   \n",
      "\n",
      "    medqa_4options/acc  medqa_4options/acc_stderr  mmlu_anatomy/acc  \\\n",
      "0             0.228594                   0.011774          0.237037   \n",
      "1             0.276512                   0.012541          0.185185   \n",
      "2             0.251375                   0.012163          0.251852   \n",
      "3             0.270228                   0.012451          0.274074   \n",
      "4             0.266300                   0.012394          0.200000   \n",
      "5             0.274156                   0.012508          0.318519   \n",
      "6             0.239592                   0.011968          0.288889   \n",
      "7             0.315789                   0.013033          0.348148   \n",
      "8             0.348782                   0.013363          0.400000   \n",
      "9             0.542812                   0.013968          0.651852   \n",
      "10            0.542812                   0.013968          0.651852   \n",
      "11            0.414768                   0.013814          0.488889   \n",
      "12            0.487824                   0.014015          0.518519   \n",
      "13            0.257659                   0.012263          0.207407   \n",
      "14            0.245090                   0.012061          0.251852   \n",
      "15            0.245090                   0.012061          0.251852   \n",
      "16            0.281225                   0.012606          0.192593   \n",
      "17            0.237235                   0.011927          0.207407   \n",
      "18            0.257659                   0.012263          0.281481   \n",
      "19            0.271799                   0.012474          0.185185   \n",
      "20            0.215240                   0.011524          0.288889   \n",
      "21            0.215240                   0.011524          0.288889   \n",
      "22            0.274941                   0.012519          0.185185   \n",
      "\n",
      "    mmlu_anatomy/acc_stderr  mmlu_clinical_knowledge/acc  \\\n",
      "0                  0.036737                     0.196226   \n",
      "1                  0.033557                     0.211321   \n",
      "2                  0.037499                     0.324528   \n",
      "3                  0.038533                     0.218868   \n",
      "4                  0.034555                     0.222642   \n",
      "5                  0.040248                     0.200000   \n",
      "6                  0.039155                     0.267925   \n",
      "7                  0.041153                     0.403774   \n",
      "8                  0.042321                     0.479245   \n",
      "9                  0.041153                     0.735849   \n",
      "10                 0.041153                     0.735849   \n",
      "11                 0.043183                     0.592453   \n",
      "12                 0.043164                     0.649057   \n",
      "13                 0.035026                     0.222642   \n",
      "14                 0.037499                     0.256604   \n",
      "15                 0.037499                     0.256604   \n",
      "16                 0.034065                     0.211321   \n",
      "17                 0.035026                     0.271698   \n",
      "18                 0.038850                     0.256604   \n",
      "19                 0.033557                     0.222642   \n",
      "20                 0.039155                     0.245283   \n",
      "21                 0.039155                     0.245283   \n",
      "22                 0.033557                     0.211321   \n",
      "\n",
      "    mmlu_clinical_knowledge/acc_stderr  ...  mmlu_college_medicine/acc  \\\n",
      "0                             0.024442  ...                   0.219653   \n",
      "1                             0.025126  ...                   0.208092   \n",
      "2                             0.028816  ...                   0.375723   \n",
      "3                             0.025448  ...                   0.202312   \n",
      "4                             0.025604  ...                   0.265896   \n",
      "5                             0.024618  ...                   0.225434   \n",
      "6                             0.027257  ...                   0.329480   \n",
      "7                             0.030198  ...                   0.346821   \n",
      "8                             0.030746  ...                   0.456647   \n",
      "9                             0.027134  ...                   0.676301   \n",
      "10                            0.027134  ...                   0.676301   \n",
      "11                            0.030242  ...                   0.537572   \n",
      "12                            0.029374  ...                   0.606936   \n",
      "13                            0.025604  ...                   0.225434   \n",
      "14                            0.026881  ...                   0.265896   \n",
      "15                            0.026881  ...                   0.265896   \n",
      "16                            0.025126  ...                   0.208092   \n",
      "17                            0.027378  ...                   0.248555   \n",
      "18                            0.026881  ...                   0.236994   \n",
      "19                            0.025604  ...                   0.236994   \n",
      "20                            0.026480  ...                   0.242775   \n",
      "21                            0.026480  ...                   0.242775   \n",
      "22                            0.025126  ...                   0.208092   \n",
      "\n",
      "    mmlu_college_medicine/acc_stderr  mmlu_medical_genetics/acc  \\\n",
      "0                           0.031568                       0.30   \n",
      "1                           0.030953                       0.30   \n",
      "2                           0.036928                       0.30   \n",
      "3                           0.030631                       0.30   \n",
      "4                           0.033688                       0.33   \n",
      "5                           0.031862                       0.24   \n",
      "6                           0.035839                       0.32   \n",
      "7                           0.036291                       0.46   \n",
      "8                           0.037981                       0.58   \n",
      "9                           0.035676                       0.77   \n",
      "10                          0.035676                       0.77   \n",
      "11                          0.038017                       0.59   \n",
      "12                          0.037242                       0.69   \n",
      "13                          0.031862                       0.23   \n",
      "14                          0.033688                       0.36   \n",
      "15                          0.033688                       0.36   \n",
      "16                          0.030953                       0.30   \n",
      "17                          0.032953                       0.30   \n",
      "18                          0.032424                       0.27   \n",
      "19                          0.032424                       0.30   \n",
      "20                          0.032693                       0.27   \n",
      "21                          0.032693                       0.27   \n",
      "22                          0.030953                       0.30   \n",
      "\n",
      "    mmlu_medical_genetics/acc_stderr  mmlu_professional_medicine/acc  \\\n",
      "0                           0.046057                        0.250000   \n",
      "1                           0.046057                        0.191176   \n",
      "2                           0.046057                        0.393382   \n",
      "3                           0.046057                        0.150735   \n",
      "4                           0.047258                        0.238971   \n",
      "5                           0.042923                        0.176471   \n",
      "6                           0.046883                        0.216912   \n",
      "7                           0.050091                        0.275735   \n",
      "8                           0.049604                        0.485294   \n",
      "9                           0.042295                        0.713235   \n",
      "10                          0.042295                        0.713235   \n",
      "11                          0.049431                        0.477941   \n",
      "12                          0.046482                        0.580882   \n",
      "13                          0.042295                        0.187500   \n",
      "14                          0.048242                        0.150735   \n",
      "15                          0.048242                        0.150735   \n",
      "16                          0.046057                        0.191176   \n",
      "17                          0.046057                        0.187500   \n",
      "18                          0.044620                        0.382353   \n",
      "19                          0.046057                        0.183824   \n",
      "20                          0.044620                        0.334559   \n",
      "21                          0.044620                        0.334559   \n",
      "22                          0.046057                        0.183824   \n",
      "\n",
      "    mmlu_professional_medicine/acc_stderr  model_family  param_count  \\\n",
      "0                                0.026304         Mamba   1400000000   \n",
      "1                                0.023887         Mamba    130000000   \n",
      "2                                0.029674         Mamba   2800000000   \n",
      "3                                0.021734         Mamba    370000000   \n",
      "4                                0.025905         Mamba    790000000   \n",
      "5                                0.023157          OLMo   1000000000   \n",
      "6                                0.025036          OLMo   7000000000   \n",
      "7                                0.027146          Qwen    500000000   \n",
      "8                                0.030360          Qwen   1800000000   \n",
      "9                                0.027472          Qwen  14000000000   \n",
      "10                               0.027472          Qwen  14000000000   \n",
      "11                               0.030343          Qwen   4000000000   \n",
      "12                               0.029973          Qwen   7000000000   \n",
      "13                               0.023710        Pythia   1400000000   \n",
      "14                               0.021734        Pythia  12000000000   \n",
      "15                               0.021734        Pythia  12000000000   \n",
      "16                               0.023887        Pythia    160000000   \n",
      "17                               0.023710        Pythia   1000000000   \n",
      "18                               0.029520        Pythia   2800000000   \n",
      "19                               0.023529        Pythia    410000000   \n",
      "20                               0.028662        Pythia   6900000000   \n",
      "21                               0.028662        Pythia   6900000000   \n",
      "22                               0.023529        Pythia     70000000   \n",
      "\n",
      "   pubmedqa/acc  pubmedqa/acc_stderr  \n",
      "0         0.652             0.021324  \n",
      "1         0.530             0.022343  \n",
      "2         0.734             0.019781  \n",
      "3         0.530             0.022343  \n",
      "4         0.660             0.021206  \n",
      "5         0.592             0.022001  \n",
      "6         0.690             0.020704  \n",
      "7         0.620             0.021729  \n",
      "8         0.520             0.022365  \n",
      "9         0.764             0.019009  \n",
      "10        0.764             0.019009  \n",
      "11        0.664             0.021145  \n",
      "12        0.678             0.020917  \n",
      "13        0.584             0.022065  \n",
      "14        0.700             0.020514  \n",
      "15        0.700             0.020514  \n",
      "16        0.446             0.022252  \n",
      "17        0.506             0.022381  \n",
      "18        0.640             0.021488  \n",
      "19        0.542             0.022304  \n",
      "20        0.608             0.021855  \n",
      "21        0.608             0.021855  \n",
      "22        0.544             0.022296  \n",
      "\n",
      "[23 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "#MultiMedQA evaluations across model scale. Raw eval data cleaned using _____\n",
    "import pandas as pd\n",
    "scale_evals = pd.read_csv('../eval-results/wandb-logs/cleaned/acc_scale_results.csv')\n",
    "print(scale_evals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Family</th>\n",
       "      <th>Task</th>\n",
       "      <th>Slope Coefficient</th>\n",
       "      <th>P-value</th>\n",
       "      <th>R^2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mamba</td>\n",
       "      <td>Medmcqa</td>\n",
       "      <td>-0.095008</td>\n",
       "      <td>0.124749</td>\n",
       "      <td>0.598614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mamba</td>\n",
       "      <td>Medqa 4Options</td>\n",
       "      <td>-0.046384</td>\n",
       "      <td>0.167476</td>\n",
       "      <td>0.522841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mamba</td>\n",
       "      <td>Mmlu Anatomy</td>\n",
       "      <td>0.067453</td>\n",
       "      <td>0.398219</td>\n",
       "      <td>0.243512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mamba</td>\n",
       "      <td>Mmlu Clinical Knowledge</td>\n",
       "      <td>0.095133</td>\n",
       "      <td>0.309719</td>\n",
       "      <td>0.331503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mamba</td>\n",
       "      <td>Mmlu College Biology</td>\n",
       "      <td>-0.084192</td>\n",
       "      <td>0.006800</td>\n",
       "      <td>0.937258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Mamba</td>\n",
       "      <td>Mmlu College Medicine</td>\n",
       "      <td>0.163580</td>\n",
       "      <td>0.138800</td>\n",
       "      <td>0.572384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Mamba</td>\n",
       "      <td>Mmlu Medical Genetics</td>\n",
       "      <td>0.002446</td>\n",
       "      <td>0.913208</td>\n",
       "      <td>0.004654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Mamba</td>\n",
       "      <td>Mmlu Professional Medicine</td>\n",
       "      <td>0.245517</td>\n",
       "      <td>0.090186</td>\n",
       "      <td>0.670091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Mamba</td>\n",
       "      <td>Pubmedqa</td>\n",
       "      <td>0.113127</td>\n",
       "      <td>0.022653</td>\n",
       "      <td>0.862306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Qwen</td>\n",
       "      <td>Medmcqa</td>\n",
       "      <td>0.153585</td>\n",
       "      <td>0.000349</td>\n",
       "      <td>0.969629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Qwen</td>\n",
       "      <td>Medqa 4Options</td>\n",
       "      <td>0.173915</td>\n",
       "      <td>0.000581</td>\n",
       "      <td>0.960888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Qwen</td>\n",
       "      <td>Mmlu Anatomy</td>\n",
       "      <td>0.192322</td>\n",
       "      <td>0.000529</td>\n",
       "      <td>0.962674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Qwen</td>\n",
       "      <td>Mmlu Clinical Knowledge</td>\n",
       "      <td>0.185895</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.990167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Qwen</td>\n",
       "      <td>Mmlu College Biology</td>\n",
       "      <td>0.241821</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.939523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Qwen</td>\n",
       "      <td>Mmlu College Medicine</td>\n",
       "      <td>0.200453</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.997306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Qwen</td>\n",
       "      <td>Mmlu Medical Genetics</td>\n",
       "      <td>0.151379</td>\n",
       "      <td>0.000350</td>\n",
       "      <td>0.969588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Qwen</td>\n",
       "      <td>Mmlu Professional Medicine</td>\n",
       "      <td>0.265157</td>\n",
       "      <td>0.001181</td>\n",
       "      <td>0.944401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Qwen</td>\n",
       "      <td>Pubmedqa</td>\n",
       "      <td>0.083410</td>\n",
       "      <td>0.086506</td>\n",
       "      <td>0.561203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Pythia</td>\n",
       "      <td>Medmcqa</td>\n",
       "      <td>-0.072522</td>\n",
       "      <td>0.003122</td>\n",
       "      <td>0.684911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Pythia</td>\n",
       "      <td>Medqa 4Options</td>\n",
       "      <td>-0.038877</td>\n",
       "      <td>0.012162</td>\n",
       "      <td>0.565111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Pythia</td>\n",
       "      <td>Mmlu Anatomy</td>\n",
       "      <td>0.086625</td>\n",
       "      <td>0.001687</td>\n",
       "      <td>0.728266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Pythia</td>\n",
       "      <td>Mmlu Clinical Knowledge</td>\n",
       "      <td>0.037346</td>\n",
       "      <td>0.011728</td>\n",
       "      <td>0.568789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Pythia</td>\n",
       "      <td>Mmlu College Biology</td>\n",
       "      <td>-0.060905</td>\n",
       "      <td>0.006221</td>\n",
       "      <td>0.628618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Pythia</td>\n",
       "      <td>Mmlu College Medicine</td>\n",
       "      <td>0.040972</td>\n",
       "      <td>0.001049</td>\n",
       "      <td>0.757804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Pythia</td>\n",
       "      <td>Mmlu Medical Genetics</td>\n",
       "      <td>0.013575</td>\n",
       "      <td>0.614117</td>\n",
       "      <td>0.033250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Pythia</td>\n",
       "      <td>Mmlu Professional Medicine</td>\n",
       "      <td>0.041474</td>\n",
       "      <td>0.543804</td>\n",
       "      <td>0.047835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Pythia</td>\n",
       "      <td>Pubmedqa</td>\n",
       "      <td>0.063938</td>\n",
       "      <td>0.003558</td>\n",
       "      <td>0.674895</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Model Family                        Task  Slope Coefficient   P-value  \\\n",
       "0         Mamba                     Medmcqa          -0.095008  0.124749   \n",
       "1         Mamba              Medqa 4Options          -0.046384  0.167476   \n",
       "2         Mamba                Mmlu Anatomy           0.067453  0.398219   \n",
       "3         Mamba     Mmlu Clinical Knowledge           0.095133  0.309719   \n",
       "4         Mamba        Mmlu College Biology          -0.084192  0.006800   \n",
       "5         Mamba       Mmlu College Medicine           0.163580  0.138800   \n",
       "6         Mamba       Mmlu Medical Genetics           0.002446  0.913208   \n",
       "7         Mamba  Mmlu Professional Medicine           0.245517  0.090186   \n",
       "8         Mamba                    Pubmedqa           0.113127  0.022653   \n",
       "9          Qwen                     Medmcqa           0.153585  0.000349   \n",
       "10         Qwen              Medqa 4Options           0.173915  0.000581   \n",
       "11         Qwen                Mmlu Anatomy           0.192322  0.000529   \n",
       "12         Qwen     Mmlu Clinical Knowledge           0.185895  0.000036   \n",
       "13         Qwen        Mmlu College Biology           0.241821  0.001400   \n",
       "14         Qwen       Mmlu College Medicine           0.200453  0.000003   \n",
       "15         Qwen       Mmlu Medical Genetics           0.151379  0.000350   \n",
       "16         Qwen  Mmlu Professional Medicine           0.265157  0.001181   \n",
       "17         Qwen                    Pubmedqa           0.083410  0.086506   \n",
       "18       Pythia                     Medmcqa          -0.072522  0.003122   \n",
       "19       Pythia              Medqa 4Options          -0.038877  0.012162   \n",
       "20       Pythia                Mmlu Anatomy           0.086625  0.001687   \n",
       "21       Pythia     Mmlu Clinical Knowledge           0.037346  0.011728   \n",
       "22       Pythia        Mmlu College Biology          -0.060905  0.006221   \n",
       "23       Pythia       Mmlu College Medicine           0.040972  0.001049   \n",
       "24       Pythia       Mmlu Medical Genetics           0.013575  0.614117   \n",
       "25       Pythia  Mmlu Professional Medicine           0.041474  0.543804   \n",
       "26       Pythia                    Pubmedqa           0.063938  0.003558   \n",
       "\n",
       "         R^2  \n",
       "0   0.598614  \n",
       "1   0.522841  \n",
       "2   0.243512  \n",
       "3   0.331503  \n",
       "4   0.937258  \n",
       "5   0.572384  \n",
       "6   0.004654  \n",
       "7   0.670091  \n",
       "8   0.862306  \n",
       "9   0.969629  \n",
       "10  0.960888  \n",
       "11  0.962674  \n",
       "12  0.990167  \n",
       "13  0.939523  \n",
       "14  0.997306  \n",
       "15  0.969588  \n",
       "16  0.944401  \n",
       "17  0.561203  \n",
       "18  0.684911  \n",
       "19  0.565111  \n",
       "20  0.728266  \n",
       "21  0.568789  \n",
       "22  0.628618  \n",
       "23  0.757804  \n",
       "24  0.033250  \n",
       "25  0.047835  \n",
       "26  0.674895  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Log-Log Regression Analysis of Model Scale vs Task Accuracy\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "data = scale_evals.copy()\n",
    "\n",
    "#remove OLMo from df\n",
    "data = data[data['model_family'] != 'OLMo']\n",
    "\n",
    "data['log_param_count'] = np.log10(data['param_count'])\n",
    "\n",
    "\n",
    "\n",
    "tasks = ['medmcqa/acc', 'medqa_4options/acc', 'mmlu_anatomy/acc', \n",
    "         'mmlu_clinical_knowledge/acc', 'mmlu_college_biology/acc', \n",
    "         'mmlu_college_medicine/acc', 'mmlu_medical_genetics/acc', \n",
    "         'mmlu_professional_medicine/acc', 'pubmedqa/acc']\n",
    "\n",
    "for task in tasks:\n",
    "    data[f'log_{task}'] = np.log10(data[task])\n",
    "\n",
    "# Function to perform log-log regression and return slope, p-value, and R^2\n",
    "def log_log_regression(x, y):\n",
    "    x = sm.add_constant(x)  # Adds a constant term to the predictor\n",
    "    model = sm.OLS(y, x).fit()\n",
    "    return model.params[1], model.pvalues[1], model.rsquared\n",
    "\n",
    "# Prepare the results table\n",
    "results = []\n",
    "\n",
    "for model_family in data['model_family'].unique():\n",
    "    for task in tasks:\n",
    "        task_label = task.replace('/acc', '').replace('_', ' ').title()\n",
    "        subset = data[data['model_family'] == model_family]\n",
    "        x = subset['log_param_count']\n",
    "        y = subset[f'log_{task}']\n",
    "        slope, p_value, r_squared = log_log_regression(x, y)\n",
    "        results.append([model_family, task_label, slope, p_value, r_squared])\n",
    "\n",
    "results_df_table4 = pd.DataFrame(results, columns=['Model Family', 'Task', 'Slope Coefficient', 'P-value', 'R^2'])\n",
    "\n",
    "\n",
    "results_df_table4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
